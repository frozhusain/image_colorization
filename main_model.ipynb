{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colorize_image.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLClv6xQZca6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT8fDRqVQus7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "d2f8a6e9-a05b-42a1-c7ce-d251e8ef2da7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7lC4QUJ8Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Conv2d(in_ch, out_ch, stride, kernel_size=3, padding=1):\n",
        "        \"\"\"Returns an instance of nn.Conv2d\"\"\"\n",
        "        return nn.Conv2d(in_channels=in_ch, out_channels=out_ch,\n",
        "                         stride=stride, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "\n",
        "class LowLevelFeatures(nn.Module):\n",
        "    \"\"\"Low-Level Features Network\"\"\"\n",
        "\n",
        "    def __init__(self, net_divisor=1):\n",
        "        super(LowLevelFeatures, self).__init__()\n",
        "\n",
        "        ksize = np.array([1, 64, 128, 128, 256, 256, 512]) // net_divisor\n",
        "        ksize[0] = 1\n",
        "\n",
        "        self.conv1 = Conv2d(1, ksize[1], 2)\n",
        "        self.conv2 = Conv2d(ksize[1], ksize[2], 1)\n",
        "        self.conv3 = Conv2d(ksize[2], ksize[3], 2)\n",
        "        self.conv4 = Conv2d(ksize[3], ksize[4], 1)\n",
        "        self.conv5 = Conv2d(ksize[4], ksize[5], 2)\n",
        "        self.conv6 = Conv2d(ksize[5], ksize[6], 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.relu(self.conv3(out))\n",
        "        out = F.relu(self.conv4(out))\n",
        "        out = F.relu(self.conv5(out))\n",
        "        out = F.relu(self.conv6(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class MidLevelFeatures(nn.Module):\n",
        "    \"\"\"Mid-Level Features Network\"\"\"\n",
        "\n",
        "    def __init__(self, net_divisor=1):\n",
        "        super(MidLevelFeatures, self).__init__()\n",
        "\n",
        "        ksize = np.array([512, 512, 256]) // net_divisor\n",
        "\n",
        "        self.conv7 = Conv2d(ksize[0], ksize[1], 1)\n",
        "        self.conv8 = Conv2d(ksize[1], ksize[2], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv7(x))\n",
        "        out = F.relu(self.conv8(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class GlobalFeatures(nn.Module):\n",
        "    \"\"\"Global Features Network\"\"\"\n",
        "\n",
        "    def __init__(self, net_divisor=1):\n",
        "        super(GlobalFeatures, self).__init__()\n",
        "\n",
        "        ksize = np.array([512, 1024, 512, 256]) // net_divisor\n",
        "        self.ksize0 = ksize[0]\n",
        "\n",
        "        self.conv1 = Conv2d(ksize[0], ksize[0], 2)\n",
        "        self.conv2 = Conv2d(ksize[0], ksize[0], 1)\n",
        "        self.conv3 = Conv2d(ksize[0], ksize[0], 2)\n",
        "        self.conv4 = Conv2d(ksize[0], ksize[0], 1)\n",
        "        self.fc1 = nn.Linear(7*7*ksize[0], ksize[1])\n",
        "        self.fc2 = nn.Linear(ksize[1], ksize[2])\n",
        "        self.fc3 = nn.Linear(ksize[2], ksize[3])\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.conv1(x))\n",
        "        y = F.relu(self.conv2(y))\n",
        "        y = F.relu(self.conv3(y))\n",
        "        y = F.relu(self.conv4(y))\n",
        "        y = y.view(-1, 7*7*self.ksize0)\n",
        "        y = F.relu(self.fc1(y))\n",
        "        y = F.relu(self.fc2(y))\n",
        "        \n",
        "        # Branching\n",
        "        out = y\n",
        "        classification_in = y\n",
        "\n",
        "        out = F.relu(self.fc3(out))\n",
        "\n",
        "        return out, classification_in\n",
        "\n",
        "class ColorizationNetwork(nn.Module):\n",
        "    \"\"\"Colorizaion Network\"\"\"\n",
        "\n",
        "    def __init__(self, net_divisor=1):\n",
        "        super(ColorizationNetwork, self).__init__()\n",
        "\n",
        "        ksize = np.array([256, 128, 64, 64, 32]) // net_divisor\n",
        "\n",
        "        self.conv9 = Conv2d(ksize[0], ksize[1], 1)\n",
        "        \n",
        "        # Here comes upsample #1\n",
        "        \n",
        "        self.conv10 = Conv2d(ksize[1], ksize[2], 1)\n",
        "        self.conv11 = Conv2d(ksize[2], ksize[3], 1)\n",
        "        \n",
        "        # Here comes upsample #2        \n",
        "        \n",
        "        self.conv12 = Conv2d(ksize[3], ksize[4], 1)\n",
        "        self.conv13 = Conv2d(ksize[4], 2, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv9(x))\n",
        "\n",
        "        # Upsample #1        \n",
        "        out = nn.functional.interpolate(input=out, scale_factor=2)\n",
        "\n",
        "        out = F.relu(self.conv10(out))\n",
        "        out = F.relu(self.conv11(out))\n",
        "        \n",
        "        # Upsample #2\n",
        "        out = nn.functional.interpolate(input=out, scale_factor=2)\n",
        "\n",
        "        out = F.relu(self.conv12(out))\n",
        "        out = torch.sigmoid(self.conv13(out))\n",
        "        \n",
        "        # Upsample #3\n",
        "        out = nn.functional.interpolate(input=out, scale_factor=2)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class ClassNet(nn.Module):\n",
        "    \"\"\"Classification Network Class\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, net_divisor=1):\n",
        "        super(ClassNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        ksize = np.array([512, 256]) // net_divisor\n",
        "\n",
        "        self.fc1 = nn.Linear(ksize[0], ksize[1])\n",
        "        self.fc2 = nn.Linear(ksize[1], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ColNet(nn.Module):\n",
        "    \"\"\"Colorization network class\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, net_divisor=1):\n",
        "        \"\"\"Initializes the network.\n",
        "        Args:\n",
        "            net_divisor - divisor of net output sizes. Useful for debugging.\n",
        "        \"\"\"\n",
        "        super(ColNet, self).__init__()\n",
        "\n",
        "        self.net_divisor = net_divisor\n",
        "\n",
        "        self.conv_fuse = Conv2d(512 // net_divisor, 256 // net_divisor, 1, kernel_size=1, padding=0)\n",
        "\n",
        "        self.low = LowLevelFeatures(net_divisor)\n",
        "        self.mid = MidLevelFeatures(net_divisor)\n",
        "        self.classifier = ClassNet(num_classes, net_divisor)\n",
        "        self.glob = GlobalFeatures(net_divisor)\n",
        "        self.col = ColorizationNetwork(net_divisor)\n",
        "\n",
        "\n",
        "\n",
        "    def fusion_layer(self, mid_out, glob_out):\n",
        "        h = mid_out.shape[2]  # Height of a picture  \n",
        "        w = mid_out.shape[3]  # Width of a picture\n",
        "        \n",
        "        glob_stack2d = torch.stack(tuple(glob_out for _ in range(w)), 1)\n",
        "        glob_stack3d = torch.stack(tuple(glob_stack2d for _ in range(h)), 1)\n",
        "        glob_stack3d = glob_stack3d.permute(0, 3, 1, 2)\n",
        "\n",
        "        # 'Merge' two volumes\n",
        "        stack_volume = torch.cat((mid_out, glob_stack3d), 1)\n",
        "\n",
        "        out = F.relu(self.conv_fuse(stack_volume))\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Low level\n",
        "        low_out = self.low(x)\n",
        "        \n",
        "        # Net branch         \n",
        "        mid_out = low_out\n",
        "        glob_out = low_out\n",
        "\n",
        "        # Mid level\n",
        "        mid_out = self.mid(mid_out)\n",
        "\n",
        "        # Global\n",
        "        glob_out, classification_in = self.glob(glob_out)\n",
        "\n",
        "        # Classification\n",
        "        classification_out = self.classifier(classification_in)\n",
        "\n",
        "        # Fusion layer\n",
        "        out = self.fusion_layer(mid_out, glob_out)\n",
        "\n",
        "        # Colorization Net\n",
        "        out = self.col(out)\n",
        "        \n",
        "        return out, classification_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHZSE64cM3U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms\n",
        "from skimage import color, io\n",
        "from random import randint\n",
        "\n",
        "\n",
        "class HandleGrayscale(object):\n",
        "    \"\"\"Feeds the pipeline with 3 - channel image.\n",
        "    \n",
        "    All transformations below work with RGB images only.\n",
        "    If a 1-channel grayscale image is given, it's converted to\n",
        "    equivalent 3-channel RGB image.\n",
        "    \"\"\"\n",
        "    def __call__(self, image):\n",
        "        if len(image.shape) < 3:\n",
        "            image = color.gray2rgb(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Randomly crops an image to size x size.\"\"\"\n",
        "    \n",
        "    def __init__(self, size=224):\n",
        "        self.size = size\n",
        "        \n",
        "    def __call__(self, image):\n",
        "\n",
        "        h, w, _ = image.shape\n",
        "        assert min(h, w) >= self.size\n",
        "\n",
        "        off_h = randint(0, h - self.size)\n",
        "        off_w = randint(0, w - self.size)\n",
        "\n",
        "        cropped = image[off_h:off_h+self.size, off_w:off_w+self.size]\n",
        "\n",
        "        assert cropped.shape == (self.size, self.size, 3)\n",
        "        return cropped\n",
        "\n",
        "    \n",
        "class Rgb2LabNorm(object):\n",
        "    \"\"\"Converts an RGB image to normalized image in LAB color sapce.\n",
        "    \n",
        "    Both (L, ab) channels are in [0, 1].\n",
        "    \"\"\"\n",
        "    def __call__(self, image):\n",
        "        assert image.shape == (224, 224, 3)\n",
        "        img_lab = color.rgb2lab(image)\n",
        "        img_lab[:,:,:1] = img_lab[:,:,:1] / 100.0\n",
        "        img_lab[:,:,1:] = (img_lab[:,:,1:] + 128.0) / 256.0\n",
        "        return img_lab\n",
        "    \n",
        "    \n",
        "class ToTensor(object):\n",
        "    \"\"\"Converts an image to torch.Tensor.\n",
        "    \n",
        "    Note:\n",
        "        Images are Height x Width x Channels format. \n",
        "        One needs to convert them to CxHxW.\n",
        "    \"\"\"\n",
        "    def __call__(self, image):\n",
        "        \n",
        "        assert image.shape == (224, 224, 3)\n",
        "        \n",
        "        transposed =  np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "        image_tensor = torch.from_numpy(transposed)\n",
        "\n",
        "        assert image_tensor.shape == (3, 224, 224)\n",
        "        return image_tensor\n",
        "\n",
        "    \n",
        "class SplitLab(object):\n",
        "    \"\"\"Splits tensor LAB image to L and ab channels.\"\"\"\n",
        "    def __call__(self, image):\n",
        "        assert image.shape == (3, 224, 224)\n",
        "        L  = image[:1,:,:]\n",
        "        ab = image[1:,:,:]\n",
        "        return (L, ab)\n",
        "    \n",
        "    \n",
        "    \n",
        "class ImagesDateset(torchvision.datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset for loading and pre-processing images.\"\"\"\n",
        "\n",
        "    def __init__(self, root, testing=False):\n",
        "        \"\"\"Initializes the dataset and loads images. \n",
        "        If testing is set, then image name is returned instead of label.\n",
        "        Args:\n",
        "            root: a directory from which images are loaded\n",
        "            testing: if set to True, an image name will \n",
        "                be returned insead of label index\n",
        "        \"\"\"\n",
        "        super().__init__(root=root, loader=io.imread)\n",
        "        \n",
        "        self.testing = testing\n",
        "\n",
        "        self.composed = torchvision.transforms.Compose(\n",
        "            [HandleGrayscale(), RandomCrop(224), Rgb2LabNorm(), \n",
        "             ToTensor(), SplitLab()]\n",
        "        )\n",
        "         \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Gets an image in LAB color space.\n",
        "        Returns:\n",
        "            Returns a tuple (L, ab, label, name), where:\n",
        "                L: stands for lightness - it's the net input\n",
        "                ab: is chrominance - something that the net learns\n",
        "                label: image label. If in testing mode, this is an image name.\n",
        "            Both L and ab are torch.tesnsor\n",
        "        \"\"\"\n",
        "        image, label =  super().__getitem__(idx)\n",
        "        \n",
        "        L, ab = self.composed(image)\n",
        "\n",
        "        if self.testing:\n",
        "            label = self.get_name(idx)\n",
        "\n",
        "        return L, ab, label\n",
        "\n",
        "    \n",
        "    def get_name(self, idx):\n",
        "        path = os.path.normpath(self.imgs[idx][0])\n",
        "        name = os.path.basename(path)\n",
        "        label = os.path.basename(os.path.dirname(path))\n",
        "        return label + \"-\" + name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCLMLS66NkMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "\n",
        "\n",
        "def net_out2rgb(L, ab_out):\n",
        "    \"\"\"Translates the net output back to an image.\n",
        "    More specifically: unnormalizes both L and ab_out channels, stacks them\n",
        "    into an image in LAB color space and converts back to RGB.\n",
        "  \n",
        "    Args:\n",
        "        L: original L channel of an image\n",
        "        ab_out: ab channel which was learnt by the network\n",
        "    \n",
        "    Retruns: \n",
        "        3 channel RGB image\n",
        "    \"\"\"\n",
        "    # Convert to numpy and unnnormalize\n",
        "    L = L.numpy() * 100.0\n",
        "    ab_out = ab_out.numpy() * 254.0 - 127.0\n",
        "    \n",
        "    \n",
        "    # L and ab_out are tenosr i.e. are of shape of\n",
        "    # Height x Width x Channels\n",
        "    # We need to transpose axis back to HxWxC\n",
        "    L = L.transpose((1, 2, 0))\n",
        "    ab_out = ab_out.transpose((1, 2, 0))\n",
        "\n",
        "    # Stack layers  \n",
        "    img_stack = np.dstack((L, ab_out))\n",
        "    \n",
        "    #   - torch requires tensors to be float32\n",
        "    #   - thus all above (L, ab) are float32\n",
        "    #   - scikit image floats are in range -1 to 1\n",
        "    #   - http://scikit-image.org/docs/dev/user_guide/data_types.html\n",
        "    #   - idk what's next, but converting image to float64 somehow\n",
        "    #     does the job. scikit automagically converts those values.\n",
        "    img_stack = img_stack.astype(np.float64)\t\n",
        "    return  color.lab2rgb(img_stack)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NemhcUcYNJni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "653f91e7-77f2-4a35-e86a-22b9d0ec1b16"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage import io\n",
        "\n",
        "#import ColNet\n",
        "#import ImagesDateset\n",
        "#import net_out2rgb\n",
        "\n",
        "\n",
        "class Training:\n",
        "    \"\"\"Trains model based on given hyperparameterms\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 epochs,\n",
        "                 img_dir_train, \n",
        "                 img_dir_val,\n",
        "                 img_dir_test,\n",
        "                 start_epoch=0,\n",
        "                 net_divisor=1,\n",
        "                 learning_rate=0.0001,\n",
        "                 model_checkpoint=None,\n",
        "                 models_dir='/content/drive/My Drive/colorization/model',\n",
        "                 img_out_dir='/content/drive/My Drive/colorization/out',\n",
        "                 num_workers=4):\n",
        "        \"\"\"Initializes training environment\n",
        "        Args:\n",
        "            batch_size: size of a batch\n",
        "            epoches: number of epoches to run\n",
        "            img_dir_train: name of directory containing images for TRAINING\n",
        "            img_dir_val: name of directory containing images for VALIDATING\n",
        "            img_dir_test: name of directory containing images for TESTING\n",
        "            start_epoch: epoch to start training with. Default: 0\n",
        "            net_divisor: divisor og the net output sizes. Default: 1\n",
        "            learning_rate: alpha parameter of GD/ADAM. Default: 0.0001\n",
        "            model_checkpoint: a path to a previously saved model. \n",
        "                Training will resume. Defaut: None\n",
        "            models_dir: directory to which models are saved. DEFAULT: ./model\n",
        "            img_out_dir: a directory where colorized\n",
        "                images are saved. DEFAULT: ./out\n",
        "        \"\"\"\n",
        "        self.img_dir_train = img_dir_train\n",
        "        self.img_dir_val = img_dir_val\n",
        "        self.img_dir_test = img_dir_test\n",
        "        self.net_divisor = net_divisor\n",
        "        \n",
        "        self.models_dir = models_dir\n",
        "        self.img_out_dir = img_out_dir\n",
        "        \n",
        "        if not os.path.exists(self.models_dir):\n",
        "              os.makedirs(self.models_dir)\n",
        "        if not os.path.exists(self.img_out_dir):\n",
        "              os.makedirs(self.img_out_dir)\n",
        "        \n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        self.trainset = ImagesDateset(self.img_dir_train)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=self.BATCH_SIZE, \n",
        "                                      shuffle=True, num_workers=num_workers)\n",
        "\n",
        "        self.testset = ImagesDateset(self.img_dir_test, testing=True)\n",
        "        self.testloader = DataLoader(self.testset, batch_size=self.BATCH_SIZE,\n",
        "                                     shuffle=False, num_workers=num_workers)\n",
        "\n",
        "        self.devset = ImagesDateset(self.img_dir_val)\n",
        "        self.devloader = DataLoader(self.devset, batch_size=self.BATCH_SIZE,\n",
        "                                    shuffle=False, num_workers=num_workers)\n",
        "\n",
        "        self.classes = self.trainloader.dataset.classes\n",
        "        self.num_classes = len(self.classes)\n",
        "        \n",
        "\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
        "                                   else \"cpu\")\n",
        "        print(\"Using {}\\n\".format(self.device))\n",
        "        \n",
        "        self.net = ColNet(net_divisor=net_divisor, num_classes=self.num_classes)\n",
        "        self.net.to(self.device)\n",
        "        \n",
        "        self.start_epoch = start_epoch\n",
        "        self.EPOCHS = epochs\n",
        "        \n",
        "        self.loss_history = { \"train\": [], \"val\":[] }\n",
        "        \n",
        "        self.mse = nn.MSELoss(reduction='sum')\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
        "        \n",
        "        if model_checkpoint:\n",
        "            self.load_checkpoint(model_checkpoint)\n",
        "        \n",
        "        self.current_model_name = model_checkpoint\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "        self.best_model_dir = os.path.join(self.models_dir, 'colnet-the-best.pt')\n",
        "\n",
        "        \n",
        "    def loss(self, col_target, col_out, class_target, class_out):\n",
        "        loss_col = self.mse(col_target, col_out)\n",
        "        loss_class = self.ce(class_out, class_target)\n",
        "        return loss_col + loss_class/300.0\n",
        "\n",
        "\n",
        "    def train(self, epoch):\n",
        "        \"\"\"One epoch network training\"\"\"\n",
        "        self.load_checkpoint(\"/content/drive/My Drive/colorization/model/colnet.pt\")\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        # Turn train mode on\n",
        "        self.net.train() \n",
        "\n",
        "        for batch_idx, train_data in enumerate(self.trainloader):\n",
        "\n",
        "            L, ab, labels = train_data\n",
        "            L, ab, labels = L.to(self.device), ab.to(self.device), labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            ab_out, labels_out = self.net(L)\n",
        "            \n",
        "            assert ab.shape == ab_out.shape\n",
        "            \n",
        "            loss = self.loss(ab, ab_out, labels, labels_out)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "            batch_loss = loss.item()\n",
        "            \n",
        "            print('[Epoch {:>2} / {} | Batch: {:>2} / {}] loss: {:>10.3f}'\n",
        "                .format(epoch+1, self.EPOCHS, batch_idx + 1, len(self.trainloader), batch_loss))\n",
        "            epoch_loss += batch_loss\n",
        "            \n",
        "        # Epoch loss = mean loss over all batches\n",
        "        # length of trainloader indicates number of batches\n",
        "        epoch_loss /= len(self.trainloader)\n",
        "        self.loss_history['train'].append(epoch_loss)\n",
        "\n",
        "        print(\"Epoch loss: {:.5f}\".format(epoch_loss))\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        \"\"\"One epoch validation on a dev set\"\"\"\n",
        "\n",
        "        print(\"\\nValidating...\")\n",
        "        dev_loss = 0.0\n",
        "\n",
        "        # Turn eval mode on\n",
        "        self.net.eval()\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            for batch_idx, dev_data in enumerate(self.devloader):\n",
        "\n",
        "                L_dev, ab_dev, labels_dev = dev_data\n",
        "                L_dev, ab_dev, labels_dev = L_dev.to(self.device), ab_dev.to(self.device), labels_dev.to(self.device)\n",
        "\n",
        "                ab_dev_output, labels_dev_out = self.net(L_dev)\n",
        "\n",
        "                assert ab_dev.shape == ab_dev_output.shape\n",
        "                \n",
        "                dev_batch_loss = self.loss(ab_dev, ab_dev_output, labels_dev, labels_dev_out )\n",
        "                dev_loss += dev_batch_loss.item()\n",
        "\n",
        "                print(\"[Validation] [Batch {:>2} / {}] dev loss: {:>10.3f}\"\n",
        "                    .format(batch_idx+1, len(self.devloader), dev_batch_loss))\n",
        "                \n",
        "                \n",
        "        dev_loss /= len(self.devloader)        \n",
        "        \n",
        "        print(\"Dev loss {:.5f}\".format(dev_loss))\n",
        "        self.loss_history['val'].append(dev_loss)\n",
        "\n",
        "\n",
        "    def test(self, model_dir=None):\n",
        "        \"\"\"Tests network on a test set.\n",
        "        \n",
        "        Saves all pics to a predefined directory (self.img_out_dir)\n",
        "        \"\"\"\n",
        "\n",
        "        if model_dir is None:\n",
        "            model_dir = self.current_model_name\n",
        "\n",
        "            if os.path.isfile(self.best_model_dir):\n",
        "                model_dir = self.best_model_dir\n",
        "\n",
        "        print(\"Make sure you're using up to date model!!!\")    \n",
        "        print(\"Colorizing {} using {}\\n\".format(self.img_dir_test, model_dir))\n",
        "\n",
        "\n",
        "        self.load_checkpoint(model_dir)\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # Switch to evaluation mode\n",
        "        self.net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_no, data in enumerate(self.testloader):\n",
        "                \n",
        "                print(\"Processing batch {} / {}\"\n",
        "                      .format(batch_no + 1, len(self.testloader)))\n",
        "                \n",
        "                L, _, name = data\n",
        "                L = L.to(self.device)\n",
        "                ab_outputs, _ = self.net(L)\n",
        "                \n",
        "                L = L.to(torch.device(\"cpu\"))\n",
        "                ab_outputs = ab_outputs.to(torch.device(\"cpu\"))\n",
        "                \n",
        "                for i in range(L.shape[0]):\n",
        "                    img = net_out2rgb(L[i], ab_outputs[i])\n",
        "                    io.imsave(os.path.join(self.img_out_dir, name[i]), img)\n",
        "                \n",
        "        print(\"Saved all photos to \" + self.img_out_dir)\n",
        "\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        \"\"\"Saves a checkpoint of the model to a file.\"\"\"\n",
        "        path = self.models_dir\n",
        "        #fname = \"colnet{}-{}.pt\".format(time.strftime(\"%y%m%d-%H-%M-%S\"), epoch)\n",
        "        fname=\"colnet.pt\"\n",
        "        full_path = os.path.join(path, fname)\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'losses': self.loss_history,\n",
        "            'net_divisor': self.net_divisor,\n",
        "            'classes': self.classes\n",
        "        }, full_path)        \n",
        "\n",
        "        self.current_model_name = full_path\n",
        "        print('\\nsaved model to {}\\n'.format(full_path))\n",
        "\n",
        "        # If current model is the best - save it!\n",
        "        current_val_loss = self.loss_history['val'][-1]\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            shutil.copy(full_path, self.best_model_dir)\n",
        "            print(\"Saved the best model on epoch: {}\\n\".format(epoch + 1))\n",
        "\n",
        "\n",
        "\n",
        "    def load_checkpoint(self, model_checkpoint):\n",
        "        \"\"\"Load a checkpoint from a given path.\n",
        "        \n",
        "        Args:\n",
        "            model_checkpoint: path to the checkpoint.\n",
        "        \"\"\"\n",
        "        print(\"Resuming training of: \" + model_checkpoint)\n",
        "        checkpoint = torch.load(model_checkpoint, map_location=torch.device(\"cpu\"))\n",
        "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.loss_history = checkpoint['losses']\n",
        "        self.start_epoch = checkpoint['epoch'] + 1 \n",
        "        self.net_divisor = checkpoint['net_divisor'] \n",
        "        self.current_model_name = model_checkpoint\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Runs both training and validating.\"\"\"\n",
        "        for epoch in range(self.start_epoch, self.EPOCHS):\n",
        "            print(\"{2}\\nEpoch {0} / {1}\\n{2}\"\n",
        "                  .format(epoch + 1, self.EPOCHS, '-'*47))\n",
        "            self.train(epoch)\n",
        "            self.validate(epoch)\n",
        "            self.save_checkpoint(epoch)\n",
        "        print('\\nFinished Training.\\n')\n",
        "\n",
        "\n",
        "    def info(self):\n",
        "        print(\"{0} Training environment info {0}\\n\".format(\"-\"*13))\n",
        "\n",
        "        print(\"Training starts from epoch: {}\".format(self.start_epoch))\n",
        "        print(\"Total number of epochs:     {}\".format(self.EPOCHS))\n",
        "        print(\"ColNet parameters are devided by: {}\".format(self.net_divisor))\n",
        "        print(\"Batch size:  {}\".format(self.BATCH_SIZE))\n",
        "        print(\"Used device: {}\".format(self.device))\n",
        "        print(\"Number of classes: {}\".format(self.num_classes))\n",
        "        print()\n",
        "\n",
        "        if self.current_model_name:\n",
        "            print(\"Current model name:      \" + self.current_model_name)\n",
        "\n",
        "        print(\"Training data directory: \" + self.img_dir_train)\n",
        "        print(\"Validate data directory: \" + self.img_dir_val)\n",
        "        print(\"Testing data directory:  \" + self.img_dir_test)\n",
        "        print(\"Models are saved to:     \" + self.models_dir)\n",
        "        print(\"Colorized images are saved to: \" + self.img_out_dir)\n",
        "        print(\"-\" * 53 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Hello, have a great day!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPUTcVrzTg52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2ea7d007-e4ff-4900-ed4d-c378af2accda"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\"\"\"This module is responsible for reading configuration from YAML file.\"\"\"\n",
        "\n",
        "\n",
        "help = \"\"\"\n",
        "Structure of YAML file:\n",
        "    model_checkpoint: (optional) path to a checkpoint of a net state. \n",
        "        If given, training resume on based on rest of parameters.\n",
        "    models_dir: a directory to which models are saved. DEFAULT: ../model\n",
        "    img_out_dir: a directory where colorized images are saved. DEFAULT: ../out\n",
        "    epochs: total number of epoches for model yo run.\n",
        "    batch_size: batch size for train, test and dev sets.\n",
        "    net_divisor: (optional) divisor of net optput sizes. DEFAULT: 1.\n",
        "    learning_rate: (optional) learning rate of a net. DEFAULT: 0.0001.\n",
        "    img_dir_train: name of directory containing images for TRAINING.\n",
        "    img_dir_val: name of directory containing images for VALIDATING.\n",
        "    img_dir_test: name of directory containing images for TESTING.\n",
        "    num_workers: number of workers in trainloader. DEFAULT: 4\n",
        "\"\"\"\n",
        "\n",
        "import yaml\n",
        "import argparse\n",
        "#import Training\n",
        "#import ColNet\n",
        "\n",
        "\n",
        "def load_config(config_file, model_checkpoint=None):\n",
        "    \"\"\"Loads config from YAML file\n",
        "    Args:\n",
        "        config_file: path to config file\n",
        "    Returns:\n",
        "        Instance of Training environment\n",
        "    \"\"\"\n",
        "\n",
        "    # Default parameters\n",
        "    net_divisor = 1\n",
        "    learning_rate = 0.0001\n",
        "    num_workers = 4\n",
        "    models_dir = \"/content/drive/My Drive/colorization/model\"\n",
        "    img_out_dir = \"/content/drive/My Drive/colorization/out\"\n",
        "\n",
        "    with open(config_file, 'r') as conf:\n",
        "        y = yaml.load(conf)\n",
        "\n",
        "        if 'net_divisor' in y:\n",
        "            net_divisor = y['net_divisor']\n",
        "        \n",
        "        if 'learning_rate' in y:\n",
        "            learning_rate = y['learning_rate']\n",
        "\n",
        "        if 'model_checkpoint' in y:\n",
        "            model_checkpoint = y['model_checkpoint']\n",
        "\n",
        "        if 'num_workers' in y:\n",
        "            num_workers = y['num_workers']\n",
        "\n",
        "        if 'models_dir' in y:\n",
        "            models_dir = y['models_dir']\n",
        "        \n",
        "        if 'img_out_dir' in y:\n",
        "            img_out_dir = y['img_out_dir']\n",
        "\n",
        "\n",
        "        train = Training(batch_size=y['batch_size'],\n",
        "                         epochs=y['epochs'],\n",
        "                         img_dir_train=y['img_dir_train'],\n",
        "                         img_dir_val=y['img_dir_val'],\n",
        "                         img_dir_test=y['img_dir_test'],\n",
        "                         net_divisor=net_divisor,\n",
        "                         learning_rate=learning_rate,\n",
        "                         model_checkpoint=model_checkpoint,\n",
        "                         num_workers=num_workers,\n",
        "                         models_dir=models_dir,\n",
        "                         img_out_dir=img_out_dir)\n",
        "\n",
        "        return train\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "    short_desc = 'Loads network configuration from YAML file.\\n'\n",
        "    parser = argparse.ArgumentParser(description=short_desc + help,\n",
        "                      formatter_class=argparse.RawDescriptionHelpFormatter)\n",
        "    parser.add_argument('config', metavar='config', help='/content/colnet/config/places13.yaml')\n",
        "    parser.add_argument('--model', help='Path to pretrained .pt model')\n",
        "    args = parser.parse_args()\n",
        " \n",
        "    t = load_config(args.config, args.model)\n",
        "    t.info()\n",
        "    t.run()\n",
        "    t.test()\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    short_desc = \\'Loads network configuration from YAML file.\\n\\'\\n    parser = argparse.ArgumentParser(description=short_desc + help,\\n                      formatter_class=argparse.RawDescriptionHelpFormatter)\\n    parser.add_argument(\\'config\\', metavar=\\'config\\', help=\\'/content/colnet/config/places13.yaml\\')\\n    parser.add_argument(\\'--model\\', help=\\'Path to pretrained .pt model\\')\\n    args = parser.parse_args()\\n \\n    t = load_config(args.config, args.model)\\n    t.info()\\n    t.run()\\n    t.test()\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxInVTZLW3NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#from loader import load_config\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "config_file = '/content/drive/My Drive/colorization/colorization.yaml'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzOkqvObZm_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "e6b0afee-716e-4e40-96a0-5badfd79dc24"
      },
      "source": [
        "t = load_config(config_file=config_file)\n",
        "t.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda:0\n",
            "\n",
            "------------- Training environment info -------------\n",
            "\n",
            "Training starts from epoch: 0\n",
            "Total number of epochs:     50\n",
            "ColNet parameters are devided by: 1\n",
            "Batch size:  32\n",
            "Used device: cuda:0\n",
            "Number of classes: 11\n",
            "\n",
            "Training data directory: /content/drive/My Drive/colorization/train_data\n",
            "Validate data directory: /content/drive/My Drive/colorization/validation\n",
            "Testing data directory:  /content/drive/My Drive/colorization/test\n",
            "Models are saved to:     /content/drive/My Drive/colorization/model\n",
            "Colorized images are saved to: /content/drive/My Drive/colorization/out\n",
            "-----------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iznufd5UZqnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8e7e8c4-f700-47bf-9297-75adcfa5c82b"
      },
      "source": [
        "t.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Epoch 35 / 50 | Batch: 233 / 301] loss:    898.235\n",
            "[Epoch 35 / 50 | Batch: 234 / 301] loss:   1285.661\n",
            "[Epoch 35 / 50 | Batch: 235 / 301] loss:    988.897\n",
            "[Epoch 35 / 50 | Batch: 236 / 301] loss:   1292.241\n",
            "[Epoch 35 / 50 | Batch: 237 / 301] loss:   1337.138\n",
            "[Epoch 35 / 50 | Batch: 238 / 301] loss:   1172.325\n",
            "[Epoch 35 / 50 | Batch: 239 / 301] loss:    929.331\n",
            "[Epoch 35 / 50 | Batch: 240 / 301] loss:   1157.465\n",
            "[Epoch 35 / 50 | Batch: 241 / 301] loss:   1663.622\n",
            "[Epoch 35 / 50 | Batch: 242 / 301] loss:   1333.661\n",
            "[Epoch 35 / 50 | Batch: 243 / 301] loss:   1185.571\n",
            "[Epoch 35 / 50 | Batch: 244 / 301] loss:   1258.856\n",
            "[Epoch 35 / 50 | Batch: 245 / 301] loss:   1267.515\n",
            "[Epoch 35 / 50 | Batch: 246 / 301] loss:   1059.312\n",
            "[Epoch 35 / 50 | Batch: 247 / 301] loss:    810.978\n",
            "[Epoch 35 / 50 | Batch: 248 / 301] loss:   1267.747\n",
            "[Epoch 35 / 50 | Batch: 249 / 301] loss:   1004.304\n",
            "[Epoch 35 / 50 | Batch: 250 / 301] loss:   1301.104\n",
            "[Epoch 35 / 50 | Batch: 251 / 301] loss:   1482.514\n",
            "[Epoch 35 / 50 | Batch: 252 / 301] loss:   1807.041\n",
            "[Epoch 35 / 50 | Batch: 253 / 301] loss:   1305.045\n",
            "[Epoch 35 / 50 | Batch: 254 / 301] loss:   1228.901\n",
            "[Epoch 35 / 50 | Batch: 255 / 301] loss:   1102.212\n",
            "[Epoch 35 / 50 | Batch: 256 / 301] loss:   1288.616\n",
            "[Epoch 35 / 50 | Batch: 257 / 301] loss:   1325.189\n",
            "[Epoch 35 / 50 | Batch: 258 / 301] loss:   1161.200\n",
            "[Epoch 35 / 50 | Batch: 259 / 301] loss:   1478.642\n",
            "[Epoch 35 / 50 | Batch: 260 / 301] loss:   1267.336\n",
            "[Epoch 35 / 50 | Batch: 261 / 301] loss:   1509.790\n",
            "[Epoch 35 / 50 | Batch: 262 / 301] loss:    976.663\n",
            "[Epoch 35 / 50 | Batch: 263 / 301] loss:   1223.001\n",
            "[Epoch 35 / 50 | Batch: 264 / 301] loss:   1317.677\n",
            "[Epoch 35 / 50 | Batch: 265 / 301] loss:   1673.880\n",
            "[Epoch 35 / 50 | Batch: 266 / 301] loss:   1158.558\n",
            "[Epoch 35 / 50 | Batch: 267 / 301] loss:   1201.894\n",
            "[Epoch 35 / 50 | Batch: 268 / 301] loss:    956.915\n",
            "[Epoch 35 / 50 | Batch: 269 / 301] loss:   1582.484\n",
            "[Epoch 35 / 50 | Batch: 270 / 301] loss:   1649.313\n",
            "[Epoch 35 / 50 | Batch: 271 / 301] loss:   1241.296\n",
            "[Epoch 35 / 50 | Batch: 272 / 301] loss:   1198.015\n",
            "[Epoch 35 / 50 | Batch: 273 / 301] loss:   1210.234\n",
            "[Epoch 35 / 50 | Batch: 274 / 301] loss:   1354.163\n",
            "[Epoch 35 / 50 | Batch: 275 / 301] loss:    812.868\n",
            "[Epoch 35 / 50 | Batch: 276 / 301] loss:    983.946\n",
            "[Epoch 35 / 50 | Batch: 277 / 301] loss:    936.935\n",
            "[Epoch 35 / 50 | Batch: 278 / 301] loss:   1415.632\n",
            "[Epoch 35 / 50 | Batch: 279 / 301] loss:   1296.100\n",
            "[Epoch 35 / 50 | Batch: 280 / 301] loss:    898.639\n",
            "[Epoch 35 / 50 | Batch: 281 / 301] loss:    943.261\n",
            "[Epoch 35 / 50 | Batch: 282 / 301] loss:   1443.886\n",
            "[Epoch 35 / 50 | Batch: 283 / 301] loss:   1179.990\n",
            "[Epoch 35 / 50 | Batch: 284 / 301] loss:   1247.491\n",
            "[Epoch 35 / 50 | Batch: 285 / 301] loss:    929.247\n",
            "[Epoch 35 / 50 | Batch: 286 / 301] loss:    885.207\n",
            "[Epoch 35 / 50 | Batch: 287 / 301] loss:   1605.553\n",
            "[Epoch 35 / 50 | Batch: 288 / 301] loss:   1046.038\n",
            "[Epoch 35 / 50 | Batch: 289 / 301] loss:   1206.563\n",
            "[Epoch 35 / 50 | Batch: 290 / 301] loss:   1173.489\n",
            "[Epoch 35 / 50 | Batch: 291 / 301] loss:    832.000\n",
            "[Epoch 35 / 50 | Batch: 292 / 301] loss:   1265.900\n",
            "[Epoch 35 / 50 | Batch: 293 / 301] loss:   1075.055\n",
            "[Epoch 35 / 50 | Batch: 294 / 301] loss:   1641.682\n",
            "[Epoch 35 / 50 | Batch: 295 / 301] loss:   1279.755\n",
            "[Epoch 35 / 50 | Batch: 296 / 301] loss:   1177.900\n",
            "[Epoch 35 / 50 | Batch: 297 / 301] loss:   1006.861\n",
            "[Epoch 35 / 50 | Batch: 298 / 301] loss:   1099.062\n",
            "[Epoch 35 / 50 | Batch: 299 / 301] loss:    957.448\n",
            "[Epoch 35 / 50 | Batch: 300 / 301] loss:   1236.824\n",
            "[Epoch 35 / 50 | Batch: 301 / 301] loss:    488.012\n",
            "Epoch loss: 1168.17339\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4355.244\n",
            "[Validation] [Batch  2 / 15] dev loss:   6415.637\n",
            "[Validation] [Batch  3 / 15] dev loss:   6338.681\n",
            "[Validation] [Batch  4 / 15] dev loss:   4922.221\n",
            "[Validation] [Batch  5 / 15] dev loss:   5257.156\n",
            "[Validation] [Batch  6 / 15] dev loss:   9245.238\n",
            "[Validation] [Batch  7 / 15] dev loss:   6151.247\n",
            "[Validation] [Batch  8 / 15] dev loss:   4121.978\n",
            "[Validation] [Batch  9 / 15] dev loss:   3271.202\n",
            "[Validation] [Batch 10 / 15] dev loss:  16026.208\n",
            "[Validation] [Batch 11 / 15] dev loss:  13043.185\n",
            "[Validation] [Batch 12 / 15] dev loss:  10499.219\n",
            "[Validation] [Batch 13 / 15] dev loss:   4184.833\n",
            "[Validation] [Batch 14 / 15] dev loss:  21479.543\n",
            "[Validation] [Batch 15 / 15] dev loss:   2317.311\n",
            "Dev loss 7841.92684\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 36 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 36 / 50 | Batch:  1 / 301] loss:    972.565\n",
            "[Epoch 36 / 50 | Batch:  2 / 301] loss:   1234.889\n",
            "[Epoch 36 / 50 | Batch:  3 / 301] loss:    990.751\n",
            "[Epoch 36 / 50 | Batch:  4 / 301] loss:    976.029\n",
            "[Epoch 36 / 50 | Batch:  5 / 301] loss:   1149.965\n",
            "[Epoch 36 / 50 | Batch:  6 / 301] loss:   1161.117\n",
            "[Epoch 36 / 50 | Batch:  7 / 301] loss:   1136.292\n",
            "[Epoch 36 / 50 | Batch:  8 / 301] loss:    809.894\n",
            "[Epoch 36 / 50 | Batch:  9 / 301] loss:   1171.230\n",
            "[Epoch 36 / 50 | Batch: 10 / 301] loss:   1406.310\n",
            "[Epoch 36 / 50 | Batch: 11 / 301] loss:   1471.314\n",
            "[Epoch 36 / 50 | Batch: 12 / 301] loss:    879.265\n",
            "[Epoch 36 / 50 | Batch: 13 / 301] loss:   1062.047\n",
            "[Epoch 36 / 50 | Batch: 14 / 301] loss:   1091.442\n",
            "[Epoch 36 / 50 | Batch: 15 / 301] loss:   1057.181\n",
            "[Epoch 36 / 50 | Batch: 16 / 301] loss:   1224.955\n",
            "[Epoch 36 / 50 | Batch: 17 / 301] loss:   1243.636\n",
            "[Epoch 36 / 50 | Batch: 18 / 301] loss:   1116.850\n",
            "[Epoch 36 / 50 | Batch: 19 / 301] loss:    856.955\n",
            "[Epoch 36 / 50 | Batch: 20 / 301] loss:   1356.797\n",
            "[Epoch 36 / 50 | Batch: 21 / 301] loss:    937.828\n",
            "[Epoch 36 / 50 | Batch: 22 / 301] loss:   1382.023\n",
            "[Epoch 36 / 50 | Batch: 23 / 301] loss:   1343.981\n",
            "[Epoch 36 / 50 | Batch: 24 / 301] loss:    880.225\n",
            "[Epoch 36 / 50 | Batch: 25 / 301] loss:   1357.379\n",
            "[Epoch 36 / 50 | Batch: 26 / 301] loss:    959.949\n",
            "[Epoch 36 / 50 | Batch: 27 / 301] loss:   1261.973\n",
            "[Epoch 36 / 50 | Batch: 28 / 301] loss:   1119.933\n",
            "[Epoch 36 / 50 | Batch: 29 / 301] loss:   1222.897\n",
            "[Epoch 36 / 50 | Batch: 30 / 301] loss:   1123.820\n",
            "[Epoch 36 / 50 | Batch: 31 / 301] loss:   1476.820\n",
            "[Epoch 36 / 50 | Batch: 32 / 301] loss:   1249.340\n",
            "[Epoch 36 / 50 | Batch: 33 / 301] loss:    910.404\n",
            "[Epoch 36 / 50 | Batch: 34 / 301] loss:   1288.001\n",
            "[Epoch 36 / 50 | Batch: 35 / 301] loss:   1346.607\n",
            "[Epoch 36 / 50 | Batch: 36 / 301] loss:   1098.280\n",
            "[Epoch 36 / 50 | Batch: 37 / 301] loss:   1154.970\n",
            "[Epoch 36 / 50 | Batch: 38 / 301] loss:    971.437\n",
            "[Epoch 36 / 50 | Batch: 39 / 301] loss:   1076.150\n",
            "[Epoch 36 / 50 | Batch: 40 / 301] loss:   1191.272\n",
            "[Epoch 36 / 50 | Batch: 41 / 301] loss:    985.926\n",
            "[Epoch 36 / 50 | Batch: 42 / 301] loss:   1049.598\n",
            "[Epoch 36 / 50 | Batch: 43 / 301] loss:   1163.855\n",
            "[Epoch 36 / 50 | Batch: 44 / 301] loss:   1235.415\n",
            "[Epoch 36 / 50 | Batch: 45 / 301] loss:   1212.277\n",
            "[Epoch 36 / 50 | Batch: 46 / 301] loss:    818.042\n",
            "[Epoch 36 / 50 | Batch: 47 / 301] loss:   1673.149\n",
            "[Epoch 36 / 50 | Batch: 48 / 301] loss:   1019.753\n",
            "[Epoch 36 / 50 | Batch: 49 / 301] loss:   1223.964\n",
            "[Epoch 36 / 50 | Batch: 50 / 301] loss:   1260.464\n",
            "[Epoch 36 / 50 | Batch: 51 / 301] loss:   1527.106\n",
            "[Epoch 36 / 50 | Batch: 52 / 301] loss:    975.351\n",
            "[Epoch 36 / 50 | Batch: 53 / 301] loss:   1308.670\n",
            "[Epoch 36 / 50 | Batch: 54 / 301] loss:    971.307\n",
            "[Epoch 36 / 50 | Batch: 55 / 301] loss:   1199.165\n",
            "[Epoch 36 / 50 | Batch: 56 / 301] loss:    788.095\n",
            "[Epoch 36 / 50 | Batch: 57 / 301] loss:   1066.836\n",
            "[Epoch 36 / 50 | Batch: 58 / 301] loss:   1319.104\n",
            "[Epoch 36 / 50 | Batch: 59 / 301] loss:    940.178\n",
            "[Epoch 36 / 50 | Batch: 60 / 301] loss:   1161.859\n",
            "[Epoch 36 / 50 | Batch: 61 / 301] loss:   1240.849\n",
            "[Epoch 36 / 50 | Batch: 62 / 301] loss:   1241.342\n",
            "[Epoch 36 / 50 | Batch: 63 / 301] loss:   1032.329\n",
            "[Epoch 36 / 50 | Batch: 64 / 301] loss:   1681.545\n",
            "[Epoch 36 / 50 | Batch: 65 / 301] loss:   1163.120\n",
            "[Epoch 36 / 50 | Batch: 66 / 301] loss:   1166.367\n",
            "[Epoch 36 / 50 | Batch: 67 / 301] loss:   1261.496\n",
            "[Epoch 36 / 50 | Batch: 68 / 301] loss:   1270.987\n",
            "[Epoch 36 / 50 | Batch: 69 / 301] loss:   1290.069\n",
            "[Epoch 36 / 50 | Batch: 70 / 301] loss:   1145.799\n",
            "[Epoch 36 / 50 | Batch: 71 / 301] loss:    965.800\n",
            "[Epoch 36 / 50 | Batch: 72 / 301] loss:   1204.170\n",
            "[Epoch 36 / 50 | Batch: 73 / 301] loss:    988.411\n",
            "[Epoch 36 / 50 | Batch: 74 / 301] loss:   1234.597\n",
            "[Epoch 36 / 50 | Batch: 75 / 301] loss:   1012.193\n",
            "[Epoch 36 / 50 | Batch: 76 / 301] loss:   1248.829\n",
            "[Epoch 36 / 50 | Batch: 77 / 301] loss:   1105.301\n",
            "[Epoch 36 / 50 | Batch: 78 / 301] loss:    997.250\n",
            "[Epoch 36 / 50 | Batch: 79 / 301] loss:   1225.289\n",
            "[Epoch 36 / 50 | Batch: 80 / 301] loss:   1186.210\n",
            "[Epoch 36 / 50 | Batch: 81 / 301] loss:   1163.247\n",
            "[Epoch 36 / 50 | Batch: 82 / 301] loss:   1222.977\n",
            "[Epoch 36 / 50 | Batch: 83 / 301] loss:   1151.765\n",
            "[Epoch 36 / 50 | Batch: 84 / 301] loss:   1287.415\n",
            "[Epoch 36 / 50 | Batch: 85 / 301] loss:   1209.296\n",
            "[Epoch 36 / 50 | Batch: 86 / 301] loss:   1300.193\n",
            "[Epoch 36 / 50 | Batch: 87 / 301] loss:   1171.337\n",
            "[Epoch 36 / 50 | Batch: 88 / 301] loss:   1188.894\n",
            "[Epoch 36 / 50 | Batch: 89 / 301] loss:   1299.521\n",
            "[Epoch 36 / 50 | Batch: 90 / 301] loss:   1422.677\n",
            "[Epoch 36 / 50 | Batch: 91 / 301] loss:   1033.920\n",
            "[Epoch 36 / 50 | Batch: 92 / 301] loss:    982.828\n",
            "[Epoch 36 / 50 | Batch: 93 / 301] loss:    836.074\n",
            "[Epoch 36 / 50 | Batch: 94 / 301] loss:   1394.727\n",
            "[Epoch 36 / 50 | Batch: 95 / 301] loss:   1273.640\n",
            "[Epoch 36 / 50 | Batch: 96 / 301] loss:   1393.846\n",
            "[Epoch 36 / 50 | Batch: 97 / 301] loss:    863.106\n",
            "[Epoch 36 / 50 | Batch: 98 / 301] loss:   1314.733\n",
            "[Epoch 36 / 50 | Batch: 99 / 301] loss:   1493.757\n",
            "[Epoch 36 / 50 | Batch: 100 / 301] loss:   1183.742\n",
            "[Epoch 36 / 50 | Batch: 101 / 301] loss:   1863.138\n",
            "[Epoch 36 / 50 | Batch: 102 / 301] loss:    976.375\n",
            "[Epoch 36 / 50 | Batch: 103 / 301] loss:    982.466\n",
            "[Epoch 36 / 50 | Batch: 104 / 301] loss:    801.353\n",
            "[Epoch 36 / 50 | Batch: 105 / 301] loss:   1204.491\n",
            "[Epoch 36 / 50 | Batch: 106 / 301] loss:   1137.921\n",
            "[Epoch 36 / 50 | Batch: 107 / 301] loss:   1006.878\n",
            "[Epoch 36 / 50 | Batch: 108 / 301] loss:   1142.897\n",
            "[Epoch 36 / 50 | Batch: 109 / 301] loss:   1423.078\n",
            "[Epoch 36 / 50 | Batch: 110 / 301] loss:   1256.354\n",
            "[Epoch 36 / 50 | Batch: 111 / 301] loss:   1140.560\n",
            "[Epoch 36 / 50 | Batch: 112 / 301] loss:   1009.009\n",
            "[Epoch 36 / 50 | Batch: 113 / 301] loss:    817.489\n",
            "[Epoch 36 / 50 | Batch: 114 / 301] loss:   1372.936\n",
            "[Epoch 36 / 50 | Batch: 115 / 301] loss:   1271.117\n",
            "[Epoch 36 / 50 | Batch: 116 / 301] loss:    943.429\n",
            "[Epoch 36 / 50 | Batch: 117 / 301] loss:   1178.425\n",
            "[Epoch 36 / 50 | Batch: 118 / 301] loss:   1496.965\n",
            "[Epoch 36 / 50 | Batch: 119 / 301] loss:   1235.215\n",
            "[Epoch 36 / 50 | Batch: 120 / 301] loss:   1092.300\n",
            "[Epoch 36 / 50 | Batch: 121 / 301] loss:   1393.485\n",
            "[Epoch 36 / 50 | Batch: 122 / 301] loss:   1508.817\n",
            "[Epoch 36 / 50 | Batch: 123 / 301] loss:   1411.083\n",
            "[Epoch 36 / 50 | Batch: 124 / 301] loss:   1383.757\n",
            "[Epoch 36 / 50 | Batch: 125 / 301] loss:    959.311\n",
            "[Epoch 36 / 50 | Batch: 126 / 301] loss:   1068.034\n",
            "[Epoch 36 / 50 | Batch: 127 / 301] loss:   1113.235\n",
            "[Epoch 36 / 50 | Batch: 128 / 301] loss:   1584.281\n",
            "[Epoch 36 / 50 | Batch: 129 / 301] loss:   1136.510\n",
            "[Epoch 36 / 50 | Batch: 130 / 301] loss:   1141.739\n",
            "[Epoch 36 / 50 | Batch: 131 / 301] loss:   1152.960\n",
            "[Epoch 36 / 50 | Batch: 132 / 301] loss:   1023.680\n",
            "[Epoch 36 / 50 | Batch: 133 / 301] loss:   1440.627\n",
            "[Epoch 36 / 50 | Batch: 134 / 301] loss:    943.268\n",
            "[Epoch 36 / 50 | Batch: 135 / 301] loss:   1652.606\n",
            "[Epoch 36 / 50 | Batch: 136 / 301] loss:   1342.432\n",
            "[Epoch 36 / 50 | Batch: 137 / 301] loss:   1000.280\n",
            "[Epoch 36 / 50 | Batch: 138 / 301] loss:   1211.285\n",
            "[Epoch 36 / 50 | Batch: 139 / 301] loss:   1356.575\n",
            "[Epoch 36 / 50 | Batch: 140 / 301] loss:   1192.265\n",
            "[Epoch 36 / 50 | Batch: 141 / 301] loss:   1429.424\n",
            "[Epoch 36 / 50 | Batch: 142 / 301] loss:   1317.238\n",
            "[Epoch 36 / 50 | Batch: 143 / 301] loss:   1374.661\n",
            "[Epoch 36 / 50 | Batch: 144 / 301] loss:   1029.728\n",
            "[Epoch 36 / 50 | Batch: 145 / 301] loss:   1059.066\n",
            "[Epoch 36 / 50 | Batch: 146 / 301] loss:   1461.359\n",
            "[Epoch 36 / 50 | Batch: 147 / 301] loss:    968.131\n",
            "[Epoch 36 / 50 | Batch: 148 / 301] loss:   1216.517\n",
            "[Epoch 36 / 50 | Batch: 149 / 301] loss:   1127.792\n",
            "[Epoch 36 / 50 | Batch: 150 / 301] loss:   1528.616\n",
            "[Epoch 36 / 50 | Batch: 151 / 301] loss:   1208.240\n",
            "[Epoch 36 / 50 | Batch: 152 / 301] loss:    779.255\n",
            "[Epoch 36 / 50 | Batch: 153 / 301] loss:    789.237\n",
            "[Epoch 36 / 50 | Batch: 154 / 301] loss:   1134.678\n",
            "[Epoch 36 / 50 | Batch: 155 / 301] loss:   1190.089\n",
            "[Epoch 36 / 50 | Batch: 156 / 301] loss:    966.519\n",
            "[Epoch 36 / 50 | Batch: 157 / 301] loss:   1029.156\n",
            "[Epoch 36 / 50 | Batch: 158 / 301] loss:   1278.780\n",
            "[Epoch 36 / 50 | Batch: 159 / 301] loss:   1019.341\n",
            "[Epoch 36 / 50 | Batch: 160 / 301] loss:   1151.827\n",
            "[Epoch 36 / 50 | Batch: 161 / 301] loss:   1069.541\n",
            "[Epoch 36 / 50 | Batch: 162 / 301] loss:   1136.506\n",
            "[Epoch 36 / 50 | Batch: 163 / 301] loss:   1128.306\n",
            "[Epoch 36 / 50 | Batch: 164 / 301] loss:   1112.254\n",
            "[Epoch 36 / 50 | Batch: 165 / 301] loss:   1130.035\n",
            "[Epoch 36 / 50 | Batch: 166 / 301] loss:   1483.897\n",
            "[Epoch 36 / 50 | Batch: 167 / 301] loss:    854.832\n",
            "[Epoch 36 / 50 | Batch: 168 / 301] loss:    794.077\n",
            "[Epoch 36 / 50 | Batch: 169 / 301] loss:    921.240\n",
            "[Epoch 36 / 50 | Batch: 170 / 301] loss:   1227.541\n",
            "[Epoch 36 / 50 | Batch: 171 / 301] loss:   1301.014\n",
            "[Epoch 36 / 50 | Batch: 172 / 301] loss:    982.369\n",
            "[Epoch 36 / 50 | Batch: 173 / 301] loss:   1033.690\n",
            "[Epoch 36 / 50 | Batch: 174 / 301] loss:   1257.269\n",
            "[Epoch 36 / 50 | Batch: 175 / 301] loss:    988.792\n",
            "[Epoch 36 / 50 | Batch: 176 / 301] loss:   1232.746\n",
            "[Epoch 36 / 50 | Batch: 177 / 301] loss:   1050.287\n",
            "[Epoch 36 / 50 | Batch: 178 / 301] loss:    991.750\n",
            "[Epoch 36 / 50 | Batch: 179 / 301] loss:    989.143\n",
            "[Epoch 36 / 50 | Batch: 180 / 301] loss:   1174.619\n",
            "[Epoch 36 / 50 | Batch: 181 / 301] loss:   1040.661\n",
            "[Epoch 36 / 50 | Batch: 182 / 301] loss:   1213.448\n",
            "[Epoch 36 / 50 | Batch: 183 / 301] loss:   1368.248\n",
            "[Epoch 36 / 50 | Batch: 184 / 301] loss:   1204.539\n",
            "[Epoch 36 / 50 | Batch: 185 / 301] loss:   1427.691\n",
            "[Epoch 36 / 50 | Batch: 186 / 301] loss:    951.320\n",
            "[Epoch 36 / 50 | Batch: 187 / 301] loss:    951.684\n",
            "[Epoch 36 / 50 | Batch: 188 / 301] loss:   1275.426\n",
            "[Epoch 36 / 50 | Batch: 189 / 301] loss:    989.119\n",
            "[Epoch 36 / 50 | Batch: 190 / 301] loss:   1091.579\n",
            "[Epoch 36 / 50 | Batch: 191 / 301] loss:   1392.895\n",
            "[Epoch 36 / 50 | Batch: 192 / 301] loss:   1211.217\n",
            "[Epoch 36 / 50 | Batch: 193 / 301] loss:   1145.929\n",
            "[Epoch 36 / 50 | Batch: 194 / 301] loss:   1344.740\n",
            "[Epoch 36 / 50 | Batch: 195 / 301] loss:   1140.469\n",
            "[Epoch 36 / 50 | Batch: 196 / 301] loss:   1246.261\n",
            "[Epoch 36 / 50 | Batch: 197 / 301] loss:    973.718\n",
            "[Epoch 36 / 50 | Batch: 198 / 301] loss:    994.928\n",
            "[Epoch 36 / 50 | Batch: 199 / 301] loss:   1191.556\n",
            "[Epoch 36 / 50 | Batch: 200 / 301] loss:   1005.771\n",
            "[Epoch 36 / 50 | Batch: 201 / 301] loss:    768.598\n",
            "[Epoch 36 / 50 | Batch: 202 / 301] loss:   1416.533\n",
            "[Epoch 36 / 50 | Batch: 203 / 301] loss:   1226.461\n",
            "[Epoch 36 / 50 | Batch: 204 / 301] loss:   1156.346\n",
            "[Epoch 36 / 50 | Batch: 205 / 301] loss:   1149.605\n",
            "[Epoch 36 / 50 | Batch: 206 / 301] loss:   1100.499\n",
            "[Epoch 36 / 50 | Batch: 207 / 301] loss:   1241.124\n",
            "[Epoch 36 / 50 | Batch: 208 / 301] loss:    882.358\n",
            "[Epoch 36 / 50 | Batch: 209 / 301] loss:   1123.101\n",
            "[Epoch 36 / 50 | Batch: 210 / 301] loss:   1365.370\n",
            "[Epoch 36 / 50 | Batch: 211 / 301] loss:   1087.878\n",
            "[Epoch 36 / 50 | Batch: 212 / 301] loss:   1299.907\n",
            "[Epoch 36 / 50 | Batch: 213 / 301] loss:   1035.819\n",
            "[Epoch 36 / 50 | Batch: 214 / 301] loss:    972.165\n",
            "[Epoch 36 / 50 | Batch: 215 / 301] loss:   1167.727\n",
            "[Epoch 36 / 50 | Batch: 216 / 301] loss:   1369.250\n",
            "[Epoch 36 / 50 | Batch: 217 / 301] loss:   1544.179\n",
            "[Epoch 36 / 50 | Batch: 218 / 301] loss:   1427.334\n",
            "[Epoch 36 / 50 | Batch: 219 / 301] loss:   1071.977\n",
            "[Epoch 36 / 50 | Batch: 220 / 301] loss:   1264.954\n",
            "[Epoch 36 / 50 | Batch: 221 / 301] loss:   1483.760\n",
            "[Epoch 36 / 50 | Batch: 222 / 301] loss:   1357.614\n",
            "[Epoch 36 / 50 | Batch: 223 / 301] loss:   1137.825\n",
            "[Epoch 36 / 50 | Batch: 224 / 301] loss:   1473.974\n",
            "[Epoch 36 / 50 | Batch: 225 / 301] loss:   1129.223\n",
            "[Epoch 36 / 50 | Batch: 226 / 301] loss:   1233.792\n",
            "[Epoch 36 / 50 | Batch: 227 / 301] loss:    918.197\n",
            "[Epoch 36 / 50 | Batch: 228 / 301] loss:    821.929\n",
            "[Epoch 36 / 50 | Batch: 229 / 301] loss:   1034.277\n",
            "[Epoch 36 / 50 | Batch: 230 / 301] loss:   1221.850\n",
            "[Epoch 36 / 50 | Batch: 231 / 301] loss:   1117.364\n",
            "[Epoch 36 / 50 | Batch: 232 / 301] loss:   1291.638\n",
            "[Epoch 36 / 50 | Batch: 233 / 301] loss:   1567.113\n",
            "[Epoch 36 / 50 | Batch: 234 / 301] loss:    965.728\n",
            "[Epoch 36 / 50 | Batch: 235 / 301] loss:   1118.637\n",
            "[Epoch 36 / 50 | Batch: 236 / 301] loss:   1488.998\n",
            "[Epoch 36 / 50 | Batch: 237 / 301] loss:   1301.067\n",
            "[Epoch 36 / 50 | Batch: 238 / 301] loss:   1026.572\n",
            "[Epoch 36 / 50 | Batch: 239 / 301] loss:   1149.550\n",
            "[Epoch 36 / 50 | Batch: 240 / 301] loss:   1292.970\n",
            "[Epoch 36 / 50 | Batch: 241 / 301] loss:   1329.679\n",
            "[Epoch 36 / 50 | Batch: 242 / 301] loss:   1455.330\n",
            "[Epoch 36 / 50 | Batch: 243 / 301] loss:   1075.451\n",
            "[Epoch 36 / 50 | Batch: 244 / 301] loss:   1498.512\n",
            "[Epoch 36 / 50 | Batch: 245 / 301] loss:   1272.691\n",
            "[Epoch 36 / 50 | Batch: 246 / 301] loss:   1310.163\n",
            "[Epoch 36 / 50 | Batch: 247 / 301] loss:   1147.753\n",
            "[Epoch 36 / 50 | Batch: 248 / 301] loss:   1061.589\n",
            "[Epoch 36 / 50 | Batch: 249 / 301] loss:   1163.706\n",
            "[Epoch 36 / 50 | Batch: 250 / 301] loss:   1071.984\n",
            "[Epoch 36 / 50 | Batch: 251 / 301] loss:   1044.567\n",
            "[Epoch 36 / 50 | Batch: 252 / 301] loss:   1122.451\n",
            "[Epoch 36 / 50 | Batch: 253 / 301] loss:   1649.933\n",
            "[Epoch 36 / 50 | Batch: 254 / 301] loss:   1273.088\n",
            "[Epoch 36 / 50 | Batch: 255 / 301] loss:    858.015\n",
            "[Epoch 36 / 50 | Batch: 256 / 301] loss:   1266.462\n",
            "[Epoch 36 / 50 | Batch: 257 / 301] loss:   1184.688\n",
            "[Epoch 36 / 50 | Batch: 258 / 301] loss:    835.527\n",
            "[Epoch 36 / 50 | Batch: 259 / 301] loss:   1284.243\n",
            "[Epoch 36 / 50 | Batch: 260 / 301] loss:   1018.478\n",
            "[Epoch 36 / 50 | Batch: 261 / 301] loss:   1050.419\n",
            "[Epoch 36 / 50 | Batch: 262 / 301] loss:   1252.623\n",
            "[Epoch 36 / 50 | Batch: 263 / 301] loss:   1110.270\n",
            "[Epoch 36 / 50 | Batch: 264 / 301] loss:   1365.008\n",
            "[Epoch 36 / 50 | Batch: 265 / 301] loss:   1306.907\n",
            "[Epoch 36 / 50 | Batch: 266 / 301] loss:   1049.345\n",
            "[Epoch 36 / 50 | Batch: 267 / 301] loss:   1257.399\n",
            "[Epoch 36 / 50 | Batch: 268 / 301] loss:   1404.315\n",
            "[Epoch 36 / 50 | Batch: 269 / 301] loss:   1360.702\n",
            "[Epoch 36 / 50 | Batch: 270 / 301] loss:   1110.927\n",
            "[Epoch 36 / 50 | Batch: 271 / 301] loss:   1166.453\n",
            "[Epoch 36 / 50 | Batch: 272 / 301] loss:    984.739\n",
            "[Epoch 36 / 50 | Batch: 273 / 301] loss:   1235.469\n",
            "[Epoch 36 / 50 | Batch: 274 / 301] loss:   1221.071\n",
            "[Epoch 36 / 50 | Batch: 275 / 301] loss:   1140.885\n",
            "[Epoch 36 / 50 | Batch: 276 / 301] loss:   1210.629\n",
            "[Epoch 36 / 50 | Batch: 277 / 301] loss:    874.864\n",
            "[Epoch 36 / 50 | Batch: 278 / 301] loss:   1461.230\n",
            "[Epoch 36 / 50 | Batch: 279 / 301] loss:   1385.712\n",
            "[Epoch 36 / 50 | Batch: 280 / 301] loss:   1203.056\n",
            "[Epoch 36 / 50 | Batch: 281 / 301] loss:   1040.326\n",
            "[Epoch 36 / 50 | Batch: 282 / 301] loss:    972.271\n",
            "[Epoch 36 / 50 | Batch: 283 / 301] loss:   1314.030\n",
            "[Epoch 36 / 50 | Batch: 284 / 301] loss:   1468.974\n",
            "[Epoch 36 / 50 | Batch: 285 / 301] loss:   1005.641\n",
            "[Epoch 36 / 50 | Batch: 286 / 301] loss:   1375.933\n",
            "[Epoch 36 / 50 | Batch: 287 / 301] loss:    966.235\n",
            "[Epoch 36 / 50 | Batch: 288 / 301] loss:   1005.535\n",
            "[Epoch 36 / 50 | Batch: 289 / 301] loss:   1230.241\n",
            "[Epoch 36 / 50 | Batch: 290 / 301] loss:    982.592\n",
            "[Epoch 36 / 50 | Batch: 291 / 301] loss:   1070.093\n",
            "[Epoch 36 / 50 | Batch: 292 / 301] loss:   1176.830\n",
            "[Epoch 36 / 50 | Batch: 293 / 301] loss:   1024.645\n",
            "[Epoch 36 / 50 | Batch: 294 / 301] loss:   1233.586\n",
            "[Epoch 36 / 50 | Batch: 295 / 301] loss:   1189.359\n",
            "[Epoch 36 / 50 | Batch: 296 / 301] loss:    963.854\n",
            "[Epoch 36 / 50 | Batch: 297 / 301] loss:    682.772\n",
            "[Epoch 36 / 50 | Batch: 298 / 301] loss:   1318.778\n",
            "[Epoch 36 / 50 | Batch: 299 / 301] loss:   1317.542\n",
            "[Epoch 36 / 50 | Batch: 300 / 301] loss:    846.428\n",
            "[Epoch 36 / 50 | Batch: 301 / 301] loss:    611.977\n",
            "Epoch loss: 1168.10351\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4458.570\n",
            "[Validation] [Batch  2 / 15] dev loss:   6497.501\n",
            "[Validation] [Batch  3 / 15] dev loss:   6827.713\n",
            "[Validation] [Batch  4 / 15] dev loss:   5419.772\n",
            "[Validation] [Batch  5 / 15] dev loss:   5339.370\n",
            "[Validation] [Batch  6 / 15] dev loss:   8788.336\n",
            "[Validation] [Batch  7 / 15] dev loss:   6762.114\n",
            "[Validation] [Batch  8 / 15] dev loss:   4495.250\n",
            "[Validation] [Batch  9 / 15] dev loss:   3566.498\n",
            "[Validation] [Batch 10 / 15] dev loss:  16363.051\n",
            "[Validation] [Batch 11 / 15] dev loss:  13079.679\n",
            "[Validation] [Batch 12 / 15] dev loss:  10994.742\n",
            "[Validation] [Batch 13 / 15] dev loss:   4623.745\n",
            "[Validation] [Batch 14 / 15] dev loss:  23487.162\n",
            "[Validation] [Batch 15 / 15] dev loss:   1950.457\n",
            "Dev loss 8176.93065\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 37 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 37 / 50 | Batch:  1 / 301] loss:    851.318\n",
            "[Epoch 37 / 50 | Batch:  2 / 301] loss:   1052.303\n",
            "[Epoch 37 / 50 | Batch:  3 / 301] loss:   1246.104\n",
            "[Epoch 37 / 50 | Batch:  4 / 301] loss:   1157.052\n",
            "[Epoch 37 / 50 | Batch:  5 / 301] loss:   1465.868\n",
            "[Epoch 37 / 50 | Batch:  6 / 301] loss:   1250.981\n",
            "[Epoch 37 / 50 | Batch:  7 / 301] loss:   1014.885\n",
            "[Epoch 37 / 50 | Batch:  8 / 301] loss:    953.376\n",
            "[Epoch 37 / 50 | Batch:  9 / 301] loss:    990.451\n",
            "[Epoch 37 / 50 | Batch: 10 / 301] loss:   1048.009\n",
            "[Epoch 37 / 50 | Batch: 11 / 301] loss:    915.611\n",
            "[Epoch 37 / 50 | Batch: 12 / 301] loss:   1251.589\n",
            "[Epoch 37 / 50 | Batch: 13 / 301] loss:   1306.852\n",
            "[Epoch 37 / 50 | Batch: 14 / 301] loss:    978.217\n",
            "[Epoch 37 / 50 | Batch: 15 / 301] loss:   1260.501\n",
            "[Epoch 37 / 50 | Batch: 16 / 301] loss:   1418.970\n",
            "[Epoch 37 / 50 | Batch: 17 / 301] loss:   1360.266\n",
            "[Epoch 37 / 50 | Batch: 18 / 301] loss:   1335.493\n",
            "[Epoch 37 / 50 | Batch: 19 / 301] loss:   1461.073\n",
            "[Epoch 37 / 50 | Batch: 20 / 301] loss:   1239.665\n",
            "[Epoch 37 / 50 | Batch: 21 / 301] loss:   1194.957\n",
            "[Epoch 37 / 50 | Batch: 22 / 301] loss:   1001.969\n",
            "[Epoch 37 / 50 | Batch: 23 / 301] loss:    821.579\n",
            "[Epoch 37 / 50 | Batch: 24 / 301] loss:   1236.796\n",
            "[Epoch 37 / 50 | Batch: 25 / 301] loss:   1042.398\n",
            "[Epoch 37 / 50 | Batch: 26 / 301] loss:   1434.080\n",
            "[Epoch 37 / 50 | Batch: 27 / 301] loss:   1155.628\n",
            "[Epoch 37 / 50 | Batch: 28 / 301] loss:   1027.814\n",
            "[Epoch 37 / 50 | Batch: 29 / 301] loss:    752.711\n",
            "[Epoch 37 / 50 | Batch: 30 / 301] loss:   1230.244\n",
            "[Epoch 37 / 50 | Batch: 31 / 301] loss:   1079.375\n",
            "[Epoch 37 / 50 | Batch: 32 / 301] loss:   1213.528\n",
            "[Epoch 37 / 50 | Batch: 33 / 301] loss:    850.828\n",
            "[Epoch 37 / 50 | Batch: 34 / 301] loss:   1304.676\n",
            "[Epoch 37 / 50 | Batch: 35 / 301] loss:    923.201\n",
            "[Epoch 37 / 50 | Batch: 36 / 301] loss:   1172.707\n",
            "[Epoch 37 / 50 | Batch: 37 / 301] loss:   1400.471\n",
            "[Epoch 37 / 50 | Batch: 38 / 301] loss:   1314.835\n",
            "[Epoch 37 / 50 | Batch: 39 / 301] loss:   1248.324\n",
            "[Epoch 37 / 50 | Batch: 40 / 301] loss:   1061.039\n",
            "[Epoch 37 / 50 | Batch: 41 / 301] loss:   1367.328\n",
            "[Epoch 37 / 50 | Batch: 42 / 301] loss:    980.144\n",
            "[Epoch 37 / 50 | Batch: 43 / 301] loss:   1155.311\n",
            "[Epoch 37 / 50 | Batch: 44 / 301] loss:   1187.360\n",
            "[Epoch 37 / 50 | Batch: 45 / 301] loss:   1420.059\n",
            "[Epoch 37 / 50 | Batch: 46 / 301] loss:   1384.728\n",
            "[Epoch 37 / 50 | Batch: 47 / 301] loss:   1214.090\n",
            "[Epoch 37 / 50 | Batch: 48 / 301] loss:   1517.366\n",
            "[Epoch 37 / 50 | Batch: 49 / 301] loss:   1077.963\n",
            "[Epoch 37 / 50 | Batch: 50 / 301] loss:   1276.679\n",
            "[Epoch 37 / 50 | Batch: 51 / 301] loss:   1105.550\n",
            "[Epoch 37 / 50 | Batch: 52 / 301] loss:   1171.465\n",
            "[Epoch 37 / 50 | Batch: 53 / 301] loss:   1167.480\n",
            "[Epoch 37 / 50 | Batch: 54 / 301] loss:   1050.406\n",
            "[Epoch 37 / 50 | Batch: 55 / 301] loss:   1234.139\n",
            "[Epoch 37 / 50 | Batch: 56 / 301] loss:    836.650\n",
            "[Epoch 37 / 50 | Batch: 57 / 301] loss:   1227.600\n",
            "[Epoch 37 / 50 | Batch: 58 / 301] loss:   1219.743\n",
            "[Epoch 37 / 50 | Batch: 59 / 301] loss:   1028.693\n",
            "[Epoch 37 / 50 | Batch: 60 / 301] loss:   1243.724\n",
            "[Epoch 37 / 50 | Batch: 61 / 301] loss:    882.070\n",
            "[Epoch 37 / 50 | Batch: 62 / 301] loss:   1074.653\n",
            "[Epoch 37 / 50 | Batch: 63 / 301] loss:   1179.285\n",
            "[Epoch 37 / 50 | Batch: 64 / 301] loss:   1327.955\n",
            "[Epoch 37 / 50 | Batch: 65 / 301] loss:   1229.482\n",
            "[Epoch 37 / 50 | Batch: 66 / 301] loss:   1252.127\n",
            "[Epoch 37 / 50 | Batch: 67 / 301] loss:   1322.799\n",
            "[Epoch 37 / 50 | Batch: 68 / 301] loss:   1785.066\n",
            "[Epoch 37 / 50 | Batch: 69 / 301] loss:    827.842\n",
            "[Epoch 37 / 50 | Batch: 70 / 301] loss:   1378.011\n",
            "[Epoch 37 / 50 | Batch: 71 / 301] loss:   1413.237\n",
            "[Epoch 37 / 50 | Batch: 72 / 301] loss:   1251.807\n",
            "[Epoch 37 / 50 | Batch: 73 / 301] loss:   1124.667\n",
            "[Epoch 37 / 50 | Batch: 74 / 301] loss:    949.203\n",
            "[Epoch 37 / 50 | Batch: 75 / 301] loss:   1787.169\n",
            "[Epoch 37 / 50 | Batch: 76 / 301] loss:    926.912\n",
            "[Epoch 37 / 50 | Batch: 77 / 301] loss:   1003.296\n",
            "[Epoch 37 / 50 | Batch: 78 / 301] loss:   1039.510\n",
            "[Epoch 37 / 50 | Batch: 79 / 301] loss:   1106.014\n",
            "[Epoch 37 / 50 | Batch: 80 / 301] loss:   1156.901\n",
            "[Epoch 37 / 50 | Batch: 81 / 301] loss:   1164.434\n",
            "[Epoch 37 / 50 | Batch: 82 / 301] loss:   1371.807\n",
            "[Epoch 37 / 50 | Batch: 83 / 301] loss:   1233.277\n",
            "[Epoch 37 / 50 | Batch: 84 / 301] loss:   1060.406\n",
            "[Epoch 37 / 50 | Batch: 85 / 301] loss:   1053.394\n",
            "[Epoch 37 / 50 | Batch: 86 / 301] loss:   1412.612\n",
            "[Epoch 37 / 50 | Batch: 87 / 301] loss:   1179.466\n",
            "[Epoch 37 / 50 | Batch: 88 / 301] loss:    971.337\n",
            "[Epoch 37 / 50 | Batch: 89 / 301] loss:    879.736\n",
            "[Epoch 37 / 50 | Batch: 90 / 301] loss:   1179.570\n",
            "[Epoch 37 / 50 | Batch: 91 / 301] loss:   1047.362\n",
            "[Epoch 37 / 50 | Batch: 92 / 301] loss:   1468.558\n",
            "[Epoch 37 / 50 | Batch: 93 / 301] loss:   1152.780\n",
            "[Epoch 37 / 50 | Batch: 94 / 301] loss:   1356.274\n",
            "[Epoch 37 / 50 | Batch: 95 / 301] loss:   1653.507\n",
            "[Epoch 37 / 50 | Batch: 96 / 301] loss:   1500.224\n",
            "[Epoch 37 / 50 | Batch: 97 / 301] loss:   1184.785\n",
            "[Epoch 37 / 50 | Batch: 98 / 301] loss:   1009.458\n",
            "[Epoch 37 / 50 | Batch: 99 / 301] loss:   1252.244\n",
            "[Epoch 37 / 50 | Batch: 100 / 301] loss:   1423.213\n",
            "[Epoch 37 / 50 | Batch: 101 / 301] loss:   1244.434\n",
            "[Epoch 37 / 50 | Batch: 102 / 301] loss:   1057.884\n",
            "[Epoch 37 / 50 | Batch: 103 / 301] loss:   1197.885\n",
            "[Epoch 37 / 50 | Batch: 104 / 301] loss:   1029.479\n",
            "[Epoch 37 / 50 | Batch: 105 / 301] loss:   1243.992\n",
            "[Epoch 37 / 50 | Batch: 106 / 301] loss:   1234.858\n",
            "[Epoch 37 / 50 | Batch: 107 / 301] loss:    975.069\n",
            "[Epoch 37 / 50 | Batch: 108 / 301] loss:    940.765\n",
            "[Epoch 37 / 50 | Batch: 109 / 301] loss:   1123.645\n",
            "[Epoch 37 / 50 | Batch: 110 / 301] loss:   1325.585\n",
            "[Epoch 37 / 50 | Batch: 111 / 301] loss:   1180.856\n",
            "[Epoch 37 / 50 | Batch: 112 / 301] loss:   1343.462\n",
            "[Epoch 37 / 50 | Batch: 113 / 301] loss:   1393.267\n",
            "[Epoch 37 / 50 | Batch: 114 / 301] loss:   1384.222\n",
            "[Epoch 37 / 50 | Batch: 115 / 301] loss:   1156.350\n",
            "[Epoch 37 / 50 | Batch: 116 / 301] loss:    752.967\n",
            "[Epoch 37 / 50 | Batch: 117 / 301] loss:   1209.282\n",
            "[Epoch 37 / 50 | Batch: 118 / 301] loss:   1736.133\n",
            "[Epoch 37 / 50 | Batch: 119 / 301] loss:   1079.854\n",
            "[Epoch 37 / 50 | Batch: 120 / 301] loss:   1132.373\n",
            "[Epoch 37 / 50 | Batch: 121 / 301] loss:   1416.786\n",
            "[Epoch 37 / 50 | Batch: 122 / 301] loss:   1043.977\n",
            "[Epoch 37 / 50 | Batch: 123 / 301] loss:   1037.914\n",
            "[Epoch 37 / 50 | Batch: 124 / 301] loss:   1155.353\n",
            "[Epoch 37 / 50 | Batch: 125 / 301] loss:   1181.176\n",
            "[Epoch 37 / 50 | Batch: 126 / 301] loss:    997.470\n",
            "[Epoch 37 / 50 | Batch: 127 / 301] loss:   1085.543\n",
            "[Epoch 37 / 50 | Batch: 128 / 301] loss:   1177.796\n",
            "[Epoch 37 / 50 | Batch: 129 / 301] loss:   1111.930\n",
            "[Epoch 37 / 50 | Batch: 130 / 301] loss:    863.281\n",
            "[Epoch 37 / 50 | Batch: 131 / 301] loss:    949.919\n",
            "[Epoch 37 / 50 | Batch: 132 / 301] loss:   1407.449\n",
            "[Epoch 37 / 50 | Batch: 133 / 301] loss:   1114.703\n",
            "[Epoch 37 / 50 | Batch: 134 / 301] loss:   1494.567\n",
            "[Epoch 37 / 50 | Batch: 135 / 301] loss:   1106.717\n",
            "[Epoch 37 / 50 | Batch: 136 / 301] loss:   1313.804\n",
            "[Epoch 37 / 50 | Batch: 137 / 301] loss:   1129.347\n",
            "[Epoch 37 / 50 | Batch: 138 / 301] loss:   1249.884\n",
            "[Epoch 37 / 50 | Batch: 139 / 301] loss:   1164.271\n",
            "[Epoch 37 / 50 | Batch: 140 / 301] loss:    988.832\n",
            "[Epoch 37 / 50 | Batch: 141 / 301] loss:   1213.856\n",
            "[Epoch 37 / 50 | Batch: 142 / 301] loss:   1349.968\n",
            "[Epoch 37 / 50 | Batch: 143 / 301] loss:   1138.463\n",
            "[Epoch 37 / 50 | Batch: 144 / 301] loss:   1354.014\n",
            "[Epoch 37 / 50 | Batch: 145 / 301] loss:    946.482\n",
            "[Epoch 37 / 50 | Batch: 146 / 301] loss:   1113.882\n",
            "[Epoch 37 / 50 | Batch: 147 / 301] loss:   1137.692\n",
            "[Epoch 37 / 50 | Batch: 148 / 301] loss:   1129.039\n",
            "[Epoch 37 / 50 | Batch: 149 / 301] loss:   1105.250\n",
            "[Epoch 37 / 50 | Batch: 150 / 301] loss:   1547.719\n",
            "[Epoch 37 / 50 | Batch: 151 / 301] loss:   1175.192\n",
            "[Epoch 37 / 50 | Batch: 152 / 301] loss:   1265.817\n",
            "[Epoch 37 / 50 | Batch: 153 / 301] loss:   1108.495\n",
            "[Epoch 37 / 50 | Batch: 154 / 301] loss:   1359.467\n",
            "[Epoch 37 / 50 | Batch: 155 / 301] loss:   1051.078\n",
            "[Epoch 37 / 50 | Batch: 156 / 301] loss:   1380.572\n",
            "[Epoch 37 / 50 | Batch: 157 / 301] loss:   1052.324\n",
            "[Epoch 37 / 50 | Batch: 158 / 301] loss:   1118.632\n",
            "[Epoch 37 / 50 | Batch: 159 / 301] loss:   1277.897\n",
            "[Epoch 37 / 50 | Batch: 160 / 301] loss:   1050.604\n",
            "[Epoch 37 / 50 | Batch: 161 / 301] loss:   1321.869\n",
            "[Epoch 37 / 50 | Batch: 162 / 301] loss:   1312.452\n",
            "[Epoch 37 / 50 | Batch: 163 / 301] loss:   1099.451\n",
            "[Epoch 37 / 50 | Batch: 164 / 301] loss:   1237.086\n",
            "[Epoch 37 / 50 | Batch: 165 / 301] loss:   1001.946\n",
            "[Epoch 37 / 50 | Batch: 166 / 301] loss:   1214.572\n",
            "[Epoch 37 / 50 | Batch: 167 / 301] loss:    906.194\n",
            "[Epoch 37 / 50 | Batch: 168 / 301] loss:   1260.146\n",
            "[Epoch 37 / 50 | Batch: 169 / 301] loss:   1464.685\n",
            "[Epoch 37 / 50 | Batch: 170 / 301] loss:   1232.345\n",
            "[Epoch 37 / 50 | Batch: 171 / 301] loss:   1076.011\n",
            "[Epoch 37 / 50 | Batch: 172 / 301] loss:   1156.197\n",
            "[Epoch 37 / 50 | Batch: 173 / 301] loss:    988.938\n",
            "[Epoch 37 / 50 | Batch: 174 / 301] loss:   1363.963\n",
            "[Epoch 37 / 50 | Batch: 175 / 301] loss:   1106.649\n",
            "[Epoch 37 / 50 | Batch: 176 / 301] loss:   1267.915\n",
            "[Epoch 37 / 50 | Batch: 177 / 301] loss:   1757.794\n",
            "[Epoch 37 / 50 | Batch: 178 / 301] loss:    966.841\n",
            "[Epoch 37 / 50 | Batch: 179 / 301] loss:   1373.659\n",
            "[Epoch 37 / 50 | Batch: 180 / 301] loss:   1396.369\n",
            "[Epoch 37 / 50 | Batch: 181 / 301] loss:   1138.034\n",
            "[Epoch 37 / 50 | Batch: 182 / 301] loss:   1449.751\n",
            "[Epoch 37 / 50 | Batch: 183 / 301] loss:   1179.713\n",
            "[Epoch 37 / 50 | Batch: 184 / 301] loss:   1132.883\n",
            "[Epoch 37 / 50 | Batch: 185 / 301] loss:    961.105\n",
            "[Epoch 37 / 50 | Batch: 186 / 301] loss:    928.916\n",
            "[Epoch 37 / 50 | Batch: 187 / 301] loss:   1244.840\n",
            "[Epoch 37 / 50 | Batch: 188 / 301] loss:    965.399\n",
            "[Epoch 37 / 50 | Batch: 189 / 301] loss:   1222.810\n",
            "[Epoch 37 / 50 | Batch: 190 / 301] loss:   1046.315\n",
            "[Epoch 37 / 50 | Batch: 191 / 301] loss:   1132.535\n",
            "[Epoch 37 / 50 | Batch: 192 / 301] loss:   1045.546\n",
            "[Epoch 37 / 50 | Batch: 193 / 301] loss:   1174.271\n",
            "[Epoch 37 / 50 | Batch: 194 / 301] loss:    862.741\n",
            "[Epoch 37 / 50 | Batch: 195 / 301] loss:   1090.700\n",
            "[Epoch 37 / 50 | Batch: 196 / 301] loss:   1301.035\n",
            "[Epoch 37 / 50 | Batch: 197 / 301] loss:    933.625\n",
            "[Epoch 37 / 50 | Batch: 198 / 301] loss:   1393.311\n",
            "[Epoch 37 / 50 | Batch: 199 / 301] loss:   1144.605\n",
            "[Epoch 37 / 50 | Batch: 200 / 301] loss:   1063.450\n",
            "[Epoch 37 / 50 | Batch: 201 / 301] loss:    986.081\n",
            "[Epoch 37 / 50 | Batch: 202 / 301] loss:   1064.635\n",
            "[Epoch 37 / 50 | Batch: 203 / 301] loss:   1284.268\n",
            "[Epoch 37 / 50 | Batch: 204 / 301] loss:   1164.282\n",
            "[Epoch 37 / 50 | Batch: 205 / 301] loss:    821.009\n",
            "[Epoch 37 / 50 | Batch: 206 / 301] loss:    947.083\n",
            "[Epoch 37 / 50 | Batch: 207 / 301] loss:   1198.388\n",
            "[Epoch 37 / 50 | Batch: 208 / 301] loss:   1406.416\n",
            "[Epoch 37 / 50 | Batch: 209 / 301] loss:   1041.472\n",
            "[Epoch 37 / 50 | Batch: 210 / 301] loss:    822.565\n",
            "[Epoch 37 / 50 | Batch: 211 / 301] loss:   1045.639\n",
            "[Epoch 37 / 50 | Batch: 212 / 301] loss:   1558.300\n",
            "[Epoch 37 / 50 | Batch: 213 / 301] loss:   1228.010\n",
            "[Epoch 37 / 50 | Batch: 214 / 301] loss:    996.150\n",
            "[Epoch 37 / 50 | Batch: 215 / 301] loss:   1021.515\n",
            "[Epoch 37 / 50 | Batch: 216 / 301] loss:    987.807\n",
            "[Epoch 37 / 50 | Batch: 217 / 301] loss:   1262.916\n",
            "[Epoch 37 / 50 | Batch: 218 / 301] loss:    947.600\n",
            "[Epoch 37 / 50 | Batch: 219 / 301] loss:   1222.776\n",
            "[Epoch 37 / 50 | Batch: 220 / 301] loss:    994.098\n",
            "[Epoch 37 / 50 | Batch: 221 / 301] loss:   1434.166\n",
            "[Epoch 37 / 50 | Batch: 222 / 301] loss:    869.314\n",
            "[Epoch 37 / 50 | Batch: 223 / 301] loss:   1224.411\n",
            "[Epoch 37 / 50 | Batch: 224 / 301] loss:    888.949\n",
            "[Epoch 37 / 50 | Batch: 225 / 301] loss:    944.020\n",
            "[Epoch 37 / 50 | Batch: 226 / 301] loss:   1310.583\n",
            "[Epoch 37 / 50 | Batch: 227 / 301] loss:    969.744\n",
            "[Epoch 37 / 50 | Batch: 228 / 301] loss:    918.347\n",
            "[Epoch 37 / 50 | Batch: 229 / 301] loss:   1131.165\n",
            "[Epoch 37 / 50 | Batch: 230 / 301] loss:   1112.608\n",
            "[Epoch 37 / 50 | Batch: 231 / 301] loss:   1027.136\n",
            "[Epoch 37 / 50 | Batch: 232 / 301] loss:   1163.598\n",
            "[Epoch 37 / 50 | Batch: 233 / 301] loss:   1064.860\n",
            "[Epoch 37 / 50 | Batch: 234 / 301] loss:   1093.785\n",
            "[Epoch 37 / 50 | Batch: 235 / 301] loss:   1220.764\n",
            "[Epoch 37 / 50 | Batch: 236 / 301] loss:   1556.902\n",
            "[Epoch 37 / 50 | Batch: 237 / 301] loss:   1464.242\n",
            "[Epoch 37 / 50 | Batch: 238 / 301] loss:   1568.532\n",
            "[Epoch 37 / 50 | Batch: 239 / 301] loss:   1261.870\n",
            "[Epoch 37 / 50 | Batch: 240 / 301] loss:    978.638\n",
            "[Epoch 37 / 50 | Batch: 241 / 301] loss:    819.489\n",
            "[Epoch 37 / 50 | Batch: 242 / 301] loss:   1175.298\n",
            "[Epoch 37 / 50 | Batch: 243 / 301] loss:    859.760\n",
            "[Epoch 37 / 50 | Batch: 244 / 301] loss:   1542.295\n",
            "[Epoch 37 / 50 | Batch: 245 / 301] loss:    996.434\n",
            "[Epoch 37 / 50 | Batch: 246 / 301] loss:   1414.821\n",
            "[Epoch 37 / 50 | Batch: 247 / 301] loss:   1136.151\n",
            "[Epoch 37 / 50 | Batch: 248 / 301] loss:    919.189\n",
            "[Epoch 37 / 50 | Batch: 249 / 301] loss:    925.916\n",
            "[Epoch 37 / 50 | Batch: 250 / 301] loss:   1123.980\n",
            "[Epoch 37 / 50 | Batch: 251 / 301] loss:   1434.923\n",
            "[Epoch 37 / 50 | Batch: 252 / 301] loss:    962.243\n",
            "[Epoch 37 / 50 | Batch: 253 / 301] loss:   1036.165\n",
            "[Epoch 37 / 50 | Batch: 254 / 301] loss:   1394.846\n",
            "[Epoch 37 / 50 | Batch: 255 / 301] loss:    923.691\n",
            "[Epoch 37 / 50 | Batch: 256 / 301] loss:   1017.094\n",
            "[Epoch 37 / 50 | Batch: 257 / 301] loss:   1018.332\n",
            "[Epoch 37 / 50 | Batch: 258 / 301] loss:   1280.673\n",
            "[Epoch 37 / 50 | Batch: 259 / 301] loss:   1425.172\n",
            "[Epoch 37 / 50 | Batch: 260 / 301] loss:   1084.245\n",
            "[Epoch 37 / 50 | Batch: 261 / 301] loss:   1262.859\n",
            "[Epoch 37 / 50 | Batch: 262 / 301] loss:   1321.113\n",
            "[Epoch 37 / 50 | Batch: 263 / 301] loss:   1236.336\n",
            "[Epoch 37 / 50 | Batch: 264 / 301] loss:   1306.681\n",
            "[Epoch 37 / 50 | Batch: 265 / 301] loss:   1115.582\n",
            "[Epoch 37 / 50 | Batch: 266 / 301] loss:   1140.041\n",
            "[Epoch 37 / 50 | Batch: 267 / 301] loss:   1553.439\n",
            "[Epoch 37 / 50 | Batch: 268 / 301] loss:   1347.012\n",
            "[Epoch 37 / 50 | Batch: 269 / 301] loss:   1244.533\n",
            "[Epoch 37 / 50 | Batch: 270 / 301] loss:   1256.730\n",
            "[Epoch 37 / 50 | Batch: 271 / 301] loss:    834.443\n",
            "[Epoch 37 / 50 | Batch: 272 / 301] loss:   1232.478\n",
            "[Epoch 37 / 50 | Batch: 273 / 301] loss:   1320.104\n",
            "[Epoch 37 / 50 | Batch: 274 / 301] loss:    788.558\n",
            "[Epoch 37 / 50 | Batch: 275 / 301] loss:   1058.241\n",
            "[Epoch 37 / 50 | Batch: 276 / 301] loss:   1028.530\n",
            "[Epoch 37 / 50 | Batch: 277 / 301] loss:   1190.812\n",
            "[Epoch 37 / 50 | Batch: 278 / 301] loss:   1205.495\n",
            "[Epoch 37 / 50 | Batch: 279 / 301] loss:   1042.724\n",
            "[Epoch 37 / 50 | Batch: 280 / 301] loss:   1121.245\n",
            "[Epoch 37 / 50 | Batch: 281 / 301] loss:   1371.097\n",
            "[Epoch 37 / 50 | Batch: 282 / 301] loss:   1039.630\n",
            "[Epoch 37 / 50 | Batch: 283 / 301] loss:   1179.091\n",
            "[Epoch 37 / 50 | Batch: 284 / 301] loss:   1428.638\n",
            "[Epoch 37 / 50 | Batch: 285 / 301] loss:   1187.047\n",
            "[Epoch 37 / 50 | Batch: 286 / 301] loss:   1145.537\n",
            "[Epoch 37 / 50 | Batch: 287 / 301] loss:    872.339\n",
            "[Epoch 37 / 50 | Batch: 288 / 301] loss:   1189.978\n",
            "[Epoch 37 / 50 | Batch: 289 / 301] loss:   1231.029\n",
            "[Epoch 37 / 50 | Batch: 290 / 301] loss:   1227.475\n",
            "[Epoch 37 / 50 | Batch: 291 / 301] loss:   1538.102\n",
            "[Epoch 37 / 50 | Batch: 292 / 301] loss:   1051.068\n",
            "[Epoch 37 / 50 | Batch: 293 / 301] loss:   1242.277\n",
            "[Epoch 37 / 50 | Batch: 294 / 301] loss:    835.805\n",
            "[Epoch 37 / 50 | Batch: 295 / 301] loss:   1178.625\n",
            "[Epoch 37 / 50 | Batch: 296 / 301] loss:   1271.542\n",
            "[Epoch 37 / 50 | Batch: 297 / 301] loss:   1585.446\n",
            "[Epoch 37 / 50 | Batch: 298 / 301] loss:    979.336\n",
            "[Epoch 37 / 50 | Batch: 299 / 301] loss:   1524.191\n",
            "[Epoch 37 / 50 | Batch: 300 / 301] loss:   1109.503\n",
            "[Epoch 37 / 50 | Batch: 301 / 301] loss:    463.405\n",
            "Epoch loss: 1169.25029\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4498.957\n",
            "[Validation] [Batch  2 / 15] dev loss:   6366.249\n",
            "[Validation] [Batch  3 / 15] dev loss:   6562.062\n",
            "[Validation] [Batch  4 / 15] dev loss:   5419.438\n",
            "[Validation] [Batch  5 / 15] dev loss:   5324.227\n",
            "[Validation] [Batch  6 / 15] dev loss:   9173.688\n",
            "[Validation] [Batch  7 / 15] dev loss:   6396.178\n",
            "[Validation] [Batch  8 / 15] dev loss:   4309.588\n",
            "[Validation] [Batch  9 / 15] dev loss:   3868.731\n",
            "[Validation] [Batch 10 / 15] dev loss:  15659.660\n",
            "[Validation] [Batch 11 / 15] dev loss:  12928.612\n",
            "[Validation] [Batch 12 / 15] dev loss:  10892.963\n",
            "[Validation] [Batch 13 / 15] dev loss:   4595.109\n",
            "[Validation] [Batch 14 / 15] dev loss:  20026.361\n",
            "[Validation] [Batch 15 / 15] dev loss:   1934.855\n",
            "Dev loss 7863.77857\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 38 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 38 / 50 | Batch:  1 / 301] loss:   1221.642\n",
            "[Epoch 38 / 50 | Batch:  2 / 301] loss:    905.219\n",
            "[Epoch 38 / 50 | Batch:  3 / 301] loss:   1153.861\n",
            "[Epoch 38 / 50 | Batch:  4 / 301] loss:   1421.062\n",
            "[Epoch 38 / 50 | Batch:  5 / 301] loss:   1723.546\n",
            "[Epoch 38 / 50 | Batch:  6 / 301] loss:   1104.453\n",
            "[Epoch 38 / 50 | Batch:  7 / 301] loss:    893.167\n",
            "[Epoch 38 / 50 | Batch:  8 / 301] loss:   1085.401\n",
            "[Epoch 38 / 50 | Batch:  9 / 301] loss:   1056.367\n",
            "[Epoch 38 / 50 | Batch: 10 / 301] loss:    974.192\n",
            "[Epoch 38 / 50 | Batch: 11 / 301] loss:   1054.956\n",
            "[Epoch 38 / 50 | Batch: 12 / 301] loss:   1377.251\n",
            "[Epoch 38 / 50 | Batch: 13 / 301] loss:    940.966\n",
            "[Epoch 38 / 50 | Batch: 14 / 301] loss:   1730.702\n",
            "[Epoch 38 / 50 | Batch: 15 / 301] loss:   1229.258\n",
            "[Epoch 38 / 50 | Batch: 16 / 301] loss:    826.721\n",
            "[Epoch 38 / 50 | Batch: 17 / 301] loss:   1353.136\n",
            "[Epoch 38 / 50 | Batch: 18 / 301] loss:   1608.981\n",
            "[Epoch 38 / 50 | Batch: 19 / 301] loss:   1280.241\n",
            "[Epoch 38 / 50 | Batch: 20 / 301] loss:   1244.165\n",
            "[Epoch 38 / 50 | Batch: 21 / 301] loss:   1300.044\n",
            "[Epoch 38 / 50 | Batch: 22 / 301] loss:   1002.144\n",
            "[Epoch 38 / 50 | Batch: 23 / 301] loss:    899.326\n",
            "[Epoch 38 / 50 | Batch: 24 / 301] loss:   1461.770\n",
            "[Epoch 38 / 50 | Batch: 25 / 301] loss:    938.901\n",
            "[Epoch 38 / 50 | Batch: 26 / 301] loss:   1239.121\n",
            "[Epoch 38 / 50 | Batch: 27 / 301] loss:   1214.878\n",
            "[Epoch 38 / 50 | Batch: 28 / 301] loss:   1336.446\n",
            "[Epoch 38 / 50 | Batch: 29 / 301] loss:   1135.418\n",
            "[Epoch 38 / 50 | Batch: 30 / 301] loss:   1642.667\n",
            "[Epoch 38 / 50 | Batch: 31 / 301] loss:   1191.121\n",
            "[Epoch 38 / 50 | Batch: 32 / 301] loss:   1326.957\n",
            "[Epoch 38 / 50 | Batch: 33 / 301] loss:   1454.684\n",
            "[Epoch 38 / 50 | Batch: 34 / 301] loss:   1075.596\n",
            "[Epoch 38 / 50 | Batch: 35 / 301] loss:   1216.149\n",
            "[Epoch 38 / 50 | Batch: 36 / 301] loss:   1027.698\n",
            "[Epoch 38 / 50 | Batch: 37 / 301] loss:   1080.329\n",
            "[Epoch 38 / 50 | Batch: 38 / 301] loss:   1150.957\n",
            "[Epoch 38 / 50 | Batch: 39 / 301] loss:    989.524\n",
            "[Epoch 38 / 50 | Batch: 40 / 301] loss:   1184.493\n",
            "[Epoch 38 / 50 | Batch: 41 / 301] loss:   1433.142\n",
            "[Epoch 38 / 50 | Batch: 42 / 301] loss:   1010.761\n",
            "[Epoch 38 / 50 | Batch: 43 / 301] loss:   1280.565\n",
            "[Epoch 38 / 50 | Batch: 44 / 301] loss:    623.076\n",
            "[Epoch 38 / 50 | Batch: 45 / 301] loss:   1226.101\n",
            "[Epoch 38 / 50 | Batch: 46 / 301] loss:   1415.956\n",
            "[Epoch 38 / 50 | Batch: 47 / 301] loss:    903.984\n",
            "[Epoch 38 / 50 | Batch: 48 / 301] loss:   1001.182\n",
            "[Epoch 38 / 50 | Batch: 49 / 301] loss:    962.004\n",
            "[Epoch 38 / 50 | Batch: 50 / 301] loss:   1688.557\n",
            "[Epoch 38 / 50 | Batch: 51 / 301] loss:   1145.873\n",
            "[Epoch 38 / 50 | Batch: 52 / 301] loss:   1316.630\n",
            "[Epoch 38 / 50 | Batch: 53 / 301] loss:   1354.742\n",
            "[Epoch 38 / 50 | Batch: 54 / 301] loss:    902.716\n",
            "[Epoch 38 / 50 | Batch: 55 / 301] loss:   1174.685\n",
            "[Epoch 38 / 50 | Batch: 56 / 301] loss:   1260.523\n",
            "[Epoch 38 / 50 | Batch: 57 / 301] loss:   1376.485\n",
            "[Epoch 38 / 50 | Batch: 58 / 301] loss:    956.860\n",
            "[Epoch 38 / 50 | Batch: 59 / 301] loss:   1128.165\n",
            "[Epoch 38 / 50 | Batch: 60 / 301] loss:   1232.817\n",
            "[Epoch 38 / 50 | Batch: 61 / 301] loss:   1231.691\n",
            "[Epoch 38 / 50 | Batch: 62 / 301] loss:   1613.141\n",
            "[Epoch 38 / 50 | Batch: 63 / 301] loss:    959.998\n",
            "[Epoch 38 / 50 | Batch: 64 / 301] loss:   1349.253\n",
            "[Epoch 38 / 50 | Batch: 65 / 301] loss:   1139.776\n",
            "[Epoch 38 / 50 | Batch: 66 / 301] loss:   1178.848\n",
            "[Epoch 38 / 50 | Batch: 67 / 301] loss:   1166.160\n",
            "[Epoch 38 / 50 | Batch: 68 / 301] loss:   1146.292\n",
            "[Epoch 38 / 50 | Batch: 69 / 301] loss:   1034.836\n",
            "[Epoch 38 / 50 | Batch: 70 / 301] loss:   1290.549\n",
            "[Epoch 38 / 50 | Batch: 71 / 301] loss:   1021.348\n",
            "[Epoch 38 / 50 | Batch: 72 / 301] loss:   1178.411\n",
            "[Epoch 38 / 50 | Batch: 73 / 301] loss:   1018.059\n",
            "[Epoch 38 / 50 | Batch: 74 / 301] loss:    966.867\n",
            "[Epoch 38 / 50 | Batch: 75 / 301] loss:   1239.970\n",
            "[Epoch 38 / 50 | Batch: 76 / 301] loss:   1365.954\n",
            "[Epoch 38 / 50 | Batch: 77 / 301] loss:   1174.364\n",
            "[Epoch 38 / 50 | Batch: 78 / 301] loss:   1214.734\n",
            "[Epoch 38 / 50 | Batch: 79 / 301] loss:    889.575\n",
            "[Epoch 38 / 50 | Batch: 80 / 301] loss:   1103.776\n",
            "[Epoch 38 / 50 | Batch: 81 / 301] loss:   1629.627\n",
            "[Epoch 38 / 50 | Batch: 82 / 301] loss:   1114.832\n",
            "[Epoch 38 / 50 | Batch: 83 / 301] loss:   1099.470\n",
            "[Epoch 38 / 50 | Batch: 84 / 301] loss:   1351.540\n",
            "[Epoch 38 / 50 | Batch: 85 / 301] loss:   1231.780\n",
            "[Epoch 38 / 50 | Batch: 86 / 301] loss:   1177.345\n",
            "[Epoch 38 / 50 | Batch: 87 / 301] loss:   1264.899\n",
            "[Epoch 38 / 50 | Batch: 88 / 301] loss:   1038.208\n",
            "[Epoch 38 / 50 | Batch: 89 / 301] loss:   1019.424\n",
            "[Epoch 38 / 50 | Batch: 90 / 301] loss:   1553.418\n",
            "[Epoch 38 / 50 | Batch: 91 / 301] loss:   1309.785\n",
            "[Epoch 38 / 50 | Batch: 92 / 301] loss:   1297.429\n",
            "[Epoch 38 / 50 | Batch: 93 / 301] loss:    809.457\n",
            "[Epoch 38 / 50 | Batch: 94 / 301] loss:    853.580\n",
            "[Epoch 38 / 50 | Batch: 95 / 301] loss:    732.824\n",
            "[Epoch 38 / 50 | Batch: 96 / 301] loss:   1128.916\n",
            "[Epoch 38 / 50 | Batch: 97 / 301] loss:   1369.871\n",
            "[Epoch 38 / 50 | Batch: 98 / 301] loss:   1853.667\n",
            "[Epoch 38 / 50 | Batch: 99 / 301] loss:   1450.748\n",
            "[Epoch 38 / 50 | Batch: 100 / 301] loss:   1210.324\n",
            "[Epoch 38 / 50 | Batch: 101 / 301] loss:   1120.472\n",
            "[Epoch 38 / 50 | Batch: 102 / 301] loss:   1145.630\n",
            "[Epoch 38 / 50 | Batch: 103 / 301] loss:   1216.448\n",
            "[Epoch 38 / 50 | Batch: 104 / 301] loss:   1068.732\n",
            "[Epoch 38 / 50 | Batch: 105 / 301] loss:   1082.467\n",
            "[Epoch 38 / 50 | Batch: 106 / 301] loss:   1271.305\n",
            "[Epoch 38 / 50 | Batch: 107 / 301] loss:   1172.714\n",
            "[Epoch 38 / 50 | Batch: 108 / 301] loss:   1520.072\n",
            "[Epoch 38 / 50 | Batch: 109 / 301] loss:    847.989\n",
            "[Epoch 38 / 50 | Batch: 110 / 301] loss:   1303.663\n",
            "[Epoch 38 / 50 | Batch: 111 / 301] loss:    910.646\n",
            "[Epoch 38 / 50 | Batch: 112 / 301] loss:   1119.483\n",
            "[Epoch 38 / 50 | Batch: 113 / 301] loss:   2313.753\n",
            "[Epoch 38 / 50 | Batch: 114 / 301] loss:   1050.006\n",
            "[Epoch 38 / 50 | Batch: 115 / 301] loss:   1081.103\n",
            "[Epoch 38 / 50 | Batch: 116 / 301] loss:   1089.812\n",
            "[Epoch 38 / 50 | Batch: 117 / 301] loss:   1146.747\n",
            "[Epoch 38 / 50 | Batch: 118 / 301] loss:   1116.173\n",
            "[Epoch 38 / 50 | Batch: 119 / 301] loss:   1025.839\n",
            "[Epoch 38 / 50 | Batch: 120 / 301] loss:   1419.331\n",
            "[Epoch 38 / 50 | Batch: 121 / 301] loss:   1076.354\n",
            "[Epoch 38 / 50 | Batch: 122 / 301] loss:   1149.267\n",
            "[Epoch 38 / 50 | Batch: 123 / 301] loss:    781.401\n",
            "[Epoch 38 / 50 | Batch: 124 / 301] loss:   1193.756\n",
            "[Epoch 38 / 50 | Batch: 125 / 301] loss:   1193.405\n",
            "[Epoch 38 / 50 | Batch: 126 / 301] loss:   1565.023\n",
            "[Epoch 38 / 50 | Batch: 127 / 301] loss:   1052.110\n",
            "[Epoch 38 / 50 | Batch: 128 / 301] loss:    962.797\n",
            "[Epoch 38 / 50 | Batch: 129 / 301] loss:   1450.102\n",
            "[Epoch 38 / 50 | Batch: 130 / 301] loss:   1129.925\n",
            "[Epoch 38 / 50 | Batch: 131 / 301] loss:   1507.391\n",
            "[Epoch 38 / 50 | Batch: 132 / 301] loss:   1046.272\n",
            "[Epoch 38 / 50 | Batch: 133 / 301] loss:   1018.916\n",
            "[Epoch 38 / 50 | Batch: 134 / 301] loss:   1097.261\n",
            "[Epoch 38 / 50 | Batch: 135 / 301] loss:   1358.334\n",
            "[Epoch 38 / 50 | Batch: 136 / 301] loss:   1416.482\n",
            "[Epoch 38 / 50 | Batch: 137 / 301] loss:    757.866\n",
            "[Epoch 38 / 50 | Batch: 138 / 301] loss:   1917.100\n",
            "[Epoch 38 / 50 | Batch: 139 / 301] loss:   1286.167\n",
            "[Epoch 38 / 50 | Batch: 140 / 301] loss:   1153.434\n",
            "[Epoch 38 / 50 | Batch: 141 / 301] loss:   1165.109\n",
            "[Epoch 38 / 50 | Batch: 142 / 301] loss:    918.192\n",
            "[Epoch 38 / 50 | Batch: 143 / 301] loss:    882.248\n",
            "[Epoch 38 / 50 | Batch: 144 / 301] loss:   1367.615\n",
            "[Epoch 38 / 50 | Batch: 145 / 301] loss:    999.731\n",
            "[Epoch 38 / 50 | Batch: 146 / 301] loss:    787.938\n",
            "[Epoch 38 / 50 | Batch: 147 / 301] loss:   1425.891\n",
            "[Epoch 38 / 50 | Batch: 148 / 301] loss:   1207.394\n",
            "[Epoch 38 / 50 | Batch: 149 / 301] loss:   1046.246\n",
            "[Epoch 38 / 50 | Batch: 150 / 301] loss:    932.441\n",
            "[Epoch 38 / 50 | Batch: 151 / 301] loss:    999.240\n",
            "[Epoch 38 / 50 | Batch: 152 / 301] loss:   1201.646\n",
            "[Epoch 38 / 50 | Batch: 153 / 301] loss:   1338.664\n",
            "[Epoch 38 / 50 | Batch: 154 / 301] loss:   1126.214\n",
            "[Epoch 38 / 50 | Batch: 155 / 301] loss:    944.059\n",
            "[Epoch 38 / 50 | Batch: 156 / 301] loss:   1372.452\n",
            "[Epoch 38 / 50 | Batch: 157 / 301] loss:    805.893\n",
            "[Epoch 38 / 50 | Batch: 158 / 301] loss:   1049.352\n",
            "[Epoch 38 / 50 | Batch: 159 / 301] loss:   1509.051\n",
            "[Epoch 38 / 50 | Batch: 160 / 301] loss:   1207.287\n",
            "[Epoch 38 / 50 | Batch: 161 / 301] loss:   1079.727\n",
            "[Epoch 38 / 50 | Batch: 162 / 301] loss:    999.620\n",
            "[Epoch 38 / 50 | Batch: 163 / 301] loss:   1359.715\n",
            "[Epoch 38 / 50 | Batch: 164 / 301] loss:   1588.816\n",
            "[Epoch 38 / 50 | Batch: 165 / 301] loss:   1334.786\n",
            "[Epoch 38 / 50 | Batch: 166 / 301] loss:   1253.382\n",
            "[Epoch 38 / 50 | Batch: 167 / 301] loss:   1006.291\n",
            "[Epoch 38 / 50 | Batch: 168 / 301] loss:   1141.764\n",
            "[Epoch 38 / 50 | Batch: 169 / 301] loss:   1367.506\n",
            "[Epoch 38 / 50 | Batch: 170 / 301] loss:   1504.185\n",
            "[Epoch 38 / 50 | Batch: 171 / 301] loss:   1406.395\n",
            "[Epoch 38 / 50 | Batch: 172 / 301] loss:   1403.638\n",
            "[Epoch 38 / 50 | Batch: 173 / 301] loss:   1067.494\n",
            "[Epoch 38 / 50 | Batch: 174 / 301] loss:   1410.359\n",
            "[Epoch 38 / 50 | Batch: 175 / 301] loss:   1222.990\n",
            "[Epoch 38 / 50 | Batch: 176 / 301] loss:   1065.388\n",
            "[Epoch 38 / 50 | Batch: 177 / 301] loss:   1055.957\n",
            "[Epoch 38 / 50 | Batch: 178 / 301] loss:   1086.154\n",
            "[Epoch 38 / 50 | Batch: 179 / 301] loss:   1135.384\n",
            "[Epoch 38 / 50 | Batch: 180 / 301] loss:   1116.687\n",
            "[Epoch 38 / 50 | Batch: 181 / 301] loss:   1134.542\n",
            "[Epoch 38 / 50 | Batch: 182 / 301] loss:   1138.505\n",
            "[Epoch 38 / 50 | Batch: 183 / 301] loss:   1208.358\n",
            "[Epoch 38 / 50 | Batch: 184 / 301] loss:   1094.502\n",
            "[Epoch 38 / 50 | Batch: 185 / 301] loss:   1336.049\n",
            "[Epoch 38 / 50 | Batch: 186 / 301] loss:    923.729\n",
            "[Epoch 38 / 50 | Batch: 187 / 301] loss:   1054.517\n",
            "[Epoch 38 / 50 | Batch: 188 / 301] loss:   1195.401\n",
            "[Epoch 38 / 50 | Batch: 189 / 301] loss:   1127.430\n",
            "[Epoch 38 / 50 | Batch: 190 / 301] loss:   1109.939\n",
            "[Epoch 38 / 50 | Batch: 191 / 301] loss:    917.540\n",
            "[Epoch 38 / 50 | Batch: 192 / 301] loss:    800.848\n",
            "[Epoch 38 / 50 | Batch: 193 / 301] loss:    951.173\n",
            "[Epoch 38 / 50 | Batch: 194 / 301] loss:   1005.701\n",
            "[Epoch 38 / 50 | Batch: 195 / 301] loss:   1345.583\n",
            "[Epoch 38 / 50 | Batch: 196 / 301] loss:   1014.382\n",
            "[Epoch 38 / 50 | Batch: 197 / 301] loss:   1139.251\n",
            "[Epoch 38 / 50 | Batch: 198 / 301] loss:   1084.781\n",
            "[Epoch 38 / 50 | Batch: 199 / 301] loss:   1206.265\n",
            "[Epoch 38 / 50 | Batch: 200 / 301] loss:   1253.772\n",
            "[Epoch 38 / 50 | Batch: 201 / 301] loss:   1319.881\n",
            "[Epoch 38 / 50 | Batch: 202 / 301] loss:   1054.432\n",
            "[Epoch 38 / 50 | Batch: 203 / 301] loss:    901.767\n",
            "[Epoch 38 / 50 | Batch: 204 / 301] loss:   1358.508\n",
            "[Epoch 38 / 50 | Batch: 205 / 301] loss:    760.938\n",
            "[Epoch 38 / 50 | Batch: 206 / 301] loss:   1058.036\n",
            "[Epoch 38 / 50 | Batch: 207 / 301] loss:    912.699\n",
            "[Epoch 38 / 50 | Batch: 208 / 301] loss:   1158.089\n",
            "[Epoch 38 / 50 | Batch: 209 / 301] loss:   1044.173\n",
            "[Epoch 38 / 50 | Batch: 210 / 301] loss:    996.008\n",
            "[Epoch 38 / 50 | Batch: 211 / 301] loss:   1015.382\n",
            "[Epoch 38 / 50 | Batch: 212 / 301] loss:   1048.057\n",
            "[Epoch 38 / 50 | Batch: 213 / 301] loss:   1109.696\n",
            "[Epoch 38 / 50 | Batch: 214 / 301] loss:   1138.500\n",
            "[Epoch 38 / 50 | Batch: 215 / 301] loss:   1271.547\n",
            "[Epoch 38 / 50 | Batch: 216 / 301] loss:   1430.441\n",
            "[Epoch 38 / 50 | Batch: 217 / 301] loss:   1042.553\n",
            "[Epoch 38 / 50 | Batch: 218 / 301] loss:   1219.028\n",
            "[Epoch 38 / 50 | Batch: 219 / 301] loss:   1292.045\n",
            "[Epoch 38 / 50 | Batch: 220 / 301] loss:   1334.919\n",
            "[Epoch 38 / 50 | Batch: 221 / 301] loss:   1208.009\n",
            "[Epoch 38 / 50 | Batch: 222 / 301] loss:   1327.509\n",
            "[Epoch 38 / 50 | Batch: 223 / 301] loss:   1217.306\n",
            "[Epoch 38 / 50 | Batch: 224 / 301] loss:   1257.891\n",
            "[Epoch 38 / 50 | Batch: 225 / 301] loss:   1010.495\n",
            "[Epoch 38 / 50 | Batch: 226 / 301] loss:   1584.825\n",
            "[Epoch 38 / 50 | Batch: 227 / 301] loss:   1484.027\n",
            "[Epoch 38 / 50 | Batch: 228 / 301] loss:    692.541\n",
            "[Epoch 38 / 50 | Batch: 229 / 301] loss:   1048.413\n",
            "[Epoch 38 / 50 | Batch: 230 / 301] loss:   1143.025\n",
            "[Epoch 38 / 50 | Batch: 231 / 301] loss:    968.369\n",
            "[Epoch 38 / 50 | Batch: 232 / 301] loss:   1158.461\n",
            "[Epoch 38 / 50 | Batch: 233 / 301] loss:   1015.028\n",
            "[Epoch 38 / 50 | Batch: 234 / 301] loss:   1247.366\n",
            "[Epoch 38 / 50 | Batch: 235 / 301] loss:   1293.207\n",
            "[Epoch 38 / 50 | Batch: 236 / 301] loss:   1377.311\n",
            "[Epoch 38 / 50 | Batch: 237 / 301] loss:    959.890\n",
            "[Epoch 38 / 50 | Batch: 238 / 301] loss:    974.233\n",
            "[Epoch 38 / 50 | Batch: 239 / 301] loss:   1539.482\n",
            "[Epoch 38 / 50 | Batch: 240 / 301] loss:    882.020\n",
            "[Epoch 38 / 50 | Batch: 241 / 301] loss:   1196.964\n",
            "[Epoch 38 / 50 | Batch: 242 / 301] loss:   1079.276\n",
            "[Epoch 38 / 50 | Batch: 243 / 301] loss:   1072.726\n",
            "[Epoch 38 / 50 | Batch: 244 / 301] loss:   1290.761\n",
            "[Epoch 38 / 50 | Batch: 245 / 301] loss:   1307.111\n",
            "[Epoch 38 / 50 | Batch: 246 / 301] loss:   1179.844\n",
            "[Epoch 38 / 50 | Batch: 247 / 301] loss:   1059.813\n",
            "[Epoch 38 / 50 | Batch: 248 / 301] loss:   1104.134\n",
            "[Epoch 38 / 50 | Batch: 249 / 301] loss:   1018.184\n",
            "[Epoch 38 / 50 | Batch: 250 / 301] loss:    786.884\n",
            "[Epoch 38 / 50 | Batch: 251 / 301] loss:   1383.075\n",
            "[Epoch 38 / 50 | Batch: 252 / 301] loss:   1017.742\n",
            "[Epoch 38 / 50 | Batch: 253 / 301] loss:   1032.397\n",
            "[Epoch 38 / 50 | Batch: 254 / 301] loss:   1033.583\n",
            "[Epoch 38 / 50 | Batch: 255 / 301] loss:   1041.163\n",
            "[Epoch 38 / 50 | Batch: 256 / 301] loss:   1046.840\n",
            "[Epoch 38 / 50 | Batch: 257 / 301] loss:   1489.025\n",
            "[Epoch 38 / 50 | Batch: 258 / 301] loss:   1263.714\n",
            "[Epoch 38 / 50 | Batch: 259 / 301] loss:   1351.278\n",
            "[Epoch 38 / 50 | Batch: 260 / 301] loss:   1480.702\n",
            "[Epoch 38 / 50 | Batch: 261 / 301] loss:   1366.708\n",
            "[Epoch 38 / 50 | Batch: 262 / 301] loss:   1101.818\n",
            "[Epoch 38 / 50 | Batch: 263 / 301] loss:   1176.191\n",
            "[Epoch 38 / 50 | Batch: 264 / 301] loss:    921.909\n",
            "[Epoch 38 / 50 | Batch: 265 / 301] loss:   1226.600\n",
            "[Epoch 38 / 50 | Batch: 266 / 301] loss:    926.932\n",
            "[Epoch 38 / 50 | Batch: 267 / 301] loss:   1128.673\n",
            "[Epoch 38 / 50 | Batch: 268 / 301] loss:   1558.019\n",
            "[Epoch 38 / 50 | Batch: 269 / 301] loss:   1439.659\n",
            "[Epoch 38 / 50 | Batch: 270 / 301] loss:   1019.360\n",
            "[Epoch 38 / 50 | Batch: 271 / 301] loss:   1057.645\n",
            "[Epoch 38 / 50 | Batch: 272 / 301] loss:   1034.757\n",
            "[Epoch 38 / 50 | Batch: 273 / 301] loss:    997.792\n",
            "[Epoch 38 / 50 | Batch: 274 / 301] loss:   1097.237\n",
            "[Epoch 38 / 50 | Batch: 275 / 301] loss:   1305.509\n",
            "[Epoch 38 / 50 | Batch: 276 / 301] loss:   1072.409\n",
            "[Epoch 38 / 50 | Batch: 277 / 301] loss:   1166.923\n",
            "[Epoch 38 / 50 | Batch: 278 / 301] loss:   1271.553\n",
            "[Epoch 38 / 50 | Batch: 279 / 301] loss:    907.865\n",
            "[Epoch 38 / 50 | Batch: 280 / 301] loss:   1028.999\n",
            "[Epoch 38 / 50 | Batch: 281 / 301] loss:    838.299\n",
            "[Epoch 38 / 50 | Batch: 282 / 301] loss:   1279.338\n",
            "[Epoch 38 / 50 | Batch: 283 / 301] loss:    866.790\n",
            "[Epoch 38 / 50 | Batch: 284 / 301] loss:    968.329\n",
            "[Epoch 38 / 50 | Batch: 285 / 301] loss:   1030.501\n",
            "[Epoch 38 / 50 | Batch: 286 / 301] loss:   1186.070\n",
            "[Epoch 38 / 50 | Batch: 287 / 301] loss:   1054.806\n",
            "[Epoch 38 / 50 | Batch: 288 / 301] loss:   1067.567\n",
            "[Epoch 38 / 50 | Batch: 289 / 301] loss:   1188.719\n",
            "[Epoch 38 / 50 | Batch: 290 / 301] loss:    953.310\n",
            "[Epoch 38 / 50 | Batch: 291 / 301] loss:    952.339\n",
            "[Epoch 38 / 50 | Batch: 292 / 301] loss:   1212.068\n",
            "[Epoch 38 / 50 | Batch: 293 / 301] loss:   1103.356\n",
            "[Epoch 38 / 50 | Batch: 294 / 301] loss:   1303.945\n",
            "[Epoch 38 / 50 | Batch: 295 / 301] loss:   1065.670\n",
            "[Epoch 38 / 50 | Batch: 296 / 301] loss:   1259.379\n",
            "[Epoch 38 / 50 | Batch: 297 / 301] loss:   1177.930\n",
            "[Epoch 38 / 50 | Batch: 298 / 301] loss:   1232.563\n",
            "[Epoch 38 / 50 | Batch: 299 / 301] loss:   1179.214\n",
            "[Epoch 38 / 50 | Batch: 300 / 301] loss:   1095.142\n",
            "[Epoch 38 / 50 | Batch: 301 / 301] loss:    485.740\n",
            "Epoch loss: 1165.11279\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4410.280\n",
            "[Validation] [Batch  2 / 15] dev loss:   6555.958\n",
            "[Validation] [Batch  3 / 15] dev loss:   6830.101\n",
            "[Validation] [Batch  4 / 15] dev loss:   5296.040\n",
            "[Validation] [Batch  5 / 15] dev loss:   5777.353\n",
            "[Validation] [Batch  6 / 15] dev loss:   9535.316\n",
            "[Validation] [Batch  7 / 15] dev loss:   5822.531\n",
            "[Validation] [Batch  8 / 15] dev loss:   4282.940\n",
            "[Validation] [Batch  9 / 15] dev loss:   3448.659\n",
            "[Validation] [Batch 10 / 15] dev loss:  15206.172\n",
            "[Validation] [Batch 11 / 15] dev loss:  12659.371\n",
            "[Validation] [Batch 12 / 15] dev loss:  10602.625\n",
            "[Validation] [Batch 13 / 15] dev loss:   4530.296\n",
            "[Validation] [Batch 14 / 15] dev loss:  20899.146\n",
            "[Validation] [Batch 15 / 15] dev loss:   1925.272\n",
            "Dev loss 7852.13736\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 39 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 39 / 50 | Batch:  1 / 301] loss:   1552.027\n",
            "[Epoch 39 / 50 | Batch:  2 / 301] loss:   1124.794\n",
            "[Epoch 39 / 50 | Batch:  3 / 301] loss:   1460.927\n",
            "[Epoch 39 / 50 | Batch:  4 / 301] loss:   1505.941\n",
            "[Epoch 39 / 50 | Batch:  5 / 301] loss:   1189.955\n",
            "[Epoch 39 / 50 | Batch:  6 / 301] loss:   1112.055\n",
            "[Epoch 39 / 50 | Batch:  7 / 301] loss:    886.638\n",
            "[Epoch 39 / 50 | Batch:  8 / 301] loss:    999.017\n",
            "[Epoch 39 / 50 | Batch:  9 / 301] loss:    879.397\n",
            "[Epoch 39 / 50 | Batch: 10 / 301] loss:   1176.747\n",
            "[Epoch 39 / 50 | Batch: 11 / 301] loss:    889.438\n",
            "[Epoch 39 / 50 | Batch: 12 / 301] loss:   1045.979\n",
            "[Epoch 39 / 50 | Batch: 13 / 301] loss:    995.827\n",
            "[Epoch 39 / 50 | Batch: 14 / 301] loss:   1206.010\n",
            "[Epoch 39 / 50 | Batch: 15 / 301] loss:   1379.696\n",
            "[Epoch 39 / 50 | Batch: 16 / 301] loss:   1658.830\n",
            "[Epoch 39 / 50 | Batch: 17 / 301] loss:   1157.631\n",
            "[Epoch 39 / 50 | Batch: 18 / 301] loss:   1306.251\n",
            "[Epoch 39 / 50 | Batch: 19 / 301] loss:    965.217\n",
            "[Epoch 39 / 50 | Batch: 20 / 301] loss:    997.937\n",
            "[Epoch 39 / 50 | Batch: 21 / 301] loss:   1265.596\n",
            "[Epoch 39 / 50 | Batch: 22 / 301] loss:   1018.199\n",
            "[Epoch 39 / 50 | Batch: 23 / 301] loss:   1016.785\n",
            "[Epoch 39 / 50 | Batch: 24 / 301] loss:   1055.642\n",
            "[Epoch 39 / 50 | Batch: 25 / 301] loss:    920.474\n",
            "[Epoch 39 / 50 | Batch: 26 / 301] loss:   1454.146\n",
            "[Epoch 39 / 50 | Batch: 27 / 301] loss:   1509.811\n",
            "[Epoch 39 / 50 | Batch: 28 / 301] loss:   1220.945\n",
            "[Epoch 39 / 50 | Batch: 29 / 301] loss:   1417.977\n",
            "[Epoch 39 / 50 | Batch: 30 / 301] loss:   1116.882\n",
            "[Epoch 39 / 50 | Batch: 31 / 301] loss:   1459.328\n",
            "[Epoch 39 / 50 | Batch: 32 / 301] loss:   1115.585\n",
            "[Epoch 39 / 50 | Batch: 33 / 301] loss:   1056.255\n",
            "[Epoch 39 / 50 | Batch: 34 / 301] loss:   1214.507\n",
            "[Epoch 39 / 50 | Batch: 35 / 301] loss:   1521.230\n",
            "[Epoch 39 / 50 | Batch: 36 / 301] loss:   1472.682\n",
            "[Epoch 39 / 50 | Batch: 37 / 301] loss:   1217.079\n",
            "[Epoch 39 / 50 | Batch: 38 / 301] loss:   1051.124\n",
            "[Epoch 39 / 50 | Batch: 39 / 301] loss:   1008.715\n",
            "[Epoch 39 / 50 | Batch: 40 / 301] loss:   1395.360\n",
            "[Epoch 39 / 50 | Batch: 41 / 301] loss:   1214.452\n",
            "[Epoch 39 / 50 | Batch: 42 / 301] loss:   1326.574\n",
            "[Epoch 39 / 50 | Batch: 43 / 301] loss:   1280.963\n",
            "[Epoch 39 / 50 | Batch: 44 / 301] loss:    995.299\n",
            "[Epoch 39 / 50 | Batch: 45 / 301] loss:   1337.972\n",
            "[Epoch 39 / 50 | Batch: 46 / 301] loss:   1166.132\n",
            "[Epoch 39 / 50 | Batch: 47 / 301] loss:   1302.644\n",
            "[Epoch 39 / 50 | Batch: 48 / 301] loss:   1507.047\n",
            "[Epoch 39 / 50 | Batch: 49 / 301] loss:    937.422\n",
            "[Epoch 39 / 50 | Batch: 50 / 301] loss:   1150.338\n",
            "[Epoch 39 / 50 | Batch: 51 / 301] loss:   1411.818\n",
            "[Epoch 39 / 50 | Batch: 52 / 301] loss:   1111.933\n",
            "[Epoch 39 / 50 | Batch: 53 / 301] loss:   1542.744\n",
            "[Epoch 39 / 50 | Batch: 54 / 301] loss:   1047.788\n",
            "[Epoch 39 / 50 | Batch: 55 / 301] loss:   1079.965\n",
            "[Epoch 39 / 50 | Batch: 56 / 301] loss:   1552.943\n",
            "[Epoch 39 / 50 | Batch: 57 / 301] loss:    911.826\n",
            "[Epoch 39 / 50 | Batch: 58 / 301] loss:   1284.851\n",
            "[Epoch 39 / 50 | Batch: 59 / 301] loss:   1338.286\n",
            "[Epoch 39 / 50 | Batch: 60 / 301] loss:   1280.720\n",
            "[Epoch 39 / 50 | Batch: 61 / 301] loss:   1311.682\n",
            "[Epoch 39 / 50 | Batch: 62 / 301] loss:   1432.113\n",
            "[Epoch 39 / 50 | Batch: 63 / 301] loss:   1261.067\n",
            "[Epoch 39 / 50 | Batch: 64 / 301] loss:   1268.811\n",
            "[Epoch 39 / 50 | Batch: 65 / 301] loss:   1199.036\n",
            "[Epoch 39 / 50 | Batch: 66 / 301] loss:   1192.438\n",
            "[Epoch 39 / 50 | Batch: 67 / 301] loss:   1068.645\n",
            "[Epoch 39 / 50 | Batch: 68 / 301] loss:   1155.595\n",
            "[Epoch 39 / 50 | Batch: 69 / 301] loss:   1110.417\n",
            "[Epoch 39 / 50 | Batch: 70 / 301] loss:   1199.060\n",
            "[Epoch 39 / 50 | Batch: 71 / 301] loss:    941.134\n",
            "[Epoch 39 / 50 | Batch: 72 / 301] loss:   1125.694\n",
            "[Epoch 39 / 50 | Batch: 73 / 301] loss:    975.357\n",
            "[Epoch 39 / 50 | Batch: 74 / 301] loss:   1449.144\n",
            "[Epoch 39 / 50 | Batch: 75 / 301] loss:   1060.889\n",
            "[Epoch 39 / 50 | Batch: 76 / 301] loss:   1276.932\n",
            "[Epoch 39 / 50 | Batch: 77 / 301] loss:    976.202\n",
            "[Epoch 39 / 50 | Batch: 78 / 301] loss:    978.728\n",
            "[Epoch 39 / 50 | Batch: 79 / 301] loss:   1287.991\n",
            "[Epoch 39 / 50 | Batch: 80 / 301] loss:   1465.620\n",
            "[Epoch 39 / 50 | Batch: 81 / 301] loss:    867.927\n",
            "[Epoch 39 / 50 | Batch: 82 / 301] loss:   1070.187\n",
            "[Epoch 39 / 50 | Batch: 83 / 301] loss:    922.459\n",
            "[Epoch 39 / 50 | Batch: 84 / 301] loss:   1202.305\n",
            "[Epoch 39 / 50 | Batch: 85 / 301] loss:   1021.937\n",
            "[Epoch 39 / 50 | Batch: 86 / 301] loss:   1037.535\n",
            "[Epoch 39 / 50 | Batch: 87 / 301] loss:   1197.513\n",
            "[Epoch 39 / 50 | Batch: 88 / 301] loss:    926.520\n",
            "[Epoch 39 / 50 | Batch: 89 / 301] loss:    917.089\n",
            "[Epoch 39 / 50 | Batch: 90 / 301] loss:   1243.527\n",
            "[Epoch 39 / 50 | Batch: 91 / 301] loss:   1152.358\n",
            "[Epoch 39 / 50 | Batch: 92 / 301] loss:    884.527\n",
            "[Epoch 39 / 50 | Batch: 93 / 301] loss:    809.011\n",
            "[Epoch 39 / 50 | Batch: 94 / 301] loss:    852.026\n",
            "[Epoch 39 / 50 | Batch: 95 / 301] loss:   1360.411\n",
            "[Epoch 39 / 50 | Batch: 96 / 301] loss:   1328.126\n",
            "[Epoch 39 / 50 | Batch: 97 / 301] loss:    889.765\n",
            "[Epoch 39 / 50 | Batch: 98 / 301] loss:   1460.270\n",
            "[Epoch 39 / 50 | Batch: 99 / 301] loss:    913.704\n",
            "[Epoch 39 / 50 | Batch: 100 / 301] loss:   1118.440\n",
            "[Epoch 39 / 50 | Batch: 101 / 301] loss:   1124.740\n",
            "[Epoch 39 / 50 | Batch: 102 / 301] loss:    980.577\n",
            "[Epoch 39 / 50 | Batch: 103 / 301] loss:   1087.668\n",
            "[Epoch 39 / 50 | Batch: 104 / 301] loss:   1100.600\n",
            "[Epoch 39 / 50 | Batch: 105 / 301] loss:    963.126\n",
            "[Epoch 39 / 50 | Batch: 106 / 301] loss:   1242.490\n",
            "[Epoch 39 / 50 | Batch: 107 / 301] loss:   1337.053\n",
            "[Epoch 39 / 50 | Batch: 108 / 301] loss:    901.984\n",
            "[Epoch 39 / 50 | Batch: 109 / 301] loss:   1390.430\n",
            "[Epoch 39 / 50 | Batch: 110 / 301] loss:   1133.605\n",
            "[Epoch 39 / 50 | Batch: 111 / 301] loss:    911.780\n",
            "[Epoch 39 / 50 | Batch: 112 / 301] loss:   1012.239\n",
            "[Epoch 39 / 50 | Batch: 113 / 301] loss:   1432.804\n",
            "[Epoch 39 / 50 | Batch: 114 / 301] loss:    960.551\n",
            "[Epoch 39 / 50 | Batch: 115 / 301] loss:   1018.406\n",
            "[Epoch 39 / 50 | Batch: 116 / 301] loss:   1129.567\n",
            "[Epoch 39 / 50 | Batch: 117 / 301] loss:    861.688\n",
            "[Epoch 39 / 50 | Batch: 118 / 301] loss:   1013.330\n",
            "[Epoch 39 / 50 | Batch: 119 / 301] loss:   1214.307\n",
            "[Epoch 39 / 50 | Batch: 120 / 301] loss:    877.895\n",
            "[Epoch 39 / 50 | Batch: 121 / 301] loss:    892.958\n",
            "[Epoch 39 / 50 | Batch: 122 / 301] loss:   1266.070\n",
            "[Epoch 39 / 50 | Batch: 123 / 301] loss:   1136.825\n",
            "[Epoch 39 / 50 | Batch: 124 / 301] loss:   1049.932\n",
            "[Epoch 39 / 50 | Batch: 125 / 301] loss:   1069.943\n",
            "[Epoch 39 / 50 | Batch: 126 / 301] loss:   1021.520\n",
            "[Epoch 39 / 50 | Batch: 127 / 301] loss:   1394.582\n",
            "[Epoch 39 / 50 | Batch: 128 / 301] loss:   1185.630\n",
            "[Epoch 39 / 50 | Batch: 129 / 301] loss:   1156.844\n",
            "[Epoch 39 / 50 | Batch: 130 / 301] loss:    745.486\n",
            "[Epoch 39 / 50 | Batch: 131 / 301] loss:   1293.188\n",
            "[Epoch 39 / 50 | Batch: 132 / 301] loss:   1159.147\n",
            "[Epoch 39 / 50 | Batch: 133 / 301] loss:    970.007\n",
            "[Epoch 39 / 50 | Batch: 134 / 301] loss:   1357.061\n",
            "[Epoch 39 / 50 | Batch: 135 / 301] loss:   1194.593\n",
            "[Epoch 39 / 50 | Batch: 136 / 301] loss:   1421.006\n",
            "[Epoch 39 / 50 | Batch: 137 / 301] loss:   1297.796\n",
            "[Epoch 39 / 50 | Batch: 138 / 301] loss:    933.352\n",
            "[Epoch 39 / 50 | Batch: 139 / 301] loss:   1332.857\n",
            "[Epoch 39 / 50 | Batch: 140 / 301] loss:    867.897\n",
            "[Epoch 39 / 50 | Batch: 141 / 301] loss:    964.849\n",
            "[Epoch 39 / 50 | Batch: 142 / 301] loss:   1485.099\n",
            "[Epoch 39 / 50 | Batch: 143 / 301] loss:   1233.123\n",
            "[Epoch 39 / 50 | Batch: 144 / 301] loss:    887.586\n",
            "[Epoch 39 / 50 | Batch: 145 / 301] loss:   1279.134\n",
            "[Epoch 39 / 50 | Batch: 146 / 301] loss:   1199.566\n",
            "[Epoch 39 / 50 | Batch: 147 / 301] loss:   1093.782\n",
            "[Epoch 39 / 50 | Batch: 148 / 301] loss:   1283.102\n",
            "[Epoch 39 / 50 | Batch: 149 / 301] loss:   1201.943\n",
            "[Epoch 39 / 50 | Batch: 150 / 301] loss:   1044.725\n",
            "[Epoch 39 / 50 | Batch: 151 / 301] loss:   1812.445\n",
            "[Epoch 39 / 50 | Batch: 152 / 301] loss:    933.063\n",
            "[Epoch 39 / 50 | Batch: 153 / 301] loss:   1705.248\n",
            "[Epoch 39 / 50 | Batch: 154 / 301] loss:    948.410\n",
            "[Epoch 39 / 50 | Batch: 155 / 301] loss:    961.659\n",
            "[Epoch 39 / 50 | Batch: 156 / 301] loss:   1026.377\n",
            "[Epoch 39 / 50 | Batch: 157 / 301] loss:   1485.923\n",
            "[Epoch 39 / 50 | Batch: 158 / 301] loss:   1269.506\n",
            "[Epoch 39 / 50 | Batch: 159 / 301] loss:   1267.112\n",
            "[Epoch 39 / 50 | Batch: 160 / 301] loss:   1266.246\n",
            "[Epoch 39 / 50 | Batch: 161 / 301] loss:   1017.689\n",
            "[Epoch 39 / 50 | Batch: 162 / 301] loss:    887.762\n",
            "[Epoch 39 / 50 | Batch: 163 / 301] loss:    934.248\n",
            "[Epoch 39 / 50 | Batch: 164 / 301] loss:    988.921\n",
            "[Epoch 39 / 50 | Batch: 165 / 301] loss:    842.699\n",
            "[Epoch 39 / 50 | Batch: 166 / 301] loss:   1629.257\n",
            "[Epoch 39 / 50 | Batch: 167 / 301] loss:    929.409\n",
            "[Epoch 39 / 50 | Batch: 168 / 301] loss:   1269.943\n",
            "[Epoch 39 / 50 | Batch: 169 / 301] loss:   1019.298\n",
            "[Epoch 39 / 50 | Batch: 170 / 301] loss:    948.479\n",
            "[Epoch 39 / 50 | Batch: 171 / 301] loss:   1206.817\n",
            "[Epoch 39 / 50 | Batch: 172 / 301] loss:    947.533\n",
            "[Epoch 39 / 50 | Batch: 173 / 301] loss:    707.892\n",
            "[Epoch 39 / 50 | Batch: 174 / 301] loss:   1295.274\n",
            "[Epoch 39 / 50 | Batch: 175 / 301] loss:   1060.667\n",
            "[Epoch 39 / 50 | Batch: 176 / 301] loss:   1024.873\n",
            "[Epoch 39 / 50 | Batch: 177 / 301] loss:   1273.973\n",
            "[Epoch 39 / 50 | Batch: 178 / 301] loss:   1296.099\n",
            "[Epoch 39 / 50 | Batch: 179 / 301] loss:   1121.764\n",
            "[Epoch 39 / 50 | Batch: 180 / 301] loss:   1042.069\n",
            "[Epoch 39 / 50 | Batch: 181 / 301] loss:   1633.034\n",
            "[Epoch 39 / 50 | Batch: 182 / 301] loss:   1271.082\n",
            "[Epoch 39 / 50 | Batch: 183 / 301] loss:   1244.639\n",
            "[Epoch 39 / 50 | Batch: 184 / 301] loss:   1576.636\n",
            "[Epoch 39 / 50 | Batch: 185 / 301] loss:   1127.483\n",
            "[Epoch 39 / 50 | Batch: 186 / 301] loss:   1133.133\n",
            "[Epoch 39 / 50 | Batch: 187 / 301] loss:   1034.188\n",
            "[Epoch 39 / 50 | Batch: 188 / 301] loss:   1493.078\n",
            "[Epoch 39 / 50 | Batch: 189 / 301] loss:    786.243\n",
            "[Epoch 39 / 50 | Batch: 190 / 301] loss:   1136.692\n",
            "[Epoch 39 / 50 | Batch: 191 / 301] loss:    963.556\n",
            "[Epoch 39 / 50 | Batch: 192 / 301] loss:   1026.898\n",
            "[Epoch 39 / 50 | Batch: 193 / 301] loss:   1096.458\n",
            "[Epoch 39 / 50 | Batch: 194 / 301] loss:   1139.889\n",
            "[Epoch 39 / 50 | Batch: 195 / 301] loss:   1343.293\n",
            "[Epoch 39 / 50 | Batch: 196 / 301] loss:   1169.286\n",
            "[Epoch 39 / 50 | Batch: 197 / 301] loss:   1220.083\n",
            "[Epoch 39 / 50 | Batch: 198 / 301] loss:   1211.576\n",
            "[Epoch 39 / 50 | Batch: 199 / 301] loss:   1041.232\n",
            "[Epoch 39 / 50 | Batch: 200 / 301] loss:   1257.989\n",
            "[Epoch 39 / 50 | Batch: 201 / 301] loss:   1309.963\n",
            "[Epoch 39 / 50 | Batch: 202 / 301] loss:   1289.545\n",
            "[Epoch 39 / 50 | Batch: 203 / 301] loss:    958.715\n",
            "[Epoch 39 / 50 | Batch: 204 / 301] loss:   1154.194\n",
            "[Epoch 39 / 50 | Batch: 205 / 301] loss:   1431.002\n",
            "[Epoch 39 / 50 | Batch: 206 / 301] loss:   1197.428\n",
            "[Epoch 39 / 50 | Batch: 207 / 301] loss:   1056.037\n",
            "[Epoch 39 / 50 | Batch: 208 / 301] loss:   1254.409\n",
            "[Epoch 39 / 50 | Batch: 209 / 301] loss:   1599.028\n",
            "[Epoch 39 / 50 | Batch: 210 / 301] loss:   1043.362\n",
            "[Epoch 39 / 50 | Batch: 211 / 301] loss:    992.051\n",
            "[Epoch 39 / 50 | Batch: 212 / 301] loss:    964.348\n",
            "[Epoch 39 / 50 | Batch: 213 / 301] loss:    988.032\n",
            "[Epoch 39 / 50 | Batch: 214 / 301] loss:   1164.233\n",
            "[Epoch 39 / 50 | Batch: 215 / 301] loss:   1313.696\n",
            "[Epoch 39 / 50 | Batch: 216 / 301] loss:   1064.438\n",
            "[Epoch 39 / 50 | Batch: 217 / 301] loss:   1130.277\n",
            "[Epoch 39 / 50 | Batch: 218 / 301] loss:   1124.142\n",
            "[Epoch 39 / 50 | Batch: 219 / 301] loss:   1064.585\n",
            "[Epoch 39 / 50 | Batch: 220 / 301] loss:    988.150\n",
            "[Epoch 39 / 50 | Batch: 221 / 301] loss:   1027.284\n",
            "[Epoch 39 / 50 | Batch: 222 / 301] loss:   1317.289\n",
            "[Epoch 39 / 50 | Batch: 223 / 301] loss:   1292.721\n",
            "[Epoch 39 / 50 | Batch: 224 / 301] loss:   1026.802\n",
            "[Epoch 39 / 50 | Batch: 225 / 301] loss:   1054.093\n",
            "[Epoch 39 / 50 | Batch: 226 / 301] loss:   1036.020\n",
            "[Epoch 39 / 50 | Batch: 227 / 301] loss:    916.994\n",
            "[Epoch 39 / 50 | Batch: 228 / 301] loss:   1424.983\n",
            "[Epoch 39 / 50 | Batch: 229 / 301] loss:    949.541\n",
            "[Epoch 39 / 50 | Batch: 230 / 301] loss:   1408.241\n",
            "[Epoch 39 / 50 | Batch: 231 / 301] loss:   1388.792\n",
            "[Epoch 39 / 50 | Batch: 232 / 301] loss:   1029.157\n",
            "[Epoch 39 / 50 | Batch: 233 / 301] loss:   1403.766\n",
            "[Epoch 39 / 50 | Batch: 234 / 301] loss:   1351.248\n",
            "[Epoch 39 / 50 | Batch: 235 / 301] loss:    994.667\n",
            "[Epoch 39 / 50 | Batch: 236 / 301] loss:   1347.044\n",
            "[Epoch 39 / 50 | Batch: 237 / 301] loss:   1122.928\n",
            "[Epoch 39 / 50 | Batch: 238 / 301] loss:   1114.196\n",
            "[Epoch 39 / 50 | Batch: 239 / 301] loss:   1250.518\n",
            "[Epoch 39 / 50 | Batch: 240 / 301] loss:   1108.740\n",
            "[Epoch 39 / 50 | Batch: 241 / 301] loss:   1248.027\n",
            "[Epoch 39 / 50 | Batch: 242 / 301] loss:   1398.604\n",
            "[Epoch 39 / 50 | Batch: 243 / 301] loss:   1246.427\n",
            "[Epoch 39 / 50 | Batch: 244 / 301] loss:   1527.604\n",
            "[Epoch 39 / 50 | Batch: 245 / 301] loss:   1206.047\n",
            "[Epoch 39 / 50 | Batch: 246 / 301] loss:    784.058\n",
            "[Epoch 39 / 50 | Batch: 247 / 301] loss:    853.183\n",
            "[Epoch 39 / 50 | Batch: 248 / 301] loss:   1258.593\n",
            "[Epoch 39 / 50 | Batch: 249 / 301] loss:   1245.542\n",
            "[Epoch 39 / 50 | Batch: 250 / 301] loss:   1238.120\n",
            "[Epoch 39 / 50 | Batch: 251 / 301] loss:   1799.684\n",
            "[Epoch 39 / 50 | Batch: 252 / 301] loss:   1466.649\n",
            "[Epoch 39 / 50 | Batch: 253 / 301] loss:   1102.424\n",
            "[Epoch 39 / 50 | Batch: 254 / 301] loss:   1612.035\n",
            "[Epoch 39 / 50 | Batch: 255 / 301] loss:   1245.655\n",
            "[Epoch 39 / 50 | Batch: 256 / 301] loss:   1017.190\n",
            "[Epoch 39 / 50 | Batch: 257 / 301] loss:   1397.780\n",
            "[Epoch 39 / 50 | Batch: 258 / 301] loss:   1069.217\n",
            "[Epoch 39 / 50 | Batch: 259 / 301] loss:   1414.948\n",
            "[Epoch 39 / 50 | Batch: 260 / 301] loss:   1583.718\n",
            "[Epoch 39 / 50 | Batch: 261 / 301] loss:   1441.636\n",
            "[Epoch 39 / 50 | Batch: 262 / 301] loss:    919.700\n",
            "[Epoch 39 / 50 | Batch: 263 / 301] loss:   1343.226\n",
            "[Epoch 39 / 50 | Batch: 264 / 301] loss:   1392.295\n",
            "[Epoch 39 / 50 | Batch: 265 / 301] loss:    919.690\n",
            "[Epoch 39 / 50 | Batch: 266 / 301] loss:   1393.037\n",
            "[Epoch 39 / 50 | Batch: 267 / 301] loss:   1333.180\n",
            "[Epoch 39 / 50 | Batch: 268 / 301] loss:   1151.934\n",
            "[Epoch 39 / 50 | Batch: 269 / 301] loss:   1020.504\n",
            "[Epoch 39 / 50 | Batch: 270 / 301] loss:   1126.057\n",
            "[Epoch 39 / 50 | Batch: 271 / 301] loss:   1372.124\n",
            "[Epoch 39 / 50 | Batch: 272 / 301] loss:    827.098\n",
            "[Epoch 39 / 50 | Batch: 273 / 301] loss:   1121.482\n",
            "[Epoch 39 / 50 | Batch: 274 / 301] loss:   1131.143\n",
            "[Epoch 39 / 50 | Batch: 275 / 301] loss:   1325.253\n",
            "[Epoch 39 / 50 | Batch: 276 / 301] loss:   1218.348\n",
            "[Epoch 39 / 50 | Batch: 277 / 301] loss:   1495.302\n",
            "[Epoch 39 / 50 | Batch: 278 / 301] loss:   1226.981\n",
            "[Epoch 39 / 50 | Batch: 279 / 301] loss:   1156.481\n",
            "[Epoch 39 / 50 | Batch: 280 / 301] loss:    907.570\n",
            "[Epoch 39 / 50 | Batch: 281 / 301] loss:   1220.507\n",
            "[Epoch 39 / 50 | Batch: 282 / 301] loss:   1148.444\n",
            "[Epoch 39 / 50 | Batch: 283 / 301] loss:   1336.384\n",
            "[Epoch 39 / 50 | Batch: 284 / 301] loss:   1062.845\n",
            "[Epoch 39 / 50 | Batch: 285 / 301] loss:   1022.906\n",
            "[Epoch 39 / 50 | Batch: 286 / 301] loss:   1023.830\n",
            "[Epoch 39 / 50 | Batch: 287 / 301] loss:   1108.079\n",
            "[Epoch 39 / 50 | Batch: 288 / 301] loss:    964.807\n",
            "[Epoch 39 / 50 | Batch: 289 / 301] loss:    870.879\n",
            "[Epoch 39 / 50 | Batch: 290 / 301] loss:   1346.056\n",
            "[Epoch 39 / 50 | Batch: 291 / 301] loss:   1348.450\n",
            "[Epoch 39 / 50 | Batch: 292 / 301] loss:   1020.706\n",
            "[Epoch 39 / 50 | Batch: 293 / 301] loss:   1310.038\n",
            "[Epoch 39 / 50 | Batch: 294 / 301] loss:   1106.794\n",
            "[Epoch 39 / 50 | Batch: 295 / 301] loss:   1013.586\n",
            "[Epoch 39 / 50 | Batch: 296 / 301] loss:   1308.358\n",
            "[Epoch 39 / 50 | Batch: 297 / 301] loss:   1205.787\n",
            "[Epoch 39 / 50 | Batch: 298 / 301] loss:   1151.601\n",
            "[Epoch 39 / 50 | Batch: 299 / 301] loss:   1584.667\n",
            "[Epoch 39 / 50 | Batch: 300 / 301] loss:   1016.267\n",
            "[Epoch 39 / 50 | Batch: 301 / 301] loss:    557.736\n",
            "Epoch loss: 1168.61806\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4515.019\n",
            "[Validation] [Batch  2 / 15] dev loss:   6649.938\n",
            "[Validation] [Batch  3 / 15] dev loss:   6781.142\n",
            "[Validation] [Batch  4 / 15] dev loss:   5254.896\n",
            "[Validation] [Batch  5 / 15] dev loss:   5208.232\n",
            "[Validation] [Batch  6 / 15] dev loss:   8761.579\n",
            "[Validation] [Batch  7 / 15] dev loss:   6176.525\n",
            "[Validation] [Batch  8 / 15] dev loss:   4109.166\n",
            "[Validation] [Batch  9 / 15] dev loss:   3517.583\n",
            "[Validation] [Batch 10 / 15] dev loss:  15828.840\n",
            "[Validation] [Batch 11 / 15] dev loss:  13138.561\n",
            "[Validation] [Batch 12 / 15] dev loss:  10930.459\n",
            "[Validation] [Batch 13 / 15] dev loss:   4678.346\n",
            "[Validation] [Batch 14 / 15] dev loss:  23774.549\n",
            "[Validation] [Batch 15 / 15] dev loss:   1972.797\n",
            "Dev loss 8086.50877\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 40 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 40 / 50 | Batch:  1 / 301] loss:   1378.958\n",
            "[Epoch 40 / 50 | Batch:  2 / 301] loss:   1105.169\n",
            "[Epoch 40 / 50 | Batch:  3 / 301] loss:   1098.551\n",
            "[Epoch 40 / 50 | Batch:  4 / 301] loss:   1126.010\n",
            "[Epoch 40 / 50 | Batch:  5 / 301] loss:   1170.708\n",
            "[Epoch 40 / 50 | Batch:  6 / 301] loss:   1240.254\n",
            "[Epoch 40 / 50 | Batch:  7 / 301] loss:   1693.771\n",
            "[Epoch 40 / 50 | Batch:  8 / 301] loss:   1094.799\n",
            "[Epoch 40 / 50 | Batch:  9 / 301] loss:   1153.547\n",
            "[Epoch 40 / 50 | Batch: 10 / 301] loss:   1124.842\n",
            "[Epoch 40 / 50 | Batch: 11 / 301] loss:    923.251\n",
            "[Epoch 40 / 50 | Batch: 12 / 301] loss:   1454.479\n",
            "[Epoch 40 / 50 | Batch: 13 / 301] loss:   1468.255\n",
            "[Epoch 40 / 50 | Batch: 14 / 301] loss:   1410.945\n",
            "[Epoch 40 / 50 | Batch: 15 / 301] loss:   1063.104\n",
            "[Epoch 40 / 50 | Batch: 16 / 301] loss:    817.653\n",
            "[Epoch 40 / 50 | Batch: 17 / 301] loss:   1353.054\n",
            "[Epoch 40 / 50 | Batch: 18 / 301] loss:   1131.167\n",
            "[Epoch 40 / 50 | Batch: 19 / 301] loss:    894.341\n",
            "[Epoch 40 / 50 | Batch: 20 / 301] loss:   1126.395\n",
            "[Epoch 40 / 50 | Batch: 21 / 301] loss:   1160.841\n",
            "[Epoch 40 / 50 | Batch: 22 / 301] loss:   1079.683\n",
            "[Epoch 40 / 50 | Batch: 23 / 301] loss:    931.698\n",
            "[Epoch 40 / 50 | Batch: 24 / 301] loss:   1084.948\n",
            "[Epoch 40 / 50 | Batch: 25 / 301] loss:   1065.151\n",
            "[Epoch 40 / 50 | Batch: 26 / 301] loss:   1022.405\n",
            "[Epoch 40 / 50 | Batch: 27 / 301] loss:   1183.072\n",
            "[Epoch 40 / 50 | Batch: 28 / 301] loss:   1100.270\n",
            "[Epoch 40 / 50 | Batch: 29 / 301] loss:    946.320\n",
            "[Epoch 40 / 50 | Batch: 30 / 301] loss:   1307.367\n",
            "[Epoch 40 / 50 | Batch: 31 / 301] loss:   1405.307\n",
            "[Epoch 40 / 50 | Batch: 32 / 301] loss:   1196.583\n",
            "[Epoch 40 / 50 | Batch: 33 / 301] loss:   1080.297\n",
            "[Epoch 40 / 50 | Batch: 34 / 301] loss:   1284.383\n",
            "[Epoch 40 / 50 | Batch: 35 / 301] loss:    975.270\n",
            "[Epoch 40 / 50 | Batch: 36 / 301] loss:   1460.272\n",
            "[Epoch 40 / 50 | Batch: 37 / 301] loss:    682.568\n",
            "[Epoch 40 / 50 | Batch: 38 / 301] loss:    835.573\n",
            "[Epoch 40 / 50 | Batch: 39 / 301] loss:   1227.863\n",
            "[Epoch 40 / 50 | Batch: 40 / 301] loss:   1066.758\n",
            "[Epoch 40 / 50 | Batch: 41 / 301] loss:   1017.367\n",
            "[Epoch 40 / 50 | Batch: 42 / 301] loss:   1201.539\n",
            "[Epoch 40 / 50 | Batch: 43 / 301] loss:    902.928\n",
            "[Epoch 40 / 50 | Batch: 44 / 301] loss:   1215.108\n",
            "[Epoch 40 / 50 | Batch: 45 / 301] loss:   1309.171\n",
            "[Epoch 40 / 50 | Batch: 46 / 301] loss:   1177.858\n",
            "[Epoch 40 / 50 | Batch: 47 / 301] loss:   1466.190\n",
            "[Epoch 40 / 50 | Batch: 48 / 301] loss:    957.282\n",
            "[Epoch 40 / 50 | Batch: 49 / 301] loss:    988.309\n",
            "[Epoch 40 / 50 | Batch: 50 / 301] loss:   1233.110\n",
            "[Epoch 40 / 50 | Batch: 51 / 301] loss:   1434.578\n",
            "[Epoch 40 / 50 | Batch: 52 / 301] loss:    821.481\n",
            "[Epoch 40 / 50 | Batch: 53 / 301] loss:    904.115\n",
            "[Epoch 40 / 50 | Batch: 54 / 301] loss:   1228.054\n",
            "[Epoch 40 / 50 | Batch: 55 / 301] loss:   1204.043\n",
            "[Epoch 40 / 50 | Batch: 56 / 301] loss:   1228.620\n",
            "[Epoch 40 / 50 | Batch: 57 / 301] loss:   1166.359\n",
            "[Epoch 40 / 50 | Batch: 58 / 301] loss:    955.336\n",
            "[Epoch 40 / 50 | Batch: 59 / 301] loss:   1513.013\n",
            "[Epoch 40 / 50 | Batch: 60 / 301] loss:   1074.551\n",
            "[Epoch 40 / 50 | Batch: 61 / 301] loss:    801.653\n",
            "[Epoch 40 / 50 | Batch: 62 / 301] loss:   1670.934\n",
            "[Epoch 40 / 50 | Batch: 63 / 301] loss:    908.420\n",
            "[Epoch 40 / 50 | Batch: 64 / 301] loss:   1130.619\n",
            "[Epoch 40 / 50 | Batch: 65 / 301] loss:   1312.885\n",
            "[Epoch 40 / 50 | Batch: 66 / 301] loss:   1211.620\n",
            "[Epoch 40 / 50 | Batch: 67 / 301] loss:   1048.110\n",
            "[Epoch 40 / 50 | Batch: 68 / 301] loss:   1182.324\n",
            "[Epoch 40 / 50 | Batch: 69 / 301] loss:   1247.049\n",
            "[Epoch 40 / 50 | Batch: 70 / 301] loss:   1147.642\n",
            "[Epoch 40 / 50 | Batch: 71 / 301] loss:   1248.466\n",
            "[Epoch 40 / 50 | Batch: 72 / 301] loss:   1015.126\n",
            "[Epoch 40 / 50 | Batch: 73 / 301] loss:   1280.950\n",
            "[Epoch 40 / 50 | Batch: 74 / 301] loss:   1063.267\n",
            "[Epoch 40 / 50 | Batch: 75 / 301] loss:   1359.864\n",
            "[Epoch 40 / 50 | Batch: 76 / 301] loss:   1163.556\n",
            "[Epoch 40 / 50 | Batch: 77 / 301] loss:   1477.823\n",
            "[Epoch 40 / 50 | Batch: 78 / 301] loss:   1350.053\n",
            "[Epoch 40 / 50 | Batch: 79 / 301] loss:   1305.225\n",
            "[Epoch 40 / 50 | Batch: 80 / 301] loss:   1090.271\n",
            "[Epoch 40 / 50 | Batch: 81 / 301] loss:   1456.121\n",
            "[Epoch 40 / 50 | Batch: 82 / 301] loss:   1350.905\n",
            "[Epoch 40 / 50 | Batch: 83 / 301] loss:   1090.131\n",
            "[Epoch 40 / 50 | Batch: 84 / 301] loss:   1525.427\n",
            "[Epoch 40 / 50 | Batch: 85 / 301] loss:   1164.150\n",
            "[Epoch 40 / 50 | Batch: 86 / 301] loss:   1449.535\n",
            "[Epoch 40 / 50 | Batch: 87 / 301] loss:   1493.656\n",
            "[Epoch 40 / 50 | Batch: 88 / 301] loss:   1507.519\n",
            "[Epoch 40 / 50 | Batch: 89 / 301] loss:   1348.736\n",
            "[Epoch 40 / 50 | Batch: 90 / 301] loss:   1259.895\n",
            "[Epoch 40 / 50 | Batch: 91 / 301] loss:   1192.864\n",
            "[Epoch 40 / 50 | Batch: 92 / 301] loss:   1304.758\n",
            "[Epoch 40 / 50 | Batch: 93 / 301] loss:   1131.085\n",
            "[Epoch 40 / 50 | Batch: 94 / 301] loss:   1077.833\n",
            "[Epoch 40 / 50 | Batch: 95 / 301] loss:   1450.765\n",
            "[Epoch 40 / 50 | Batch: 96 / 301] loss:   1264.372\n",
            "[Epoch 40 / 50 | Batch: 97 / 301] loss:   1076.117\n",
            "[Epoch 40 / 50 | Batch: 98 / 301] loss:    839.315\n",
            "[Epoch 40 / 50 | Batch: 99 / 301] loss:   1250.207\n",
            "[Epoch 40 / 50 | Batch: 100 / 301] loss:   1165.031\n",
            "[Epoch 40 / 50 | Batch: 101 / 301] loss:   1418.469\n",
            "[Epoch 40 / 50 | Batch: 102 / 301] loss:    820.578\n",
            "[Epoch 40 / 50 | Batch: 103 / 301] loss:    983.247\n",
            "[Epoch 40 / 50 | Batch: 104 / 301] loss:   1218.349\n",
            "[Epoch 40 / 50 | Batch: 105 / 301] loss:   1283.083\n",
            "[Epoch 40 / 50 | Batch: 106 / 301] loss:   1041.237\n",
            "[Epoch 40 / 50 | Batch: 107 / 301] loss:   1148.640\n",
            "[Epoch 40 / 50 | Batch: 108 / 301] loss:   1258.542\n",
            "[Epoch 40 / 50 | Batch: 109 / 301] loss:    982.539\n",
            "[Epoch 40 / 50 | Batch: 110 / 301] loss:   1062.740\n",
            "[Epoch 40 / 50 | Batch: 111 / 301] loss:   1106.728\n",
            "[Epoch 40 / 50 | Batch: 112 / 301] loss:   1047.414\n",
            "[Epoch 40 / 50 | Batch: 113 / 301] loss:   1224.556\n",
            "[Epoch 40 / 50 | Batch: 114 / 301] loss:   1313.195\n",
            "[Epoch 40 / 50 | Batch: 115 / 301] loss:   1324.093\n",
            "[Epoch 40 / 50 | Batch: 116 / 301] loss:   1168.531\n",
            "[Epoch 40 / 50 | Batch: 117 / 301] loss:    812.872\n",
            "[Epoch 40 / 50 | Batch: 118 / 301] loss:   1013.511\n",
            "[Epoch 40 / 50 | Batch: 119 / 301] loss:   1204.528\n",
            "[Epoch 40 / 50 | Batch: 120 / 301] loss:   1005.045\n",
            "[Epoch 40 / 50 | Batch: 121 / 301] loss:   1353.718\n",
            "[Epoch 40 / 50 | Batch: 122 / 301] loss:    898.976\n",
            "[Epoch 40 / 50 | Batch: 123 / 301] loss:   1399.394\n",
            "[Epoch 40 / 50 | Batch: 124 / 301] loss:   1118.947\n",
            "[Epoch 40 / 50 | Batch: 125 / 301] loss:   1200.352\n",
            "[Epoch 40 / 50 | Batch: 126 / 301] loss:   1561.742\n",
            "[Epoch 40 / 50 | Batch: 127 / 301] loss:   1063.690\n",
            "[Epoch 40 / 50 | Batch: 128 / 301] loss:   1096.483\n",
            "[Epoch 40 / 50 | Batch: 129 / 301] loss:   1127.275\n",
            "[Epoch 40 / 50 | Batch: 130 / 301] loss:   1628.129\n",
            "[Epoch 40 / 50 | Batch: 131 / 301] loss:   1050.464\n",
            "[Epoch 40 / 50 | Batch: 132 / 301] loss:   1006.363\n",
            "[Epoch 40 / 50 | Batch: 133 / 301] loss:   1454.635\n",
            "[Epoch 40 / 50 | Batch: 134 / 301] loss:    851.267\n",
            "[Epoch 40 / 50 | Batch: 135 / 301] loss:   1631.779\n",
            "[Epoch 40 / 50 | Batch: 136 / 301] loss:   1121.718\n",
            "[Epoch 40 / 50 | Batch: 137 / 301] loss:   1040.981\n",
            "[Epoch 40 / 50 | Batch: 138 / 301] loss:   1235.050\n",
            "[Epoch 40 / 50 | Batch: 139 / 301] loss:   1010.393\n",
            "[Epoch 40 / 50 | Batch: 140 / 301] loss:   1125.275\n",
            "[Epoch 40 / 50 | Batch: 141 / 301] loss:   1206.412\n",
            "[Epoch 40 / 50 | Batch: 142 / 301] loss:   1263.682\n",
            "[Epoch 40 / 50 | Batch: 143 / 301] loss:   1151.541\n",
            "[Epoch 40 / 50 | Batch: 144 / 301] loss:    954.851\n",
            "[Epoch 40 / 50 | Batch: 145 / 301] loss:    810.195\n",
            "[Epoch 40 / 50 | Batch: 146 / 301] loss:    946.153\n",
            "[Epoch 40 / 50 | Batch: 147 / 301] loss:   1183.462\n",
            "[Epoch 40 / 50 | Batch: 148 / 301] loss:   1191.256\n",
            "[Epoch 40 / 50 | Batch: 149 / 301] loss:   1464.035\n",
            "[Epoch 40 / 50 | Batch: 150 / 301] loss:   1355.964\n",
            "[Epoch 40 / 50 | Batch: 151 / 301] loss:    848.641\n",
            "[Epoch 40 / 50 | Batch: 152 / 301] loss:   1212.514\n",
            "[Epoch 40 / 50 | Batch: 153 / 301] loss:   1061.787\n",
            "[Epoch 40 / 50 | Batch: 154 / 301] loss:   1093.490\n",
            "[Epoch 40 / 50 | Batch: 155 / 301] loss:   1051.990\n",
            "[Epoch 40 / 50 | Batch: 156 / 301] loss:    787.292\n",
            "[Epoch 40 / 50 | Batch: 157 / 301] loss:   1504.728\n",
            "[Epoch 40 / 50 | Batch: 158 / 301] loss:   1009.611\n",
            "[Epoch 40 / 50 | Batch: 159 / 301] loss:   1134.188\n",
            "[Epoch 40 / 50 | Batch: 160 / 301] loss:   1128.022\n",
            "[Epoch 40 / 50 | Batch: 161 / 301] loss:   1252.645\n",
            "[Epoch 40 / 50 | Batch: 162 / 301] loss:   1180.774\n",
            "[Epoch 40 / 50 | Batch: 163 / 301] loss:   1008.765\n",
            "[Epoch 40 / 50 | Batch: 164 / 301] loss:   1005.971\n",
            "[Epoch 40 / 50 | Batch: 165 / 301] loss:   1948.249\n",
            "[Epoch 40 / 50 | Batch: 166 / 301] loss:   1121.326\n",
            "[Epoch 40 / 50 | Batch: 167 / 301] loss:   1080.266\n",
            "[Epoch 40 / 50 | Batch: 168 / 301] loss:   1346.382\n",
            "[Epoch 40 / 50 | Batch: 169 / 301] loss:   1247.259\n",
            "[Epoch 40 / 50 | Batch: 170 / 301] loss:   1218.476\n",
            "[Epoch 40 / 50 | Batch: 171 / 301] loss:   1232.212\n",
            "[Epoch 40 / 50 | Batch: 172 / 301] loss:   1703.219\n",
            "[Epoch 40 / 50 | Batch: 173 / 301] loss:    918.022\n",
            "[Epoch 40 / 50 | Batch: 174 / 301] loss:    958.213\n",
            "[Epoch 40 / 50 | Batch: 175 / 301] loss:   1332.820\n",
            "[Epoch 40 / 50 | Batch: 176 / 301] loss:   1026.909\n",
            "[Epoch 40 / 50 | Batch: 177 / 301] loss:   1174.756\n",
            "[Epoch 40 / 50 | Batch: 178 / 301] loss:   1006.462\n",
            "[Epoch 40 / 50 | Batch: 179 / 301] loss:   1041.718\n",
            "[Epoch 40 / 50 | Batch: 180 / 301] loss:   1107.270\n",
            "[Epoch 40 / 50 | Batch: 181 / 301] loss:   1178.057\n",
            "[Epoch 40 / 50 | Batch: 182 / 301] loss:   1222.543\n",
            "[Epoch 40 / 50 | Batch: 183 / 301] loss:   1321.265\n",
            "[Epoch 40 / 50 | Batch: 184 / 301] loss:   1308.477\n",
            "[Epoch 40 / 50 | Batch: 185 / 301] loss:    924.578\n",
            "[Epoch 40 / 50 | Batch: 186 / 301] loss:   1593.752\n",
            "[Epoch 40 / 50 | Batch: 187 / 301] loss:    807.311\n",
            "[Epoch 40 / 50 | Batch: 188 / 301] loss:    956.129\n",
            "[Epoch 40 / 50 | Batch: 189 / 301] loss:   1435.746\n",
            "[Epoch 40 / 50 | Batch: 190 / 301] loss:    961.535\n",
            "[Epoch 40 / 50 | Batch: 191 / 301] loss:   1214.967\n",
            "[Epoch 40 / 50 | Batch: 192 / 301] loss:   1107.923\n",
            "[Epoch 40 / 50 | Batch: 193 / 301] loss:    888.802\n",
            "[Epoch 40 / 50 | Batch: 194 / 301] loss:   1047.280\n",
            "[Epoch 40 / 50 | Batch: 195 / 301] loss:    729.571\n",
            "[Epoch 40 / 50 | Batch: 196 / 301] loss:   1403.015\n",
            "[Epoch 40 / 50 | Batch: 197 / 301] loss:   1332.171\n",
            "[Epoch 40 / 50 | Batch: 198 / 301] loss:   1286.337\n",
            "[Epoch 40 / 50 | Batch: 199 / 301] loss:   1146.864\n",
            "[Epoch 40 / 50 | Batch: 200 / 301] loss:   1097.595\n",
            "[Epoch 40 / 50 | Batch: 201 / 301] loss:   1341.911\n",
            "[Epoch 40 / 50 | Batch: 202 / 301] loss:    972.480\n",
            "[Epoch 40 / 50 | Batch: 203 / 301] loss:   1024.894\n",
            "[Epoch 40 / 50 | Batch: 204 / 301] loss:   1122.391\n",
            "[Epoch 40 / 50 | Batch: 205 / 301] loss:    984.301\n",
            "[Epoch 40 / 50 | Batch: 206 / 301] loss:    930.521\n",
            "[Epoch 40 / 50 | Batch: 207 / 301] loss:   1324.682\n",
            "[Epoch 40 / 50 | Batch: 208 / 301] loss:   1064.926\n",
            "[Epoch 40 / 50 | Batch: 209 / 301] loss:   1416.160\n",
            "[Epoch 40 / 50 | Batch: 210 / 301] loss:   1191.647\n",
            "[Epoch 40 / 50 | Batch: 211 / 301] loss:   1409.733\n",
            "[Epoch 40 / 50 | Batch: 212 / 301] loss:   1102.214\n",
            "[Epoch 40 / 50 | Batch: 213 / 301] loss:   1461.907\n",
            "[Epoch 40 / 50 | Batch: 214 / 301] loss:   1205.940\n",
            "[Epoch 40 / 50 | Batch: 215 / 301] loss:    916.577\n",
            "[Epoch 40 / 50 | Batch: 216 / 301] loss:   1728.644\n",
            "[Epoch 40 / 50 | Batch: 217 / 301] loss:    949.486\n",
            "[Epoch 40 / 50 | Batch: 218 / 301] loss:   1261.869\n",
            "[Epoch 40 / 50 | Batch: 219 / 301] loss:   1328.772\n",
            "[Epoch 40 / 50 | Batch: 220 / 301] loss:   1039.505\n",
            "[Epoch 40 / 50 | Batch: 221 / 301] loss:   1133.798\n",
            "[Epoch 40 / 50 | Batch: 222 / 301] loss:   1336.601\n",
            "[Epoch 40 / 50 | Batch: 223 / 301] loss:   1583.975\n",
            "[Epoch 40 / 50 | Batch: 224 / 301] loss:   1397.553\n",
            "[Epoch 40 / 50 | Batch: 225 / 301] loss:   1198.857\n",
            "[Epoch 40 / 50 | Batch: 226 / 301] loss:   1183.619\n",
            "[Epoch 40 / 50 | Batch: 227 / 301] loss:    879.778\n",
            "[Epoch 40 / 50 | Batch: 228 / 301] loss:   1090.006\n",
            "[Epoch 40 / 50 | Batch: 229 / 301] loss:   1299.166\n",
            "[Epoch 40 / 50 | Batch: 230 / 301] loss:   1048.520\n",
            "[Epoch 40 / 50 | Batch: 231 / 301] loss:    987.569\n",
            "[Epoch 40 / 50 | Batch: 232 / 301] loss:    911.976\n",
            "[Epoch 40 / 50 | Batch: 233 / 301] loss:   1437.657\n",
            "[Epoch 40 / 50 | Batch: 234 / 301] loss:   1262.801\n",
            "[Epoch 40 / 50 | Batch: 235 / 301] loss:   1273.793\n",
            "[Epoch 40 / 50 | Batch: 236 / 301] loss:   1035.639\n",
            "[Epoch 40 / 50 | Batch: 237 / 301] loss:    966.707\n",
            "[Epoch 40 / 50 | Batch: 238 / 301] loss:   1099.638\n",
            "[Epoch 40 / 50 | Batch: 239 / 301] loss:   1268.451\n",
            "[Epoch 40 / 50 | Batch: 240 / 301] loss:   1397.252\n",
            "[Epoch 40 / 50 | Batch: 241 / 301] loss:   1277.827\n",
            "[Epoch 40 / 50 | Batch: 242 / 301] loss:   1717.031\n",
            "[Epoch 40 / 50 | Batch: 243 / 301] loss:   1080.480\n",
            "[Epoch 40 / 50 | Batch: 244 / 301] loss:    985.688\n",
            "[Epoch 40 / 50 | Batch: 245 / 301] loss:   1471.086\n",
            "[Epoch 40 / 50 | Batch: 246 / 301] loss:    978.795\n",
            "[Epoch 40 / 50 | Batch: 247 / 301] loss:    948.460\n",
            "[Epoch 40 / 50 | Batch: 248 / 301] loss:   1475.885\n",
            "[Epoch 40 / 50 | Batch: 249 / 301] loss:    916.142\n",
            "[Epoch 40 / 50 | Batch: 250 / 301] loss:   1544.209\n",
            "[Epoch 40 / 50 | Batch: 251 / 301] loss:    859.838\n",
            "[Epoch 40 / 50 | Batch: 252 / 301] loss:   1270.654\n",
            "[Epoch 40 / 50 | Batch: 253 / 301] loss:   1203.422\n",
            "[Epoch 40 / 50 | Batch: 254 / 301] loss:    947.117\n",
            "[Epoch 40 / 50 | Batch: 255 / 301] loss:   1347.215\n",
            "[Epoch 40 / 50 | Batch: 256 / 301] loss:   1336.857\n",
            "[Epoch 40 / 50 | Batch: 257 / 301] loss:   1165.300\n",
            "[Epoch 40 / 50 | Batch: 258 / 301] loss:    947.750\n",
            "[Epoch 40 / 50 | Batch: 259 / 301] loss:   1223.106\n",
            "[Epoch 40 / 50 | Batch: 260 / 301] loss:   1105.696\n",
            "[Epoch 40 / 50 | Batch: 261 / 301] loss:   1012.299\n",
            "[Epoch 40 / 50 | Batch: 262 / 301] loss:   1179.551\n",
            "[Epoch 40 / 50 | Batch: 263 / 301] loss:   1082.171\n",
            "[Epoch 40 / 50 | Batch: 264 / 301] loss:   1252.169\n",
            "[Epoch 40 / 50 | Batch: 265 / 301] loss:   1519.504\n",
            "[Epoch 40 / 50 | Batch: 266 / 301] loss:   1471.674\n",
            "[Epoch 40 / 50 | Batch: 267 / 301] loss:   1531.204\n",
            "[Epoch 40 / 50 | Batch: 268 / 301] loss:   1037.813\n",
            "[Epoch 40 / 50 | Batch: 269 / 301] loss:   1113.252\n",
            "[Epoch 40 / 50 | Batch: 270 / 301] loss:    753.044\n",
            "[Epoch 40 / 50 | Batch: 271 / 301] loss:   1116.365\n",
            "[Epoch 40 / 50 | Batch: 272 / 301] loss:   1269.955\n",
            "[Epoch 40 / 50 | Batch: 273 / 301] loss:   1183.994\n",
            "[Epoch 40 / 50 | Batch: 274 / 301] loss:   1110.856\n",
            "[Epoch 40 / 50 | Batch: 275 / 301] loss:   1877.676\n",
            "[Epoch 40 / 50 | Batch: 276 / 301] loss:    945.047\n",
            "[Epoch 40 / 50 | Batch: 277 / 301] loss:   1037.337\n",
            "[Epoch 40 / 50 | Batch: 278 / 301] loss:   1442.039\n",
            "[Epoch 40 / 50 | Batch: 279 / 301] loss:    955.453\n",
            "[Epoch 40 / 50 | Batch: 280 / 301] loss:   1014.895\n",
            "[Epoch 40 / 50 | Batch: 281 / 301] loss:   1264.686\n",
            "[Epoch 40 / 50 | Batch: 282 / 301] loss:   1050.835\n",
            "[Epoch 40 / 50 | Batch: 283 / 301] loss:   1120.775\n",
            "[Epoch 40 / 50 | Batch: 284 / 301] loss:   1379.026\n",
            "[Epoch 40 / 50 | Batch: 285 / 301] loss:   1111.951\n",
            "[Epoch 40 / 50 | Batch: 286 / 301] loss:   1205.541\n",
            "[Epoch 40 / 50 | Batch: 287 / 301] loss:    995.458\n",
            "[Epoch 40 / 50 | Batch: 288 / 301] loss:    991.550\n",
            "[Epoch 40 / 50 | Batch: 289 / 301] loss:   1147.805\n",
            "[Epoch 40 / 50 | Batch: 290 / 301] loss:    919.410\n",
            "[Epoch 40 / 50 | Batch: 291 / 301] loss:    895.390\n",
            "[Epoch 40 / 50 | Batch: 292 / 301] loss:   1210.383\n",
            "[Epoch 40 / 50 | Batch: 293 / 301] loss:   1139.035\n",
            "[Epoch 40 / 50 | Batch: 294 / 301] loss:    894.615\n",
            "[Epoch 40 / 50 | Batch: 295 / 301] loss:    929.641\n",
            "[Epoch 40 / 50 | Batch: 296 / 301] loss:   1263.234\n",
            "[Epoch 40 / 50 | Batch: 297 / 301] loss:   1296.274\n",
            "[Epoch 40 / 50 | Batch: 298 / 301] loss:   1069.220\n",
            "[Epoch 40 / 50 | Batch: 299 / 301] loss:   1183.936\n",
            "[Epoch 40 / 50 | Batch: 300 / 301] loss:    995.680\n",
            "[Epoch 40 / 50 | Batch: 301 / 301] loss:    620.213\n",
            "Epoch loss: 1169.28429\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4469.287\n",
            "[Validation] [Batch  2 / 15] dev loss:   6339.972\n",
            "[Validation] [Batch  3 / 15] dev loss:   6624.136\n",
            "[Validation] [Batch  4 / 15] dev loss:   5191.924\n",
            "[Validation] [Batch  5 / 15] dev loss:   5537.836\n",
            "[Validation] [Batch  6 / 15] dev loss:  10194.520\n",
            "[Validation] [Batch  7 / 15] dev loss:   6265.958\n",
            "[Validation] [Batch  8 / 15] dev loss:   4230.874\n",
            "[Validation] [Batch  9 / 15] dev loss:   3242.788\n",
            "[Validation] [Batch 10 / 15] dev loss:  15162.310\n",
            "[Validation] [Batch 11 / 15] dev loss:  13078.158\n",
            "[Validation] [Batch 12 / 15] dev loss:  10835.705\n",
            "[Validation] [Batch 13 / 15] dev loss:   4711.020\n",
            "[Validation] [Batch 14 / 15] dev loss:  24307.529\n",
            "[Validation] [Batch 15 / 15] dev loss:   2129.615\n",
            "Dev loss 8154.77531\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 41 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 41 / 50 | Batch:  1 / 301] loss:   1469.190\n",
            "[Epoch 41 / 50 | Batch:  2 / 301] loss:   1162.584\n",
            "[Epoch 41 / 50 | Batch:  3 / 301] loss:   1226.531\n",
            "[Epoch 41 / 50 | Batch:  4 / 301] loss:   1012.293\n",
            "[Epoch 41 / 50 | Batch:  5 / 301] loss:    755.067\n",
            "[Epoch 41 / 50 | Batch:  6 / 301] loss:   1170.145\n",
            "[Epoch 41 / 50 | Batch:  7 / 301] loss:   1103.808\n",
            "[Epoch 41 / 50 | Batch:  8 / 301] loss:   1012.749\n",
            "[Epoch 41 / 50 | Batch:  9 / 301] loss:    956.670\n",
            "[Epoch 41 / 50 | Batch: 10 / 301] loss:   1307.605\n",
            "[Epoch 41 / 50 | Batch: 11 / 301] loss:    963.702\n",
            "[Epoch 41 / 50 | Batch: 12 / 301] loss:   1065.228\n",
            "[Epoch 41 / 50 | Batch: 13 / 301] loss:   1098.875\n",
            "[Epoch 41 / 50 | Batch: 14 / 301] loss:   1254.744\n",
            "[Epoch 41 / 50 | Batch: 15 / 301] loss:   1227.953\n",
            "[Epoch 41 / 50 | Batch: 16 / 301] loss:   1180.892\n",
            "[Epoch 41 / 50 | Batch: 17 / 301] loss:   1080.134\n",
            "[Epoch 41 / 50 | Batch: 18 / 301] loss:   1046.100\n",
            "[Epoch 41 / 50 | Batch: 19 / 301] loss:   1075.733\n",
            "[Epoch 41 / 50 | Batch: 20 / 301] loss:    919.028\n",
            "[Epoch 41 / 50 | Batch: 21 / 301] loss:   1182.010\n",
            "[Epoch 41 / 50 | Batch: 22 / 301] loss:   1250.758\n",
            "[Epoch 41 / 50 | Batch: 23 / 301] loss:   1186.579\n",
            "[Epoch 41 / 50 | Batch: 24 / 301] loss:    937.598\n",
            "[Epoch 41 / 50 | Batch: 25 / 301] loss:   1042.014\n",
            "[Epoch 41 / 50 | Batch: 26 / 301] loss:   1008.741\n",
            "[Epoch 41 / 50 | Batch: 27 / 301] loss:   1023.238\n",
            "[Epoch 41 / 50 | Batch: 28 / 301] loss:   1266.390\n",
            "[Epoch 41 / 50 | Batch: 29 / 301] loss:   1397.676\n",
            "[Epoch 41 / 50 | Batch: 30 / 301] loss:   1437.202\n",
            "[Epoch 41 / 50 | Batch: 31 / 301] loss:   1410.421\n",
            "[Epoch 41 / 50 | Batch: 32 / 301] loss:   1169.429\n",
            "[Epoch 41 / 50 | Batch: 33 / 301] loss:   1031.920\n",
            "[Epoch 41 / 50 | Batch: 34 / 301] loss:   1201.197\n",
            "[Epoch 41 / 50 | Batch: 35 / 301] loss:    987.217\n",
            "[Epoch 41 / 50 | Batch: 36 / 301] loss:   1191.278\n",
            "[Epoch 41 / 50 | Batch: 37 / 301] loss:    767.002\n",
            "[Epoch 41 / 50 | Batch: 38 / 301] loss:   1304.885\n",
            "[Epoch 41 / 50 | Batch: 39 / 301] loss:   1152.017\n",
            "[Epoch 41 / 50 | Batch: 40 / 301] loss:    944.824\n",
            "[Epoch 41 / 50 | Batch: 41 / 301] loss:   1075.056\n",
            "[Epoch 41 / 50 | Batch: 42 / 301] loss:   1167.897\n",
            "[Epoch 41 / 50 | Batch: 43 / 301] loss:   1260.817\n",
            "[Epoch 41 / 50 | Batch: 44 / 301] loss:   1070.946\n",
            "[Epoch 41 / 50 | Batch: 45 / 301] loss:    926.319\n",
            "[Epoch 41 / 50 | Batch: 46 / 301] loss:    940.005\n",
            "[Epoch 41 / 50 | Batch: 47 / 301] loss:   1547.098\n",
            "[Epoch 41 / 50 | Batch: 48 / 301] loss:   1121.920\n",
            "[Epoch 41 / 50 | Batch: 49 / 301] loss:   1396.066\n",
            "[Epoch 41 / 50 | Batch: 50 / 301] loss:   1179.551\n",
            "[Epoch 41 / 50 | Batch: 51 / 301] loss:   1207.032\n",
            "[Epoch 41 / 50 | Batch: 52 / 301] loss:   1353.520\n",
            "[Epoch 41 / 50 | Batch: 53 / 301] loss:    961.620\n",
            "[Epoch 41 / 50 | Batch: 54 / 301] loss:   1120.291\n",
            "[Epoch 41 / 50 | Batch: 55 / 301] loss:   1070.722\n",
            "[Epoch 41 / 50 | Batch: 56 / 301] loss:   1291.993\n",
            "[Epoch 41 / 50 | Batch: 57 / 301] loss:   1847.822\n",
            "[Epoch 41 / 50 | Batch: 58 / 301] loss:   1031.310\n",
            "[Epoch 41 / 50 | Batch: 59 / 301] loss:   1323.442\n",
            "[Epoch 41 / 50 | Batch: 60 / 301] loss:   1368.768\n",
            "[Epoch 41 / 50 | Batch: 61 / 301] loss:   1202.222\n",
            "[Epoch 41 / 50 | Batch: 62 / 301] loss:   1026.854\n",
            "[Epoch 41 / 50 | Batch: 63 / 301] loss:   1541.646\n",
            "[Epoch 41 / 50 | Batch: 64 / 301] loss:   1261.848\n",
            "[Epoch 41 / 50 | Batch: 65 / 301] loss:   1431.153\n",
            "[Epoch 41 / 50 | Batch: 66 / 301] loss:    975.240\n",
            "[Epoch 41 / 50 | Batch: 67 / 301] loss:   1244.595\n",
            "[Epoch 41 / 50 | Batch: 68 / 301] loss:   1096.996\n",
            "[Epoch 41 / 50 | Batch: 69 / 301] loss:    905.148\n",
            "[Epoch 41 / 50 | Batch: 70 / 301] loss:    830.925\n",
            "[Epoch 41 / 50 | Batch: 71 / 301] loss:   1239.830\n",
            "[Epoch 41 / 50 | Batch: 72 / 301] loss:    960.896\n",
            "[Epoch 41 / 50 | Batch: 73 / 301] loss:   1067.856\n",
            "[Epoch 41 / 50 | Batch: 74 / 301] loss:    847.803\n",
            "[Epoch 41 / 50 | Batch: 75 / 301] loss:   1143.635\n",
            "[Epoch 41 / 50 | Batch: 76 / 301] loss:   1483.251\n",
            "[Epoch 41 / 50 | Batch: 77 / 301] loss:   1157.188\n",
            "[Epoch 41 / 50 | Batch: 78 / 301] loss:   1270.591\n",
            "[Epoch 41 / 50 | Batch: 79 / 301] loss:    973.771\n",
            "[Epoch 41 / 50 | Batch: 80 / 301] loss:    961.207\n",
            "[Epoch 41 / 50 | Batch: 81 / 301] loss:    888.283\n",
            "[Epoch 41 / 50 | Batch: 82 / 301] loss:   1094.073\n",
            "[Epoch 41 / 50 | Batch: 83 / 301] loss:   1197.854\n",
            "[Epoch 41 / 50 | Batch: 84 / 301] loss:   1452.738\n",
            "[Epoch 41 / 50 | Batch: 85 / 301] loss:   1322.234\n",
            "[Epoch 41 / 50 | Batch: 86 / 301] loss:    945.491\n",
            "[Epoch 41 / 50 | Batch: 87 / 301] loss:    932.046\n",
            "[Epoch 41 / 50 | Batch: 88 / 301] loss:   1189.764\n",
            "[Epoch 41 / 50 | Batch: 89 / 301] loss:    996.312\n",
            "[Epoch 41 / 50 | Batch: 90 / 301] loss:   1219.120\n",
            "[Epoch 41 / 50 | Batch: 91 / 301] loss:   1771.203\n",
            "[Epoch 41 / 50 | Batch: 92 / 301] loss:   1361.687\n",
            "[Epoch 41 / 50 | Batch: 93 / 301] loss:    917.226\n",
            "[Epoch 41 / 50 | Batch: 94 / 301] loss:   1231.590\n",
            "[Epoch 41 / 50 | Batch: 95 / 301] loss:   1003.028\n",
            "[Epoch 41 / 50 | Batch: 96 / 301] loss:    965.577\n",
            "[Epoch 41 / 50 | Batch: 97 / 301] loss:   1060.318\n",
            "[Epoch 41 / 50 | Batch: 98 / 301] loss:   1110.609\n",
            "[Epoch 41 / 50 | Batch: 99 / 301] loss:   1228.280\n",
            "[Epoch 41 / 50 | Batch: 100 / 301] loss:   1044.453\n",
            "[Epoch 41 / 50 | Batch: 101 / 301] loss:   1293.385\n",
            "[Epoch 41 / 50 | Batch: 102 / 301] loss:    950.759\n",
            "[Epoch 41 / 50 | Batch: 103 / 301] loss:   1358.908\n",
            "[Epoch 41 / 50 | Batch: 104 / 301] loss:   1635.129\n",
            "[Epoch 41 / 50 | Batch: 105 / 301] loss:   1832.043\n",
            "[Epoch 41 / 50 | Batch: 106 / 301] loss:   1062.566\n",
            "[Epoch 41 / 50 | Batch: 107 / 301] loss:   1083.032\n",
            "[Epoch 41 / 50 | Batch: 108 / 301] loss:   1163.660\n",
            "[Epoch 41 / 50 | Batch: 109 / 301] loss:   1132.975\n",
            "[Epoch 41 / 50 | Batch: 110 / 301] loss:    955.834\n",
            "[Epoch 41 / 50 | Batch: 111 / 301] loss:   1173.688\n",
            "[Epoch 41 / 50 | Batch: 112 / 301] loss:    951.149\n",
            "[Epoch 41 / 50 | Batch: 113 / 301] loss:   1095.658\n",
            "[Epoch 41 / 50 | Batch: 114 / 301] loss:   1497.151\n",
            "[Epoch 41 / 50 | Batch: 115 / 301] loss:   1295.648\n",
            "[Epoch 41 / 50 | Batch: 116 / 301] loss:   1457.259\n",
            "[Epoch 41 / 50 | Batch: 117 / 301] loss:   1163.421\n",
            "[Epoch 41 / 50 | Batch: 118 / 301] loss:   1333.584\n",
            "[Epoch 41 / 50 | Batch: 119 / 301] loss:   1008.725\n",
            "[Epoch 41 / 50 | Batch: 120 / 301] loss:   1261.146\n",
            "[Epoch 41 / 50 | Batch: 121 / 301] loss:   1472.632\n",
            "[Epoch 41 / 50 | Batch: 122 / 301] loss:   1001.075\n",
            "[Epoch 41 / 50 | Batch: 123 / 301] loss:   1045.813\n",
            "[Epoch 41 / 50 | Batch: 124 / 301] loss:   1031.779\n",
            "[Epoch 41 / 50 | Batch: 125 / 301] loss:   1000.061\n",
            "[Epoch 41 / 50 | Batch: 126 / 301] loss:   1323.603\n",
            "[Epoch 41 / 50 | Batch: 127 / 301] loss:   1079.633\n",
            "[Epoch 41 / 50 | Batch: 128 / 301] loss:    923.208\n",
            "[Epoch 41 / 50 | Batch: 129 / 301] loss:    972.147\n",
            "[Epoch 41 / 50 | Batch: 130 / 301] loss:    889.438\n",
            "[Epoch 41 / 50 | Batch: 131 / 301] loss:   1033.899\n",
            "[Epoch 41 / 50 | Batch: 132 / 301] loss:   1289.572\n",
            "[Epoch 41 / 50 | Batch: 133 / 301] loss:    861.115\n",
            "[Epoch 41 / 50 | Batch: 134 / 301] loss:    932.528\n",
            "[Epoch 41 / 50 | Batch: 135 / 301] loss:   1119.934\n",
            "[Epoch 41 / 50 | Batch: 136 / 301] loss:   1028.993\n",
            "[Epoch 41 / 50 | Batch: 137 / 301] loss:   1192.612\n",
            "[Epoch 41 / 50 | Batch: 138 / 301] loss:   1200.259\n",
            "[Epoch 41 / 50 | Batch: 139 / 301] loss:   1172.589\n",
            "[Epoch 41 / 50 | Batch: 140 / 301] loss:   1026.939\n",
            "[Epoch 41 / 50 | Batch: 141 / 301] loss:    838.555\n",
            "[Epoch 41 / 50 | Batch: 142 / 301] loss:   1087.474\n",
            "[Epoch 41 / 50 | Batch: 143 / 301] loss:   1280.079\n",
            "[Epoch 41 / 50 | Batch: 144 / 301] loss:   1434.255\n",
            "[Epoch 41 / 50 | Batch: 145 / 301] loss:   1216.685\n",
            "[Epoch 41 / 50 | Batch: 146 / 301] loss:   1208.974\n",
            "[Epoch 41 / 50 | Batch: 147 / 301] loss:    826.748\n",
            "[Epoch 41 / 50 | Batch: 148 / 301] loss:   1021.376\n",
            "[Epoch 41 / 50 | Batch: 149 / 301] loss:   1260.521\n",
            "[Epoch 41 / 50 | Batch: 150 / 301] loss:   1315.164\n",
            "[Epoch 41 / 50 | Batch: 151 / 301] loss:    958.499\n",
            "[Epoch 41 / 50 | Batch: 152 / 301] loss:   1022.814\n",
            "[Epoch 41 / 50 | Batch: 153 / 301] loss:   1949.795\n",
            "[Epoch 41 / 50 | Batch: 154 / 301] loss:   1195.775\n",
            "[Epoch 41 / 50 | Batch: 155 / 301] loss:   1157.580\n",
            "[Epoch 41 / 50 | Batch: 156 / 301] loss:   1069.681\n",
            "[Epoch 41 / 50 | Batch: 157 / 301] loss:   1149.648\n",
            "[Epoch 41 / 50 | Batch: 158 / 301] loss:    960.126\n",
            "[Epoch 41 / 50 | Batch: 159 / 301] loss:    958.143\n",
            "[Epoch 41 / 50 | Batch: 160 / 301] loss:   1389.201\n",
            "[Epoch 41 / 50 | Batch: 161 / 301] loss:   1018.000\n",
            "[Epoch 41 / 50 | Batch: 162 / 301] loss:   1071.764\n",
            "[Epoch 41 / 50 | Batch: 163 / 301] loss:    991.004\n",
            "[Epoch 41 / 50 | Batch: 164 / 301] loss:   1159.361\n",
            "[Epoch 41 / 50 | Batch: 165 / 301] loss:   1479.155\n",
            "[Epoch 41 / 50 | Batch: 166 / 301] loss:   1405.097\n",
            "[Epoch 41 / 50 | Batch: 167 / 301] loss:   1702.608\n",
            "[Epoch 41 / 50 | Batch: 168 / 301] loss:   1464.083\n",
            "[Epoch 41 / 50 | Batch: 169 / 301] loss:   1145.826\n",
            "[Epoch 41 / 50 | Batch: 170 / 301] loss:   1158.971\n",
            "[Epoch 41 / 50 | Batch: 171 / 301] loss:    988.446\n",
            "[Epoch 41 / 50 | Batch: 172 / 301] loss:   1008.456\n",
            "[Epoch 41 / 50 | Batch: 173 / 301] loss:    787.377\n",
            "[Epoch 41 / 50 | Batch: 174 / 301] loss:    876.892\n",
            "[Epoch 41 / 50 | Batch: 175 / 301] loss:   1120.276\n",
            "[Epoch 41 / 50 | Batch: 176 / 301] loss:   1190.252\n",
            "[Epoch 41 / 50 | Batch: 177 / 301] loss:   1049.257\n",
            "[Epoch 41 / 50 | Batch: 178 / 301] loss:   1216.815\n",
            "[Epoch 41 / 50 | Batch: 179 / 301] loss:   1021.905\n",
            "[Epoch 41 / 50 | Batch: 180 / 301] loss:   1579.392\n",
            "[Epoch 41 / 50 | Batch: 181 / 301] loss:   1296.531\n",
            "[Epoch 41 / 50 | Batch: 182 / 301] loss:   1260.801\n",
            "[Epoch 41 / 50 | Batch: 183 / 301] loss:   1448.455\n",
            "[Epoch 41 / 50 | Batch: 184 / 301] loss:   1190.386\n",
            "[Epoch 41 / 50 | Batch: 185 / 301] loss:   1086.098\n",
            "[Epoch 41 / 50 | Batch: 186 / 301] loss:   1303.344\n",
            "[Epoch 41 / 50 | Batch: 187 / 301] loss:    980.086\n",
            "[Epoch 41 / 50 | Batch: 188 / 301] loss:   1477.672\n",
            "[Epoch 41 / 50 | Batch: 189 / 301] loss:   1205.182\n",
            "[Epoch 41 / 50 | Batch: 190 / 301] loss:   1774.204\n",
            "[Epoch 41 / 50 | Batch: 191 / 301] loss:   1152.929\n",
            "[Epoch 41 / 50 | Batch: 192 / 301] loss:    998.799\n",
            "[Epoch 41 / 50 | Batch: 193 / 301] loss:    705.371\n",
            "[Epoch 41 / 50 | Batch: 194 / 301] loss:   1230.925\n",
            "[Epoch 41 / 50 | Batch: 195 / 301] loss:    826.845\n",
            "[Epoch 41 / 50 | Batch: 196 / 301] loss:    991.877\n",
            "[Epoch 41 / 50 | Batch: 197 / 301] loss:   1142.853\n",
            "[Epoch 41 / 50 | Batch: 198 / 301] loss:   1124.913\n",
            "[Epoch 41 / 50 | Batch: 199 / 301] loss:   1236.338\n",
            "[Epoch 41 / 50 | Batch: 200 / 301] loss:   1073.752\n",
            "[Epoch 41 / 50 | Batch: 201 / 301] loss:   1532.446\n",
            "[Epoch 41 / 50 | Batch: 202 / 301] loss:   1130.477\n",
            "[Epoch 41 / 50 | Batch: 203 / 301] loss:   1450.804\n",
            "[Epoch 41 / 50 | Batch: 204 / 301] loss:   1518.977\n",
            "[Epoch 41 / 50 | Batch: 205 / 301] loss:   1704.172\n",
            "[Epoch 41 / 50 | Batch: 206 / 301] loss:    869.114\n",
            "[Epoch 41 / 50 | Batch: 207 / 301] loss:   1203.079\n",
            "[Epoch 41 / 50 | Batch: 208 / 301] loss:   1055.214\n",
            "[Epoch 41 / 50 | Batch: 209 / 301] loss:    830.523\n",
            "[Epoch 41 / 50 | Batch: 210 / 301] loss:   1230.816\n",
            "[Epoch 41 / 50 | Batch: 211 / 301] loss:   1125.086\n",
            "[Epoch 41 / 50 | Batch: 212 / 301] loss:   1477.174\n",
            "[Epoch 41 / 50 | Batch: 213 / 301] loss:   1231.719\n",
            "[Epoch 41 / 50 | Batch: 214 / 301] loss:   1710.585\n",
            "[Epoch 41 / 50 | Batch: 215 / 301] loss:    789.628\n",
            "[Epoch 41 / 50 | Batch: 216 / 301] loss:    955.249\n",
            "[Epoch 41 / 50 | Batch: 217 / 301] loss:   1346.924\n",
            "[Epoch 41 / 50 | Batch: 218 / 301] loss:   1257.094\n",
            "[Epoch 41 / 50 | Batch: 219 / 301] loss:    839.509\n",
            "[Epoch 41 / 50 | Batch: 220 / 301] loss:    993.211\n",
            "[Epoch 41 / 50 | Batch: 221 / 301] loss:   1060.772\n",
            "[Epoch 41 / 50 | Batch: 222 / 301] loss:   1197.523\n",
            "[Epoch 41 / 50 | Batch: 223 / 301] loss:   1344.228\n",
            "[Epoch 41 / 50 | Batch: 224 / 301] loss:   1413.918\n",
            "[Epoch 41 / 50 | Batch: 225 / 301] loss:   1049.183\n",
            "[Epoch 41 / 50 | Batch: 226 / 301] loss:   1407.119\n",
            "[Epoch 41 / 50 | Batch: 227 / 301] loss:   1151.707\n",
            "[Epoch 41 / 50 | Batch: 228 / 301] loss:    915.116\n",
            "[Epoch 41 / 50 | Batch: 229 / 301] loss:   1963.010\n",
            "[Epoch 41 / 50 | Batch: 230 / 301] loss:   1382.481\n",
            "[Epoch 41 / 50 | Batch: 231 / 301] loss:   1071.634\n",
            "[Epoch 41 / 50 | Batch: 232 / 301] loss:    863.233\n",
            "[Epoch 41 / 50 | Batch: 233 / 301] loss:   1179.410\n",
            "[Epoch 41 / 50 | Batch: 234 / 301] loss:   1072.231\n",
            "[Epoch 41 / 50 | Batch: 235 / 301] loss:   1039.067\n",
            "[Epoch 41 / 50 | Batch: 236 / 301] loss:    915.346\n",
            "[Epoch 41 / 50 | Batch: 237 / 301] loss:   1052.402\n",
            "[Epoch 41 / 50 | Batch: 238 / 301] loss:   1406.606\n",
            "[Epoch 41 / 50 | Batch: 239 / 301] loss:    800.078\n",
            "[Epoch 41 / 50 | Batch: 240 / 301] loss:   1094.821\n",
            "[Epoch 41 / 50 | Batch: 241 / 301] loss:   1122.015\n",
            "[Epoch 41 / 50 | Batch: 242 / 301] loss:   1097.761\n",
            "[Epoch 41 / 50 | Batch: 243 / 301] loss:   1045.827\n",
            "[Epoch 41 / 50 | Batch: 244 / 301] loss:    895.293\n",
            "[Epoch 41 / 50 | Batch: 245 / 301] loss:   1092.950\n",
            "[Epoch 41 / 50 | Batch: 246 / 301] loss:   1118.016\n",
            "[Epoch 41 / 50 | Batch: 247 / 301] loss:   1472.396\n",
            "[Epoch 41 / 50 | Batch: 248 / 301] loss:   1342.278\n",
            "[Epoch 41 / 50 | Batch: 249 / 301] loss:   1129.532\n",
            "[Epoch 41 / 50 | Batch: 250 / 301] loss:   1092.425\n",
            "[Epoch 41 / 50 | Batch: 251 / 301] loss:   1514.654\n",
            "[Epoch 41 / 50 | Batch: 252 / 301] loss:   1063.100\n",
            "[Epoch 41 / 50 | Batch: 253 / 301] loss:   1204.312\n",
            "[Epoch 41 / 50 | Batch: 254 / 301] loss:    882.822\n",
            "[Epoch 41 / 50 | Batch: 255 / 301] loss:   1177.078\n",
            "[Epoch 41 / 50 | Batch: 256 / 301] loss:    962.920\n",
            "[Epoch 41 / 50 | Batch: 257 / 301] loss:    903.259\n",
            "[Epoch 41 / 50 | Batch: 258 / 301] loss:   1081.343\n",
            "[Epoch 41 / 50 | Batch: 259 / 301] loss:   1103.220\n",
            "[Epoch 41 / 50 | Batch: 260 / 301] loss:   1106.544\n",
            "[Epoch 41 / 50 | Batch: 261 / 301] loss:   1444.328\n",
            "[Epoch 41 / 50 | Batch: 262 / 301] loss:   1239.855\n",
            "[Epoch 41 / 50 | Batch: 263 / 301] loss:   1243.732\n",
            "[Epoch 41 / 50 | Batch: 264 / 301] loss:   1282.280\n",
            "[Epoch 41 / 50 | Batch: 265 / 301] loss:   1486.189\n",
            "[Epoch 41 / 50 | Batch: 266 / 301] loss:   1406.215\n",
            "[Epoch 41 / 50 | Batch: 267 / 301] loss:   1137.162\n",
            "[Epoch 41 / 50 | Batch: 268 / 301] loss:   1130.630\n",
            "[Epoch 41 / 50 | Batch: 269 / 301] loss:   1350.123\n",
            "[Epoch 41 / 50 | Batch: 270 / 301] loss:    909.722\n",
            "[Epoch 41 / 50 | Batch: 271 / 301] loss:   1089.409\n",
            "[Epoch 41 / 50 | Batch: 272 / 301] loss:   1338.827\n",
            "[Epoch 41 / 50 | Batch: 273 / 301] loss:   1121.725\n",
            "[Epoch 41 / 50 | Batch: 274 / 301] loss:   1449.863\n",
            "[Epoch 41 / 50 | Batch: 275 / 301] loss:   1043.321\n",
            "[Epoch 41 / 50 | Batch: 276 / 301] loss:   1516.097\n",
            "[Epoch 41 / 50 | Batch: 277 / 301] loss:   1228.460\n",
            "[Epoch 41 / 50 | Batch: 278 / 301] loss:   1219.762\n",
            "[Epoch 41 / 50 | Batch: 279 / 301] loss:   1196.364\n",
            "[Epoch 41 / 50 | Batch: 280 / 301] loss:   1348.899\n",
            "[Epoch 41 / 50 | Batch: 281 / 301] loss:   1573.002\n",
            "[Epoch 41 / 50 | Batch: 282 / 301] loss:    975.362\n",
            "[Epoch 41 / 50 | Batch: 283 / 301] loss:   1388.375\n",
            "[Epoch 41 / 50 | Batch: 284 / 301] loss:   1127.463\n",
            "[Epoch 41 / 50 | Batch: 285 / 301] loss:   1246.698\n",
            "[Epoch 41 / 50 | Batch: 286 / 301] loss:   1056.739\n",
            "[Epoch 41 / 50 | Batch: 287 / 301] loss:   1298.488\n",
            "[Epoch 41 / 50 | Batch: 288 / 301] loss:   1003.678\n",
            "[Epoch 41 / 50 | Batch: 289 / 301] loss:   1120.297\n",
            "[Epoch 41 / 50 | Batch: 290 / 301] loss:   1030.473\n",
            "[Epoch 41 / 50 | Batch: 291 / 301] loss:   1030.880\n",
            "[Epoch 41 / 50 | Batch: 292 / 301] loss:   1249.046\n",
            "[Epoch 41 / 50 | Batch: 293 / 301] loss:   1091.143\n",
            "[Epoch 41 / 50 | Batch: 294 / 301] loss:   1055.988\n",
            "[Epoch 41 / 50 | Batch: 295 / 301] loss:   1159.975\n",
            "[Epoch 41 / 50 | Batch: 296 / 301] loss:   1173.027\n",
            "[Epoch 41 / 50 | Batch: 297 / 301] loss:   1162.120\n",
            "[Epoch 41 / 50 | Batch: 298 / 301] loss:   1011.100\n",
            "[Epoch 41 / 50 | Batch: 299 / 301] loss:   1252.366\n",
            "[Epoch 41 / 50 | Batch: 300 / 301] loss:   1234.837\n",
            "[Epoch 41 / 50 | Batch: 301 / 301] loss:    716.618\n",
            "Epoch loss: 1164.53548\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4311.721\n",
            "[Validation] [Batch  2 / 15] dev loss:   6774.233\n",
            "[Validation] [Batch  3 / 15] dev loss:   6601.628\n",
            "[Validation] [Batch  4 / 15] dev loss:   5510.683\n",
            "[Validation] [Batch  5 / 15] dev loss:   5075.387\n",
            "[Validation] [Batch  6 / 15] dev loss:   9244.984\n",
            "[Validation] [Batch  7 / 15] dev loss:   6381.416\n",
            "[Validation] [Batch  8 / 15] dev loss:   4110.975\n",
            "[Validation] [Batch  9 / 15] dev loss:   3885.826\n",
            "[Validation] [Batch 10 / 15] dev loss:  15453.726\n",
            "[Validation] [Batch 11 / 15] dev loss:  12903.279\n",
            "[Validation] [Batch 12 / 15] dev loss:  10946.802\n",
            "[Validation] [Batch 13 / 15] dev loss:   4557.875\n",
            "[Validation] [Batch 14 / 15] dev loss:  21263.816\n",
            "[Validation] [Batch 15 / 15] dev loss:   1802.266\n",
            "Dev loss 7921.64113\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 42 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 42 / 50 | Batch:  1 / 301] loss:   1107.425\n",
            "[Epoch 42 / 50 | Batch:  2 / 301] loss:    998.339\n",
            "[Epoch 42 / 50 | Batch:  3 / 301] loss:    968.472\n",
            "[Epoch 42 / 50 | Batch:  4 / 301] loss:   1106.209\n",
            "[Epoch 42 / 50 | Batch:  5 / 301] loss:   1283.017\n",
            "[Epoch 42 / 50 | Batch:  6 / 301] loss:   1249.451\n",
            "[Epoch 42 / 50 | Batch:  7 / 301] loss:    921.407\n",
            "[Epoch 42 / 50 | Batch:  8 / 301] loss:   1211.558\n",
            "[Epoch 42 / 50 | Batch:  9 / 301] loss:   1039.118\n",
            "[Epoch 42 / 50 | Batch: 10 / 301] loss:   1041.535\n",
            "[Epoch 42 / 50 | Batch: 11 / 301] loss:    859.047\n",
            "[Epoch 42 / 50 | Batch: 12 / 301] loss:    945.575\n",
            "[Epoch 42 / 50 | Batch: 13 / 301] loss:   1221.822\n",
            "[Epoch 42 / 50 | Batch: 14 / 301] loss:   1097.668\n",
            "[Epoch 42 / 50 | Batch: 15 / 301] loss:   1287.035\n",
            "[Epoch 42 / 50 | Batch: 16 / 301] loss:    853.548\n",
            "[Epoch 42 / 50 | Batch: 17 / 301] loss:   1334.196\n",
            "[Epoch 42 / 50 | Batch: 18 / 301] loss:    816.738\n",
            "[Epoch 42 / 50 | Batch: 19 / 301] loss:   1159.844\n",
            "[Epoch 42 / 50 | Batch: 20 / 301] loss:   1064.015\n",
            "[Epoch 42 / 50 | Batch: 21 / 301] loss:   1119.840\n",
            "[Epoch 42 / 50 | Batch: 22 / 301] loss:   1000.999\n",
            "[Epoch 42 / 50 | Batch: 23 / 301] loss:   1230.102\n",
            "[Epoch 42 / 50 | Batch: 24 / 301] loss:   1207.818\n",
            "[Epoch 42 / 50 | Batch: 25 / 301] loss:   1081.408\n",
            "[Epoch 42 / 50 | Batch: 26 / 301] loss:    941.001\n",
            "[Epoch 42 / 50 | Batch: 27 / 301] loss:   1167.315\n",
            "[Epoch 42 / 50 | Batch: 28 / 301] loss:   1288.573\n",
            "[Epoch 42 / 50 | Batch: 29 / 301] loss:   1375.006\n",
            "[Epoch 42 / 50 | Batch: 30 / 301] loss:   1016.170\n",
            "[Epoch 42 / 50 | Batch: 31 / 301] loss:   1563.233\n",
            "[Epoch 42 / 50 | Batch: 32 / 301] loss:   1201.671\n",
            "[Epoch 42 / 50 | Batch: 33 / 301] loss:    919.553\n",
            "[Epoch 42 / 50 | Batch: 34 / 301] loss:   1322.763\n",
            "[Epoch 42 / 50 | Batch: 35 / 301] loss:   1102.723\n",
            "[Epoch 42 / 50 | Batch: 36 / 301] loss:   1446.053\n",
            "[Epoch 42 / 50 | Batch: 37 / 301] loss:   1171.931\n",
            "[Epoch 42 / 50 | Batch: 38 / 301] loss:   1155.085\n",
            "[Epoch 42 / 50 | Batch: 39 / 301] loss:   1066.644\n",
            "[Epoch 42 / 50 | Batch: 40 / 301] loss:   1279.550\n",
            "[Epoch 42 / 50 | Batch: 41 / 301] loss:   1158.917\n",
            "[Epoch 42 / 50 | Batch: 42 / 301] loss:   1272.602\n",
            "[Epoch 42 / 50 | Batch: 43 / 301] loss:   1316.499\n",
            "[Epoch 42 / 50 | Batch: 44 / 301] loss:   1137.568\n",
            "[Epoch 42 / 50 | Batch: 45 / 301] loss:    785.557\n",
            "[Epoch 42 / 50 | Batch: 46 / 301] loss:   1108.124\n",
            "[Epoch 42 / 50 | Batch: 47 / 301] loss:    934.871\n",
            "[Epoch 42 / 50 | Batch: 48 / 301] loss:    893.425\n",
            "[Epoch 42 / 50 | Batch: 49 / 301] loss:   1258.308\n",
            "[Epoch 42 / 50 | Batch: 50 / 301] loss:   1227.771\n",
            "[Epoch 42 / 50 | Batch: 51 / 301] loss:   1364.595\n",
            "[Epoch 42 / 50 | Batch: 52 / 301] loss:    893.907\n",
            "[Epoch 42 / 50 | Batch: 53 / 301] loss:    990.716\n",
            "[Epoch 42 / 50 | Batch: 54 / 301] loss:   1370.100\n",
            "[Epoch 42 / 50 | Batch: 55 / 301] loss:   1041.369\n",
            "[Epoch 42 / 50 | Batch: 56 / 301] loss:   1506.362\n",
            "[Epoch 42 / 50 | Batch: 57 / 301] loss:   1455.421\n",
            "[Epoch 42 / 50 | Batch: 58 / 301] loss:   1494.350\n",
            "[Epoch 42 / 50 | Batch: 59 / 301] loss:   1422.719\n",
            "[Epoch 42 / 50 | Batch: 60 / 301] loss:   1167.257\n",
            "[Epoch 42 / 50 | Batch: 61 / 301] loss:   1272.422\n",
            "[Epoch 42 / 50 | Batch: 62 / 301] loss:   1171.264\n",
            "[Epoch 42 / 50 | Batch: 63 / 301] loss:   1279.414\n",
            "[Epoch 42 / 50 | Batch: 64 / 301] loss:   1185.658\n",
            "[Epoch 42 / 50 | Batch: 65 / 301] loss:   1722.640\n",
            "[Epoch 42 / 50 | Batch: 66 / 301] loss:    979.021\n",
            "[Epoch 42 / 50 | Batch: 67 / 301] loss:   1196.214\n",
            "[Epoch 42 / 50 | Batch: 68 / 301] loss:    948.480\n",
            "[Epoch 42 / 50 | Batch: 69 / 301] loss:   1136.602\n",
            "[Epoch 42 / 50 | Batch: 70 / 301] loss:   1034.576\n",
            "[Epoch 42 / 50 | Batch: 71 / 301] loss:   1116.080\n",
            "[Epoch 42 / 50 | Batch: 72 / 301] loss:   1218.177\n",
            "[Epoch 42 / 50 | Batch: 73 / 301] loss:   1438.992\n",
            "[Epoch 42 / 50 | Batch: 74 / 301] loss:   1198.404\n",
            "[Epoch 42 / 50 | Batch: 75 / 301] loss:    923.856\n",
            "[Epoch 42 / 50 | Batch: 76 / 301] loss:   1055.309\n",
            "[Epoch 42 / 50 | Batch: 77 / 301] loss:   1295.696\n",
            "[Epoch 42 / 50 | Batch: 78 / 301] loss:   1345.501\n",
            "[Epoch 42 / 50 | Batch: 79 / 301] loss:   1454.433\n",
            "[Epoch 42 / 50 | Batch: 80 / 301] loss:   1520.065\n",
            "[Epoch 42 / 50 | Batch: 81 / 301] loss:    901.369\n",
            "[Epoch 42 / 50 | Batch: 82 / 301] loss:    987.607\n",
            "[Epoch 42 / 50 | Batch: 83 / 301] loss:   1283.378\n",
            "[Epoch 42 / 50 | Batch: 84 / 301] loss:   1096.823\n",
            "[Epoch 42 / 50 | Batch: 85 / 301] loss:   1084.881\n",
            "[Epoch 42 / 50 | Batch: 86 / 301] loss:   1036.759\n",
            "[Epoch 42 / 50 | Batch: 87 / 301] loss:    990.741\n",
            "[Epoch 42 / 50 | Batch: 88 / 301] loss:   1040.614\n",
            "[Epoch 42 / 50 | Batch: 89 / 301] loss:   1149.975\n",
            "[Epoch 42 / 50 | Batch: 90 / 301] loss:   1202.234\n",
            "[Epoch 42 / 50 | Batch: 91 / 301] loss:   1243.674\n",
            "[Epoch 42 / 50 | Batch: 92 / 301] loss:    981.805\n",
            "[Epoch 42 / 50 | Batch: 93 / 301] loss:   1180.344\n",
            "[Epoch 42 / 50 | Batch: 94 / 301] loss:   1257.521\n",
            "[Epoch 42 / 50 | Batch: 95 / 301] loss:   1114.770\n",
            "[Epoch 42 / 50 | Batch: 96 / 301] loss:   1284.099\n",
            "[Epoch 42 / 50 | Batch: 97 / 301] loss:   1431.734\n",
            "[Epoch 42 / 50 | Batch: 98 / 301] loss:   1143.811\n",
            "[Epoch 42 / 50 | Batch: 99 / 301] loss:   1515.939\n",
            "[Epoch 42 / 50 | Batch: 100 / 301] loss:   1140.342\n",
            "[Epoch 42 / 50 | Batch: 101 / 301] loss:    861.849\n",
            "[Epoch 42 / 50 | Batch: 102 / 301] loss:   1211.553\n",
            "[Epoch 42 / 50 | Batch: 103 / 301] loss:   1326.823\n",
            "[Epoch 42 / 50 | Batch: 104 / 301] loss:   1198.192\n",
            "[Epoch 42 / 50 | Batch: 105 / 301] loss:   1115.343\n",
            "[Epoch 42 / 50 | Batch: 106 / 301] loss:    832.489\n",
            "[Epoch 42 / 50 | Batch: 107 / 301] loss:   1182.803\n",
            "[Epoch 42 / 50 | Batch: 108 / 301] loss:   1024.436\n",
            "[Epoch 42 / 50 | Batch: 109 / 301] loss:   1287.416\n",
            "[Epoch 42 / 50 | Batch: 110 / 301] loss:   1329.531\n",
            "[Epoch 42 / 50 | Batch: 111 / 301] loss:    950.414\n",
            "[Epoch 42 / 50 | Batch: 112 / 301] loss:   1041.408\n",
            "[Epoch 42 / 50 | Batch: 113 / 301] loss:   1049.053\n",
            "[Epoch 42 / 50 | Batch: 114 / 301] loss:    876.622\n",
            "[Epoch 42 / 50 | Batch: 115 / 301] loss:    947.772\n",
            "[Epoch 42 / 50 | Batch: 116 / 301] loss:   1283.820\n",
            "[Epoch 42 / 50 | Batch: 117 / 301] loss:   1274.543\n",
            "[Epoch 42 / 50 | Batch: 118 / 301] loss:   1289.050\n",
            "[Epoch 42 / 50 | Batch: 119 / 301] loss:   1203.382\n",
            "[Epoch 42 / 50 | Batch: 120 / 301] loss:   1210.622\n",
            "[Epoch 42 / 50 | Batch: 121 / 301] loss:   1152.000\n",
            "[Epoch 42 / 50 | Batch: 122 / 301] loss:   1040.312\n",
            "[Epoch 42 / 50 | Batch: 123 / 301] loss:    890.079\n",
            "[Epoch 42 / 50 | Batch: 124 / 301] loss:   1300.539\n",
            "[Epoch 42 / 50 | Batch: 125 / 301] loss:   1220.325\n",
            "[Epoch 42 / 50 | Batch: 126 / 301] loss:   1371.550\n",
            "[Epoch 42 / 50 | Batch: 127 / 301] loss:   1238.145\n",
            "[Epoch 42 / 50 | Batch: 128 / 301] loss:   1286.151\n",
            "[Epoch 42 / 50 | Batch: 129 / 301] loss:    967.910\n",
            "[Epoch 42 / 50 | Batch: 130 / 301] loss:   1159.819\n",
            "[Epoch 42 / 50 | Batch: 131 / 301] loss:    945.284\n",
            "[Epoch 42 / 50 | Batch: 132 / 301] loss:   1495.682\n",
            "[Epoch 42 / 50 | Batch: 133 / 301] loss:   1217.320\n",
            "[Epoch 42 / 50 | Batch: 134 / 301] loss:   1153.625\n",
            "[Epoch 42 / 50 | Batch: 135 / 301] loss:   1000.785\n",
            "[Epoch 42 / 50 | Batch: 136 / 301] loss:   1126.109\n",
            "[Epoch 42 / 50 | Batch: 137 / 301] loss:   1017.529\n",
            "[Epoch 42 / 50 | Batch: 138 / 301] loss:   1220.599\n",
            "[Epoch 42 / 50 | Batch: 139 / 301] loss:    999.109\n",
            "[Epoch 42 / 50 | Batch: 140 / 301] loss:   1357.987\n",
            "[Epoch 42 / 50 | Batch: 141 / 301] loss:   1128.105\n",
            "[Epoch 42 / 50 | Batch: 142 / 301] loss:   1153.628\n",
            "[Epoch 42 / 50 | Batch: 143 / 301] loss:   1071.317\n",
            "[Epoch 42 / 50 | Batch: 144 / 301] loss:   1055.943\n",
            "[Epoch 42 / 50 | Batch: 145 / 301] loss:    996.372\n",
            "[Epoch 42 / 50 | Batch: 146 / 301] loss:   1556.624\n",
            "[Epoch 42 / 50 | Batch: 147 / 301] loss:   1182.005\n",
            "[Epoch 42 / 50 | Batch: 148 / 301] loss:   1231.626\n",
            "[Epoch 42 / 50 | Batch: 149 / 301] loss:    831.469\n",
            "[Epoch 42 / 50 | Batch: 150 / 301] loss:   1169.369\n",
            "[Epoch 42 / 50 | Batch: 151 / 301] loss:   1154.608\n",
            "[Epoch 42 / 50 | Batch: 152 / 301] loss:   1474.948\n",
            "[Epoch 42 / 50 | Batch: 153 / 301] loss:   1420.386\n",
            "[Epoch 42 / 50 | Batch: 154 / 301] loss:   1318.656\n",
            "[Epoch 42 / 50 | Batch: 155 / 301] loss:    952.169\n",
            "[Epoch 42 / 50 | Batch: 156 / 301] loss:   1224.675\n",
            "[Epoch 42 / 50 | Batch: 157 / 301] loss:    937.815\n",
            "[Epoch 42 / 50 | Batch: 158 / 301] loss:   1038.404\n",
            "[Epoch 42 / 50 | Batch: 159 / 301] loss:   1011.830\n",
            "[Epoch 42 / 50 | Batch: 160 / 301] loss:   1140.800\n",
            "[Epoch 42 / 50 | Batch: 161 / 301] loss:   1035.515\n",
            "[Epoch 42 / 50 | Batch: 162 / 301] loss:   1057.362\n",
            "[Epoch 42 / 50 | Batch: 163 / 301] loss:   1211.330\n",
            "[Epoch 42 / 50 | Batch: 164 / 301] loss:   1093.955\n",
            "[Epoch 42 / 50 | Batch: 165 / 301] loss:   1384.925\n",
            "[Epoch 42 / 50 | Batch: 166 / 301] loss:   1595.749\n",
            "[Epoch 42 / 50 | Batch: 167 / 301] loss:    954.348\n",
            "[Epoch 42 / 50 | Batch: 168 / 301] loss:   1151.345\n",
            "[Epoch 42 / 50 | Batch: 169 / 301] loss:    952.416\n",
            "[Epoch 42 / 50 | Batch: 170 / 301] loss:   1371.099\n",
            "[Epoch 42 / 50 | Batch: 171 / 301] loss:   1190.711\n",
            "[Epoch 42 / 50 | Batch: 172 / 301] loss:   1768.783\n",
            "[Epoch 42 / 50 | Batch: 173 / 301] loss:    848.072\n",
            "[Epoch 42 / 50 | Batch: 174 / 301] loss:   1226.375\n",
            "[Epoch 42 / 50 | Batch: 175 / 301] loss:    928.352\n",
            "[Epoch 42 / 50 | Batch: 176 / 301] loss:    997.219\n",
            "[Epoch 42 / 50 | Batch: 177 / 301] loss:   1222.039\n",
            "[Epoch 42 / 50 | Batch: 178 / 301] loss:   1158.661\n",
            "[Epoch 42 / 50 | Batch: 179 / 301] loss:   1326.696\n",
            "[Epoch 42 / 50 | Batch: 180 / 301] loss:   1218.316\n",
            "[Epoch 42 / 50 | Batch: 181 / 301] loss:    918.966\n",
            "[Epoch 42 / 50 | Batch: 182 / 301] loss:   1088.676\n",
            "[Epoch 42 / 50 | Batch: 183 / 301] loss:    866.052\n",
            "[Epoch 42 / 50 | Batch: 184 / 301] loss:   1413.355\n",
            "[Epoch 42 / 50 | Batch: 185 / 301] loss:   1128.698\n",
            "[Epoch 42 / 50 | Batch: 186 / 301] loss:   1096.030\n",
            "[Epoch 42 / 50 | Batch: 187 / 301] loss:    833.427\n",
            "[Epoch 42 / 50 | Batch: 188 / 301] loss:   1216.615\n",
            "[Epoch 42 / 50 | Batch: 189 / 301] loss:   1171.405\n",
            "[Epoch 42 / 50 | Batch: 190 / 301] loss:   1067.163\n",
            "[Epoch 42 / 50 | Batch: 191 / 301] loss:   1054.991\n",
            "[Epoch 42 / 50 | Batch: 192 / 301] loss:    821.559\n",
            "[Epoch 42 / 50 | Batch: 193 / 301] loss:   1234.448\n",
            "[Epoch 42 / 50 | Batch: 194 / 301] loss:   1256.268\n",
            "[Epoch 42 / 50 | Batch: 195 / 301] loss:   1060.965\n",
            "[Epoch 42 / 50 | Batch: 196 / 301] loss:   1316.820\n",
            "[Epoch 42 / 50 | Batch: 197 / 301] loss:   1010.167\n",
            "[Epoch 42 / 50 | Batch: 198 / 301] loss:   1759.540\n",
            "[Epoch 42 / 50 | Batch: 199 / 301] loss:    854.562\n",
            "[Epoch 42 / 50 | Batch: 200 / 301] loss:    944.786\n",
            "[Epoch 42 / 50 | Batch: 201 / 301] loss:    976.393\n",
            "[Epoch 42 / 50 | Batch: 202 / 301] loss:   1273.537\n",
            "[Epoch 42 / 50 | Batch: 203 / 301] loss:   1230.141\n",
            "[Epoch 42 / 50 | Batch: 204 / 301] loss:   1304.369\n",
            "[Epoch 42 / 50 | Batch: 205 / 301] loss:   1098.896\n",
            "[Epoch 42 / 50 | Batch: 206 / 301] loss:    953.544\n",
            "[Epoch 42 / 50 | Batch: 207 / 301] loss:    973.286\n",
            "[Epoch 42 / 50 | Batch: 208 / 301] loss:   1120.032\n",
            "[Epoch 42 / 50 | Batch: 209 / 301] loss:   1069.312\n",
            "[Epoch 42 / 50 | Batch: 210 / 301] loss:    993.083\n",
            "[Epoch 42 / 50 | Batch: 211 / 301] loss:   1052.977\n",
            "[Epoch 42 / 50 | Batch: 212 / 301] loss:   1101.090\n",
            "[Epoch 42 / 50 | Batch: 213 / 301] loss:    945.883\n",
            "[Epoch 42 / 50 | Batch: 214 / 301] loss:   1149.971\n",
            "[Epoch 42 / 50 | Batch: 215 / 301] loss:   1246.478\n",
            "[Epoch 42 / 50 | Batch: 216 / 301] loss:   1119.806\n",
            "[Epoch 42 / 50 | Batch: 217 / 301] loss:    919.653\n",
            "[Epoch 42 / 50 | Batch: 218 / 301] loss:   1236.173\n",
            "[Epoch 42 / 50 | Batch: 219 / 301] loss:   1063.490\n",
            "[Epoch 42 / 50 | Batch: 220 / 301] loss:   1142.693\n",
            "[Epoch 42 / 50 | Batch: 221 / 301] loss:   1038.994\n",
            "[Epoch 42 / 50 | Batch: 222 / 301] loss:   1386.458\n",
            "[Epoch 42 / 50 | Batch: 223 / 301] loss:   1686.598\n",
            "[Epoch 42 / 50 | Batch: 224 / 301] loss:   1199.812\n",
            "[Epoch 42 / 50 | Batch: 225 / 301] loss:   1182.781\n",
            "[Epoch 42 / 50 | Batch: 226 / 301] loss:   1194.893\n",
            "[Epoch 42 / 50 | Batch: 227 / 301] loss:    989.517\n",
            "[Epoch 42 / 50 | Batch: 228 / 301] loss:    862.344\n",
            "[Epoch 42 / 50 | Batch: 229 / 301] loss:   1219.200\n",
            "[Epoch 42 / 50 | Batch: 230 / 301] loss:    944.842\n",
            "[Epoch 42 / 50 | Batch: 231 / 301] loss:   1204.526\n",
            "[Epoch 42 / 50 | Batch: 232 / 301] loss:   1167.718\n",
            "[Epoch 42 / 50 | Batch: 233 / 301] loss:   1837.130\n",
            "[Epoch 42 / 50 | Batch: 234 / 301] loss:   1230.574\n",
            "[Epoch 42 / 50 | Batch: 235 / 301] loss:   1048.672\n",
            "[Epoch 42 / 50 | Batch: 236 / 301] loss:   1285.781\n",
            "[Epoch 42 / 50 | Batch: 237 / 301] loss:    855.457\n",
            "[Epoch 42 / 50 | Batch: 238 / 301] loss:   1279.665\n",
            "[Epoch 42 / 50 | Batch: 239 / 301] loss:   1255.501\n",
            "[Epoch 42 / 50 | Batch: 240 / 301] loss:   1294.593\n",
            "[Epoch 42 / 50 | Batch: 241 / 301] loss:   1037.490\n",
            "[Epoch 42 / 50 | Batch: 242 / 301] loss:   1141.833\n",
            "[Epoch 42 / 50 | Batch: 243 / 301] loss:   1323.182\n",
            "[Epoch 42 / 50 | Batch: 244 / 301] loss:   1206.177\n",
            "[Epoch 42 / 50 | Batch: 245 / 301] loss:    707.525\n",
            "[Epoch 42 / 50 | Batch: 246 / 301] loss:   1279.419\n",
            "[Epoch 42 / 50 | Batch: 247 / 301] loss:   1128.971\n",
            "[Epoch 42 / 50 | Batch: 248 / 301] loss:    802.529\n",
            "[Epoch 42 / 50 | Batch: 249 / 301] loss:    991.236\n",
            "[Epoch 42 / 50 | Batch: 250 / 301] loss:   1447.779\n",
            "[Epoch 42 / 50 | Batch: 251 / 301] loss:   1016.158\n",
            "[Epoch 42 / 50 | Batch: 252 / 301] loss:   1023.839\n",
            "[Epoch 42 / 50 | Batch: 253 / 301] loss:   1038.689\n",
            "[Epoch 42 / 50 | Batch: 254 / 301] loss:   1141.657\n",
            "[Epoch 42 / 50 | Batch: 255 / 301] loss:   1082.400\n",
            "[Epoch 42 / 50 | Batch: 256 / 301] loss:   1555.526\n",
            "[Epoch 42 / 50 | Batch: 257 / 301] loss:   1328.981\n",
            "[Epoch 42 / 50 | Batch: 258 / 301] loss:   1240.503\n",
            "[Epoch 42 / 50 | Batch: 259 / 301] loss:   1112.444\n",
            "[Epoch 42 / 50 | Batch: 260 / 301] loss:    930.139\n",
            "[Epoch 42 / 50 | Batch: 261 / 301] loss:   1149.308\n",
            "[Epoch 42 / 50 | Batch: 262 / 301] loss:   1373.375\n",
            "[Epoch 42 / 50 | Batch: 263 / 301] loss:    928.039\n",
            "[Epoch 42 / 50 | Batch: 264 / 301] loss:   1390.960\n",
            "[Epoch 42 / 50 | Batch: 265 / 301] loss:   1111.824\n",
            "[Epoch 42 / 50 | Batch: 266 / 301] loss:   1188.583\n",
            "[Epoch 42 / 50 | Batch: 267 / 301] loss:   1919.466\n",
            "[Epoch 42 / 50 | Batch: 268 / 301] loss:    907.603\n",
            "[Epoch 42 / 50 | Batch: 269 / 301] loss:   1249.133\n",
            "[Epoch 42 / 50 | Batch: 270 / 301] loss:   1008.363\n",
            "[Epoch 42 / 50 | Batch: 271 / 301] loss:   1138.200\n",
            "[Epoch 42 / 50 | Batch: 272 / 301] loss:   1104.822\n",
            "[Epoch 42 / 50 | Batch: 273 / 301] loss:   1448.186\n",
            "[Epoch 42 / 50 | Batch: 274 / 301] loss:   1399.196\n",
            "[Epoch 42 / 50 | Batch: 275 / 301] loss:   1101.479\n",
            "[Epoch 42 / 50 | Batch: 276 / 301] loss:   1177.768\n",
            "[Epoch 42 / 50 | Batch: 277 / 301] loss:   1012.296\n",
            "[Epoch 42 / 50 | Batch: 278 / 301] loss:   1054.512\n",
            "[Epoch 42 / 50 | Batch: 279 / 301] loss:    912.807\n",
            "[Epoch 42 / 50 | Batch: 280 / 301] loss:   1317.194\n",
            "[Epoch 42 / 50 | Batch: 281 / 301] loss:    975.012\n",
            "[Epoch 42 / 50 | Batch: 282 / 301] loss:   1366.948\n",
            "[Epoch 42 / 50 | Batch: 283 / 301] loss:   1115.164\n",
            "[Epoch 42 / 50 | Batch: 284 / 301] loss:   1203.551\n",
            "[Epoch 42 / 50 | Batch: 285 / 301] loss:   1248.716\n",
            "[Epoch 42 / 50 | Batch: 286 / 301] loss:   1006.312\n",
            "[Epoch 42 / 50 | Batch: 287 / 301] loss:   1269.977\n",
            "[Epoch 42 / 50 | Batch: 288 / 301] loss:   1405.877\n",
            "[Epoch 42 / 50 | Batch: 289 / 301] loss:   1401.357\n",
            "[Epoch 42 / 50 | Batch: 290 / 301] loss:   1372.529\n",
            "[Epoch 42 / 50 | Batch: 291 / 301] loss:   1427.823\n",
            "[Epoch 42 / 50 | Batch: 292 / 301] loss:    913.038\n",
            "[Epoch 42 / 50 | Batch: 293 / 301] loss:   1438.213\n",
            "[Epoch 42 / 50 | Batch: 294 / 301] loss:   1301.091\n",
            "[Epoch 42 / 50 | Batch: 295 / 301] loss:   1548.251\n",
            "[Epoch 42 / 50 | Batch: 296 / 301] loss:   1079.082\n",
            "[Epoch 42 / 50 | Batch: 297 / 301] loss:   1088.973\n",
            "[Epoch 42 / 50 | Batch: 298 / 301] loss:   1225.425\n",
            "[Epoch 42 / 50 | Batch: 299 / 301] loss:   1464.004\n",
            "[Epoch 42 / 50 | Batch: 300 / 301] loss:   1259.697\n",
            "[Epoch 42 / 50 | Batch: 301 / 301] loss:    853.605\n",
            "Epoch loss: 1161.56714\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4630.891\n",
            "[Validation] [Batch  2 / 15] dev loss:   6790.164\n",
            "[Validation] [Batch  3 / 15] dev loss:   6658.641\n",
            "[Validation] [Batch  4 / 15] dev loss:   5584.973\n",
            "[Validation] [Batch  5 / 15] dev loss:   5287.622\n",
            "[Validation] [Batch  6 / 15] dev loss:  10384.953\n",
            "[Validation] [Batch  7 / 15] dev loss:   5885.432\n",
            "[Validation] [Batch  8 / 15] dev loss:   4395.079\n",
            "[Validation] [Batch  9 / 15] dev loss:   3296.522\n",
            "[Validation] [Batch 10 / 15] dev loss:  15000.564\n",
            "[Validation] [Batch 11 / 15] dev loss:  12690.880\n",
            "[Validation] [Batch 12 / 15] dev loss:  11050.547\n",
            "[Validation] [Batch 13 / 15] dev loss:   4561.423\n",
            "[Validation] [Batch 14 / 15] dev loss:  22138.881\n",
            "[Validation] [Batch 15 / 15] dev loss:   2251.487\n",
            "Dev loss 8040.53724\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 43 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 43 / 50 | Batch:  1 / 301] loss:    810.509\n",
            "[Epoch 43 / 50 | Batch:  2 / 301] loss:   1081.014\n",
            "[Epoch 43 / 50 | Batch:  3 / 301] loss:   1322.798\n",
            "[Epoch 43 / 50 | Batch:  4 / 301] loss:    882.719\n",
            "[Epoch 43 / 50 | Batch:  5 / 301] loss:   1153.807\n",
            "[Epoch 43 / 50 | Batch:  6 / 301] loss:   1277.848\n",
            "[Epoch 43 / 50 | Batch:  7 / 301] loss:   1260.415\n",
            "[Epoch 43 / 50 | Batch:  8 / 301] loss:    983.339\n",
            "[Epoch 43 / 50 | Batch:  9 / 301] loss:   1330.919\n",
            "[Epoch 43 / 50 | Batch: 10 / 301] loss:    998.371\n",
            "[Epoch 43 / 50 | Batch: 11 / 301] loss:   1154.521\n",
            "[Epoch 43 / 50 | Batch: 12 / 301] loss:    987.006\n",
            "[Epoch 43 / 50 | Batch: 13 / 301] loss:   1392.104\n",
            "[Epoch 43 / 50 | Batch: 14 / 301] loss:   1420.264\n",
            "[Epoch 43 / 50 | Batch: 15 / 301] loss:   1655.275\n",
            "[Epoch 43 / 50 | Batch: 16 / 301] loss:   1220.382\n",
            "[Epoch 43 / 50 | Batch: 17 / 301] loss:   1049.002\n",
            "[Epoch 43 / 50 | Batch: 18 / 301] loss:   1134.826\n",
            "[Epoch 43 / 50 | Batch: 19 / 301] loss:    877.913\n",
            "[Epoch 43 / 50 | Batch: 20 / 301] loss:   1017.633\n",
            "[Epoch 43 / 50 | Batch: 21 / 301] loss:   1506.743\n",
            "[Epoch 43 / 50 | Batch: 22 / 301] loss:   1156.604\n",
            "[Epoch 43 / 50 | Batch: 23 / 301] loss:   1267.912\n",
            "[Epoch 43 / 50 | Batch: 24 / 301] loss:   1160.142\n",
            "[Epoch 43 / 50 | Batch: 25 / 301] loss:   1491.557\n",
            "[Epoch 43 / 50 | Batch: 26 / 301] loss:   1294.754\n",
            "[Epoch 43 / 50 | Batch: 27 / 301] loss:   1153.648\n",
            "[Epoch 43 / 50 | Batch: 28 / 301] loss:   1312.862\n",
            "[Epoch 43 / 50 | Batch: 29 / 301] loss:   1312.214\n",
            "[Epoch 43 / 50 | Batch: 30 / 301] loss:   1318.924\n",
            "[Epoch 43 / 50 | Batch: 31 / 301] loss:   1162.490\n",
            "[Epoch 43 / 50 | Batch: 32 / 301] loss:   1150.415\n",
            "[Epoch 43 / 50 | Batch: 33 / 301] loss:    969.752\n",
            "[Epoch 43 / 50 | Batch: 34 / 301] loss:   1052.007\n",
            "[Epoch 43 / 50 | Batch: 35 / 301] loss:   1479.697\n",
            "[Epoch 43 / 50 | Batch: 36 / 301] loss:   1045.047\n",
            "[Epoch 43 / 50 | Batch: 37 / 301] loss:   1296.125\n",
            "[Epoch 43 / 50 | Batch: 38 / 301] loss:   1125.271\n",
            "[Epoch 43 / 50 | Batch: 39 / 301] loss:   1511.695\n",
            "[Epoch 43 / 50 | Batch: 40 / 301] loss:    954.766\n",
            "[Epoch 43 / 50 | Batch: 41 / 301] loss:    991.232\n",
            "[Epoch 43 / 50 | Batch: 42 / 301] loss:   1219.357\n",
            "[Epoch 43 / 50 | Batch: 43 / 301] loss:   1146.819\n",
            "[Epoch 43 / 50 | Batch: 44 / 301] loss:   1164.731\n",
            "[Epoch 43 / 50 | Batch: 45 / 301] loss:   1115.201\n",
            "[Epoch 43 / 50 | Batch: 46 / 301] loss:   1036.665\n",
            "[Epoch 43 / 50 | Batch: 47 / 301] loss:   1044.045\n",
            "[Epoch 43 / 50 | Batch: 48 / 301] loss:   1339.875\n",
            "[Epoch 43 / 50 | Batch: 49 / 301] loss:   1001.929\n",
            "[Epoch 43 / 50 | Batch: 50 / 301] loss:   1211.563\n",
            "[Epoch 43 / 50 | Batch: 51 / 301] loss:   1286.819\n",
            "[Epoch 43 / 50 | Batch: 52 / 301] loss:    949.041\n",
            "[Epoch 43 / 50 | Batch: 53 / 301] loss:   1440.033\n",
            "[Epoch 43 / 50 | Batch: 54 / 301] loss:   1255.396\n",
            "[Epoch 43 / 50 | Batch: 55 / 301] loss:   1097.102\n",
            "[Epoch 43 / 50 | Batch: 56 / 301] loss:   1179.689\n",
            "[Epoch 43 / 50 | Batch: 57 / 301] loss:    993.779\n",
            "[Epoch 43 / 50 | Batch: 58 / 301] loss:   1262.631\n",
            "[Epoch 43 / 50 | Batch: 59 / 301] loss:   1212.636\n",
            "[Epoch 43 / 50 | Batch: 60 / 301] loss:   1213.389\n",
            "[Epoch 43 / 50 | Batch: 61 / 301] loss:   1077.446\n",
            "[Epoch 43 / 50 | Batch: 62 / 301] loss:   1412.944\n",
            "[Epoch 43 / 50 | Batch: 63 / 301] loss:   1206.177\n",
            "[Epoch 43 / 50 | Batch: 64 / 301] loss:    804.355\n",
            "[Epoch 43 / 50 | Batch: 65 / 301] loss:   1163.023\n",
            "[Epoch 43 / 50 | Batch: 66 / 301] loss:   1183.270\n",
            "[Epoch 43 / 50 | Batch: 67 / 301] loss:    992.385\n",
            "[Epoch 43 / 50 | Batch: 68 / 301] loss:   1085.220\n",
            "[Epoch 43 / 50 | Batch: 69 / 301] loss:   1200.296\n",
            "[Epoch 43 / 50 | Batch: 70 / 301] loss:    928.569\n",
            "[Epoch 43 / 50 | Batch: 71 / 301] loss:   1095.108\n",
            "[Epoch 43 / 50 | Batch: 72 / 301] loss:   1337.168\n",
            "[Epoch 43 / 50 | Batch: 73 / 301] loss:   1035.576\n",
            "[Epoch 43 / 50 | Batch: 74 / 301] loss:   1157.859\n",
            "[Epoch 43 / 50 | Batch: 75 / 301] loss:    814.733\n",
            "[Epoch 43 / 50 | Batch: 76 / 301] loss:   1233.705\n",
            "[Epoch 43 / 50 | Batch: 77 / 301] loss:    915.651\n",
            "[Epoch 43 / 50 | Batch: 78 / 301] loss:   1223.942\n",
            "[Epoch 43 / 50 | Batch: 79 / 301] loss:   1459.851\n",
            "[Epoch 43 / 50 | Batch: 80 / 301] loss:   1058.456\n",
            "[Epoch 43 / 50 | Batch: 81 / 301] loss:   1176.567\n",
            "[Epoch 43 / 50 | Batch: 82 / 301] loss:   1284.176\n",
            "[Epoch 43 / 50 | Batch: 83 / 301] loss:   1057.037\n",
            "[Epoch 43 / 50 | Batch: 84 / 301] loss:    876.177\n",
            "[Epoch 43 / 50 | Batch: 85 / 301] loss:    835.355\n",
            "[Epoch 43 / 50 | Batch: 86 / 301] loss:   1503.786\n",
            "[Epoch 43 / 50 | Batch: 87 / 301] loss:   1093.226\n",
            "[Epoch 43 / 50 | Batch: 88 / 301] loss:   1128.303\n",
            "[Epoch 43 / 50 | Batch: 89 / 301] loss:   1200.275\n",
            "[Epoch 43 / 50 | Batch: 90 / 301] loss:   1162.868\n",
            "[Epoch 43 / 50 | Batch: 91 / 301] loss:   1170.183\n",
            "[Epoch 43 / 50 | Batch: 92 / 301] loss:    867.141\n",
            "[Epoch 43 / 50 | Batch: 93 / 301] loss:   1234.560\n",
            "[Epoch 43 / 50 | Batch: 94 / 301] loss:    989.857\n",
            "[Epoch 43 / 50 | Batch: 95 / 301] loss:   1064.230\n",
            "[Epoch 43 / 50 | Batch: 96 / 301] loss:    910.316\n",
            "[Epoch 43 / 50 | Batch: 97 / 301] loss:    798.993\n",
            "[Epoch 43 / 50 | Batch: 98 / 301] loss:   1209.758\n",
            "[Epoch 43 / 50 | Batch: 99 / 301] loss:   1179.409\n",
            "[Epoch 43 / 50 | Batch: 100 / 301] loss:   1223.817\n",
            "[Epoch 43 / 50 | Batch: 101 / 301] loss:   1006.384\n",
            "[Epoch 43 / 50 | Batch: 102 / 301] loss:   1049.627\n",
            "[Epoch 43 / 50 | Batch: 103 / 301] loss:   1217.288\n",
            "[Epoch 43 / 50 | Batch: 104 / 301] loss:   1089.739\n",
            "[Epoch 43 / 50 | Batch: 105 / 301] loss:   1034.056\n",
            "[Epoch 43 / 50 | Batch: 106 / 301] loss:   1282.647\n",
            "[Epoch 43 / 50 | Batch: 107 / 301] loss:   1142.929\n",
            "[Epoch 43 / 50 | Batch: 108 / 301] loss:   1079.203\n",
            "[Epoch 43 / 50 | Batch: 109 / 301] loss:    822.911\n",
            "[Epoch 43 / 50 | Batch: 110 / 301] loss:   1249.438\n",
            "[Epoch 43 / 50 | Batch: 111 / 301] loss:   1080.136\n",
            "[Epoch 43 / 50 | Batch: 112 / 301] loss:   1263.225\n",
            "[Epoch 43 / 50 | Batch: 113 / 301] loss:    997.508\n",
            "[Epoch 43 / 50 | Batch: 114 / 301] loss:   1335.332\n",
            "[Epoch 43 / 50 | Batch: 115 / 301] loss:   1346.233\n",
            "[Epoch 43 / 50 | Batch: 116 / 301] loss:   1075.052\n",
            "[Epoch 43 / 50 | Batch: 117 / 301] loss:   1159.601\n",
            "[Epoch 43 / 50 | Batch: 118 / 301] loss:   1660.104\n",
            "[Epoch 43 / 50 | Batch: 119 / 301] loss:    699.745\n",
            "[Epoch 43 / 50 | Batch: 120 / 301] loss:   1123.660\n",
            "[Epoch 43 / 50 | Batch: 121 / 301] loss:   1180.035\n",
            "[Epoch 43 / 50 | Batch: 122 / 301] loss:   1027.089\n",
            "[Epoch 43 / 50 | Batch: 123 / 301] loss:   1225.368\n",
            "[Epoch 43 / 50 | Batch: 124 / 301] loss:   1552.553\n",
            "[Epoch 43 / 50 | Batch: 125 / 301] loss:   1013.971\n",
            "[Epoch 43 / 50 | Batch: 126 / 301] loss:   1296.354\n",
            "[Epoch 43 / 50 | Batch: 127 / 301] loss:    918.936\n",
            "[Epoch 43 / 50 | Batch: 128 / 301] loss:   1202.958\n",
            "[Epoch 43 / 50 | Batch: 129 / 301] loss:    969.643\n",
            "[Epoch 43 / 50 | Batch: 130 / 301] loss:   1621.646\n",
            "[Epoch 43 / 50 | Batch: 131 / 301] loss:   1191.631\n",
            "[Epoch 43 / 50 | Batch: 132 / 301] loss:    906.633\n",
            "[Epoch 43 / 50 | Batch: 133 / 301] loss:    936.313\n",
            "[Epoch 43 / 50 | Batch: 134 / 301] loss:   1733.490\n",
            "[Epoch 43 / 50 | Batch: 135 / 301] loss:   1488.394\n",
            "[Epoch 43 / 50 | Batch: 136 / 301] loss:   1447.100\n",
            "[Epoch 43 / 50 | Batch: 137 / 301] loss:   1421.443\n",
            "[Epoch 43 / 50 | Batch: 138 / 301] loss:   1137.650\n",
            "[Epoch 43 / 50 | Batch: 139 / 301] loss:   1300.799\n",
            "[Epoch 43 / 50 | Batch: 140 / 301] loss:    780.486\n",
            "[Epoch 43 / 50 | Batch: 141 / 301] loss:   1017.133\n",
            "[Epoch 43 / 50 | Batch: 142 / 301] loss:    932.943\n",
            "[Epoch 43 / 50 | Batch: 143 / 301] loss:   1355.674\n",
            "[Epoch 43 / 50 | Batch: 144 / 301] loss:    996.249\n",
            "[Epoch 43 / 50 | Batch: 145 / 301] loss:   1369.135\n",
            "[Epoch 43 / 50 | Batch: 146 / 301] loss:   1395.465\n",
            "[Epoch 43 / 50 | Batch: 147 / 301] loss:   1030.265\n",
            "[Epoch 43 / 50 | Batch: 148 / 301] loss:   1339.505\n",
            "[Epoch 43 / 50 | Batch: 149 / 301] loss:   1213.190\n",
            "[Epoch 43 / 50 | Batch: 150 / 301] loss:   1195.852\n",
            "[Epoch 43 / 50 | Batch: 151 / 301] loss:    995.301\n",
            "[Epoch 43 / 50 | Batch: 152 / 301] loss:   1052.428\n",
            "[Epoch 43 / 50 | Batch: 153 / 301] loss:   1278.108\n",
            "[Epoch 43 / 50 | Batch: 154 / 301] loss:   1005.359\n",
            "[Epoch 43 / 50 | Batch: 155 / 301] loss:   1483.587\n",
            "[Epoch 43 / 50 | Batch: 156 / 301] loss:   1636.106\n",
            "[Epoch 43 / 50 | Batch: 157 / 301] loss:   1183.562\n",
            "[Epoch 43 / 50 | Batch: 158 / 301] loss:   1190.547\n",
            "[Epoch 43 / 50 | Batch: 159 / 301] loss:   1249.188\n",
            "[Epoch 43 / 50 | Batch: 160 / 301] loss:   1003.154\n",
            "[Epoch 43 / 50 | Batch: 161 / 301] loss:    889.432\n",
            "[Epoch 43 / 50 | Batch: 162 / 301] loss:    917.396\n",
            "[Epoch 43 / 50 | Batch: 163 / 301] loss:   1222.763\n",
            "[Epoch 43 / 50 | Batch: 164 / 301] loss:   1105.441\n",
            "[Epoch 43 / 50 | Batch: 165 / 301] loss:   1205.220\n",
            "[Epoch 43 / 50 | Batch: 166 / 301] loss:   1254.915\n",
            "[Epoch 43 / 50 | Batch: 167 / 301] loss:   1209.267\n",
            "[Epoch 43 / 50 | Batch: 168 / 301] loss:   1084.983\n",
            "[Epoch 43 / 50 | Batch: 169 / 301] loss:   1196.126\n",
            "[Epoch 43 / 50 | Batch: 170 / 301] loss:   1290.527\n",
            "[Epoch 43 / 50 | Batch: 171 / 301] loss:   1329.891\n",
            "[Epoch 43 / 50 | Batch: 172 / 301] loss:   1023.947\n",
            "[Epoch 43 / 50 | Batch: 173 / 301] loss:   1085.739\n",
            "[Epoch 43 / 50 | Batch: 174 / 301] loss:   1153.591\n",
            "[Epoch 43 / 50 | Batch: 175 / 301] loss:   1353.841\n",
            "[Epoch 43 / 50 | Batch: 176 / 301] loss:   1191.905\n",
            "[Epoch 43 / 50 | Batch: 177 / 301] loss:   1298.555\n",
            "[Epoch 43 / 50 | Batch: 178 / 301] loss:   1120.714\n",
            "[Epoch 43 / 50 | Batch: 179 / 301] loss:   1174.438\n",
            "[Epoch 43 / 50 | Batch: 180 / 301] loss:   1542.137\n",
            "[Epoch 43 / 50 | Batch: 181 / 301] loss:   1228.157\n",
            "[Epoch 43 / 50 | Batch: 182 / 301] loss:   1347.404\n",
            "[Epoch 43 / 50 | Batch: 183 / 301] loss:    989.728\n",
            "[Epoch 43 / 50 | Batch: 184 / 301] loss:    985.263\n",
            "[Epoch 43 / 50 | Batch: 185 / 301] loss:   1055.792\n",
            "[Epoch 43 / 50 | Batch: 186 / 301] loss:   1317.507\n",
            "[Epoch 43 / 50 | Batch: 187 / 301] loss:   1045.745\n",
            "[Epoch 43 / 50 | Batch: 188 / 301] loss:   1075.862\n",
            "[Epoch 43 / 50 | Batch: 189 / 301] loss:    906.968\n",
            "[Epoch 43 / 50 | Batch: 190 / 301] loss:   1231.712\n",
            "[Epoch 43 / 50 | Batch: 191 / 301] loss:   1197.798\n",
            "[Epoch 43 / 50 | Batch: 192 / 301] loss:   1256.475\n",
            "[Epoch 43 / 50 | Batch: 193 / 301] loss:   1159.470\n",
            "[Epoch 43 / 50 | Batch: 194 / 301] loss:   1033.630\n",
            "[Epoch 43 / 50 | Batch: 195 / 301] loss:   1264.379\n",
            "[Epoch 43 / 50 | Batch: 196 / 301] loss:    971.243\n",
            "[Epoch 43 / 50 | Batch: 197 / 301] loss:   1255.925\n",
            "[Epoch 43 / 50 | Batch: 198 / 301] loss:   1208.870\n",
            "[Epoch 43 / 50 | Batch: 199 / 301] loss:    907.966\n",
            "[Epoch 43 / 50 | Batch: 200 / 301] loss:   1088.295\n",
            "[Epoch 43 / 50 | Batch: 201 / 301] loss:   1630.804\n",
            "[Epoch 43 / 50 | Batch: 202 / 301] loss:   1013.571\n",
            "[Epoch 43 / 50 | Batch: 203 / 301] loss:   1428.018\n",
            "[Epoch 43 / 50 | Batch: 204 / 301] loss:   1065.903\n",
            "[Epoch 43 / 50 | Batch: 205 / 301] loss:    906.557\n",
            "[Epoch 43 / 50 | Batch: 206 / 301] loss:   1004.789\n",
            "[Epoch 43 / 50 | Batch: 207 / 301] loss:   1234.310\n",
            "[Epoch 43 / 50 | Batch: 208 / 301] loss:    949.494\n",
            "[Epoch 43 / 50 | Batch: 209 / 301] loss:   1011.778\n",
            "[Epoch 43 / 50 | Batch: 210 / 301] loss:    868.830\n",
            "[Epoch 43 / 50 | Batch: 211 / 301] loss:   1151.193\n",
            "[Epoch 43 / 50 | Batch: 212 / 301] loss:    750.962\n",
            "[Epoch 43 / 50 | Batch: 213 / 301] loss:   1093.190\n",
            "[Epoch 43 / 50 | Batch: 214 / 301] loss:   1220.434\n",
            "[Epoch 43 / 50 | Batch: 215 / 301] loss:   1159.400\n",
            "[Epoch 43 / 50 | Batch: 216 / 301] loss:   1113.700\n",
            "[Epoch 43 / 50 | Batch: 217 / 301] loss:   1478.514\n",
            "[Epoch 43 / 50 | Batch: 218 / 301] loss:    839.272\n",
            "[Epoch 43 / 50 | Batch: 219 / 301] loss:   1520.405\n",
            "[Epoch 43 / 50 | Batch: 220 / 301] loss:    950.269\n",
            "[Epoch 43 / 50 | Batch: 221 / 301] loss:   1199.012\n",
            "[Epoch 43 / 50 | Batch: 222 / 301] loss:   1068.109\n",
            "[Epoch 43 / 50 | Batch: 223 / 301] loss:   1149.264\n",
            "[Epoch 43 / 50 | Batch: 224 / 301] loss:   1093.715\n",
            "[Epoch 43 / 50 | Batch: 225 / 301] loss:   1166.727\n",
            "[Epoch 43 / 50 | Batch: 226 / 301] loss:   1262.735\n",
            "[Epoch 43 / 50 | Batch: 227 / 301] loss:   1405.671\n",
            "[Epoch 43 / 50 | Batch: 228 / 301] loss:   1309.288\n",
            "[Epoch 43 / 50 | Batch: 229 / 301] loss:   1178.218\n",
            "[Epoch 43 / 50 | Batch: 230 / 301] loss:    905.912\n",
            "[Epoch 43 / 50 | Batch: 231 / 301] loss:   1142.123\n",
            "[Epoch 43 / 50 | Batch: 232 / 301] loss:   1300.616\n",
            "[Epoch 43 / 50 | Batch: 233 / 301] loss:   1311.862\n",
            "[Epoch 43 / 50 | Batch: 234 / 301] loss:   1401.640\n",
            "[Epoch 43 / 50 | Batch: 235 / 301] loss:    909.191\n",
            "[Epoch 43 / 50 | Batch: 236 / 301] loss:   1315.735\n",
            "[Epoch 43 / 50 | Batch: 237 / 301] loss:   1025.454\n",
            "[Epoch 43 / 50 | Batch: 238 / 301] loss:   1155.437\n",
            "[Epoch 43 / 50 | Batch: 239 / 301] loss:   1293.337\n",
            "[Epoch 43 / 50 | Batch: 240 / 301] loss:   1138.092\n",
            "[Epoch 43 / 50 | Batch: 241 / 301] loss:   1010.499\n",
            "[Epoch 43 / 50 | Batch: 242 / 301] loss:   1237.956\n",
            "[Epoch 43 / 50 | Batch: 243 / 301] loss:   1071.803\n",
            "[Epoch 43 / 50 | Batch: 244 / 301] loss:   1145.805\n",
            "[Epoch 43 / 50 | Batch: 245 / 301] loss:   1100.836\n",
            "[Epoch 43 / 50 | Batch: 246 / 301] loss:   1544.139\n",
            "[Epoch 43 / 50 | Batch: 247 / 301] loss:   1373.224\n",
            "[Epoch 43 / 50 | Batch: 248 / 301] loss:   1131.810\n",
            "[Epoch 43 / 50 | Batch: 249 / 301] loss:   1057.658\n",
            "[Epoch 43 / 50 | Batch: 250 / 301] loss:   1173.849\n",
            "[Epoch 43 / 50 | Batch: 251 / 301] loss:    941.839\n",
            "[Epoch 43 / 50 | Batch: 252 / 301] loss:   1299.738\n",
            "[Epoch 43 / 50 | Batch: 253 / 301] loss:   1488.605\n",
            "[Epoch 43 / 50 | Batch: 254 / 301] loss:   1241.404\n",
            "[Epoch 43 / 50 | Batch: 255 / 301] loss:   1326.828\n",
            "[Epoch 43 / 50 | Batch: 256 / 301] loss:   1468.461\n",
            "[Epoch 43 / 50 | Batch: 257 / 301] loss:   1432.710\n",
            "[Epoch 43 / 50 | Batch: 258 / 301] loss:   1017.234\n",
            "[Epoch 43 / 50 | Batch: 259 / 301] loss:   1198.330\n",
            "[Epoch 43 / 50 | Batch: 260 / 301] loss:   1214.427\n",
            "[Epoch 43 / 50 | Batch: 261 / 301] loss:   1287.308\n",
            "[Epoch 43 / 50 | Batch: 262 / 301] loss:   1228.928\n",
            "[Epoch 43 / 50 | Batch: 263 / 301] loss:    804.233\n",
            "[Epoch 43 / 50 | Batch: 264 / 301] loss:   1003.620\n",
            "[Epoch 43 / 50 | Batch: 265 / 301] loss:   1367.148\n",
            "[Epoch 43 / 50 | Batch: 266 / 301] loss:    994.550\n",
            "[Epoch 43 / 50 | Batch: 267 / 301] loss:   1205.596\n",
            "[Epoch 43 / 50 | Batch: 268 / 301] loss:   1080.813\n",
            "[Epoch 43 / 50 | Batch: 269 / 301] loss:   1589.779\n",
            "[Epoch 43 / 50 | Batch: 270 / 301] loss:    972.650\n",
            "[Epoch 43 / 50 | Batch: 271 / 301] loss:   1245.458\n",
            "[Epoch 43 / 50 | Batch: 272 / 301] loss:   1159.110\n",
            "[Epoch 43 / 50 | Batch: 273 / 301] loss:    835.892\n",
            "[Epoch 43 / 50 | Batch: 274 / 301] loss:   1188.509\n",
            "[Epoch 43 / 50 | Batch: 275 / 301] loss:   1037.540\n",
            "[Epoch 43 / 50 | Batch: 276 / 301] loss:   1063.082\n",
            "[Epoch 43 / 50 | Batch: 277 / 301] loss:   1333.974\n",
            "[Epoch 43 / 50 | Batch: 278 / 301] loss:    846.059\n",
            "[Epoch 43 / 50 | Batch: 279 / 301] loss:    881.150\n",
            "[Epoch 43 / 50 | Batch: 280 / 301] loss:   1097.983\n",
            "[Epoch 43 / 50 | Batch: 281 / 301] loss:   1149.113\n",
            "[Epoch 43 / 50 | Batch: 282 / 301] loss:   1007.345\n",
            "[Epoch 43 / 50 | Batch: 283 / 301] loss:   1110.286\n",
            "[Epoch 43 / 50 | Batch: 284 / 301] loss:   1011.575\n",
            "[Epoch 43 / 50 | Batch: 285 / 301] loss:   1227.084\n",
            "[Epoch 43 / 50 | Batch: 286 / 301] loss:    856.155\n",
            "[Epoch 43 / 50 | Batch: 287 / 301] loss:   1288.473\n",
            "[Epoch 43 / 50 | Batch: 288 / 301] loss:   1147.885\n",
            "[Epoch 43 / 50 | Batch: 289 / 301] loss:   1188.138\n",
            "[Epoch 43 / 50 | Batch: 290 / 301] loss:   1333.433\n",
            "[Epoch 43 / 50 | Batch: 291 / 301] loss:   1038.027\n",
            "[Epoch 43 / 50 | Batch: 292 / 301] loss:   1327.522\n",
            "[Epoch 43 / 50 | Batch: 293 / 301] loss:    921.775\n",
            "[Epoch 43 / 50 | Batch: 294 / 301] loss:   1318.160\n",
            "[Epoch 43 / 50 | Batch: 295 / 301] loss:   1407.573\n",
            "[Epoch 43 / 50 | Batch: 296 / 301] loss:   1069.761\n",
            "[Epoch 43 / 50 | Batch: 297 / 301] loss:    916.187\n",
            "[Epoch 43 / 50 | Batch: 298 / 301] loss:   1269.263\n",
            "[Epoch 43 / 50 | Batch: 299 / 301] loss:   1072.622\n",
            "[Epoch 43 / 50 | Batch: 300 / 301] loss:   1219.391\n",
            "[Epoch 43 / 50 | Batch: 301 / 301] loss:    681.396\n",
            "Epoch loss: 1158.80832\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4361.466\n",
            "[Validation] [Batch  2 / 15] dev loss:   6465.299\n",
            "[Validation] [Batch  3 / 15] dev loss:   7286.070\n",
            "[Validation] [Batch  4 / 15] dev loss:   4969.853\n",
            "[Validation] [Batch  5 / 15] dev loss:   5465.662\n",
            "[Validation] [Batch  6 / 15] dev loss:   8523.979\n",
            "[Validation] [Batch  7 / 15] dev loss:   6376.219\n",
            "[Validation] [Batch  8 / 15] dev loss:   4218.005\n",
            "[Validation] [Batch  9 / 15] dev loss:   3823.427\n",
            "[Validation] [Batch 10 / 15] dev loss:  15880.565\n",
            "[Validation] [Batch 11 / 15] dev loss:  12927.724\n",
            "[Validation] [Batch 12 / 15] dev loss:  10711.147\n",
            "[Validation] [Batch 13 / 15] dev loss:   4561.615\n",
            "[Validation] [Batch 14 / 15] dev loss:  24309.391\n",
            "[Validation] [Batch 15 / 15] dev loss:   2369.395\n",
            "Dev loss 8149.98781\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 44 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 44 / 50 | Batch:  1 / 301] loss:   1402.402\n",
            "[Epoch 44 / 50 | Batch:  2 / 301] loss:    914.095\n",
            "[Epoch 44 / 50 | Batch:  3 / 301] loss:   1008.595\n",
            "[Epoch 44 / 50 | Batch:  4 / 301] loss:   1223.807\n",
            "[Epoch 44 / 50 | Batch:  5 / 301] loss:    927.007\n",
            "[Epoch 44 / 50 | Batch:  6 / 301] loss:   1286.253\n",
            "[Epoch 44 / 50 | Batch:  7 / 301] loss:   1071.504\n",
            "[Epoch 44 / 50 | Batch:  8 / 301] loss:   1098.904\n",
            "[Epoch 44 / 50 | Batch:  9 / 301] loss:    993.932\n",
            "[Epoch 44 / 50 | Batch: 10 / 301] loss:   1188.212\n",
            "[Epoch 44 / 50 | Batch: 11 / 301] loss:   1610.423\n",
            "[Epoch 44 / 50 | Batch: 12 / 301] loss:    919.717\n",
            "[Epoch 44 / 50 | Batch: 13 / 301] loss:   1158.068\n",
            "[Epoch 44 / 50 | Batch: 14 / 301] loss:   1215.887\n",
            "[Epoch 44 / 50 | Batch: 15 / 301] loss:   1434.141\n",
            "[Epoch 44 / 50 | Batch: 16 / 301] loss:   1469.700\n",
            "[Epoch 44 / 50 | Batch: 17 / 301] loss:    683.524\n",
            "[Epoch 44 / 50 | Batch: 18 / 301] loss:    906.019\n",
            "[Epoch 44 / 50 | Batch: 19 / 301] loss:   1311.193\n",
            "[Epoch 44 / 50 | Batch: 20 / 301] loss:   1165.014\n",
            "[Epoch 44 / 50 | Batch: 21 / 301] loss:    770.557\n",
            "[Epoch 44 / 50 | Batch: 22 / 301] loss:   1278.384\n",
            "[Epoch 44 / 50 | Batch: 23 / 301] loss:   1235.770\n",
            "[Epoch 44 / 50 | Batch: 24 / 301] loss:   1196.875\n",
            "[Epoch 44 / 50 | Batch: 25 / 301] loss:   1390.883\n",
            "[Epoch 44 / 50 | Batch: 26 / 301] loss:   1326.507\n",
            "[Epoch 44 / 50 | Batch: 27 / 301] loss:    870.548\n",
            "[Epoch 44 / 50 | Batch: 28 / 301] loss:   1054.046\n",
            "[Epoch 44 / 50 | Batch: 29 / 301] loss:    903.003\n",
            "[Epoch 44 / 50 | Batch: 30 / 301] loss:   1142.122\n",
            "[Epoch 44 / 50 | Batch: 31 / 301] loss:   1140.453\n",
            "[Epoch 44 / 50 | Batch: 32 / 301] loss:   1112.727\n",
            "[Epoch 44 / 50 | Batch: 33 / 301] loss:    828.016\n",
            "[Epoch 44 / 50 | Batch: 34 / 301] loss:   1002.983\n",
            "[Epoch 44 / 50 | Batch: 35 / 301] loss:   1006.623\n",
            "[Epoch 44 / 50 | Batch: 36 / 301] loss:    854.323\n",
            "[Epoch 44 / 50 | Batch: 37 / 301] loss:   1118.646\n",
            "[Epoch 44 / 50 | Batch: 38 / 301] loss:    939.850\n",
            "[Epoch 44 / 50 | Batch: 39 / 301] loss:   1217.654\n",
            "[Epoch 44 / 50 | Batch: 40 / 301] loss:   1022.509\n",
            "[Epoch 44 / 50 | Batch: 41 / 301] loss:   1153.741\n",
            "[Epoch 44 / 50 | Batch: 42 / 301] loss:   1138.380\n",
            "[Epoch 44 / 50 | Batch: 43 / 301] loss:   1423.043\n",
            "[Epoch 44 / 50 | Batch: 44 / 301] loss:    864.471\n",
            "[Epoch 44 / 50 | Batch: 45 / 301] loss:   1500.077\n",
            "[Epoch 44 / 50 | Batch: 46 / 301] loss:    945.837\n",
            "[Epoch 44 / 50 | Batch: 47 / 301] loss:   1079.307\n",
            "[Epoch 44 / 50 | Batch: 48 / 301] loss:   1144.315\n",
            "[Epoch 44 / 50 | Batch: 49 / 301] loss:   1333.387\n",
            "[Epoch 44 / 50 | Batch: 50 / 301] loss:   1615.440\n",
            "[Epoch 44 / 50 | Batch: 51 / 301] loss:   1345.428\n",
            "[Epoch 44 / 50 | Batch: 52 / 301] loss:   1152.822\n",
            "[Epoch 44 / 50 | Batch: 53 / 301] loss:   1202.464\n",
            "[Epoch 44 / 50 | Batch: 54 / 301] loss:   1243.954\n",
            "[Epoch 44 / 50 | Batch: 55 / 301] loss:   1252.870\n",
            "[Epoch 44 / 50 | Batch: 56 / 301] loss:   1412.632\n",
            "[Epoch 44 / 50 | Batch: 57 / 301] loss:   1129.856\n",
            "[Epoch 44 / 50 | Batch: 58 / 301] loss:   1356.147\n",
            "[Epoch 44 / 50 | Batch: 59 / 301] loss:   1157.959\n",
            "[Epoch 44 / 50 | Batch: 60 / 301] loss:    899.648\n",
            "[Epoch 44 / 50 | Batch: 61 / 301] loss:   1256.839\n",
            "[Epoch 44 / 50 | Batch: 62 / 301] loss:   1432.297\n",
            "[Epoch 44 / 50 | Batch: 63 / 301] loss:   1145.080\n",
            "[Epoch 44 / 50 | Batch: 64 / 301] loss:   1061.404\n",
            "[Epoch 44 / 50 | Batch: 65 / 301] loss:    829.934\n",
            "[Epoch 44 / 50 | Batch: 66 / 301] loss:   1026.532\n",
            "[Epoch 44 / 50 | Batch: 67 / 301] loss:    956.768\n",
            "[Epoch 44 / 50 | Batch: 68 / 301] loss:   1093.743\n",
            "[Epoch 44 / 50 | Batch: 69 / 301] loss:   1176.708\n",
            "[Epoch 44 / 50 | Batch: 70 / 301] loss:   1130.901\n",
            "[Epoch 44 / 50 | Batch: 71 / 301] loss:    884.041\n",
            "[Epoch 44 / 50 | Batch: 72 / 301] loss:   1491.648\n",
            "[Epoch 44 / 50 | Batch: 73 / 301] loss:    985.884\n",
            "[Epoch 44 / 50 | Batch: 74 / 301] loss:   1218.859\n",
            "[Epoch 44 / 50 | Batch: 75 / 301] loss:   1475.708\n",
            "[Epoch 44 / 50 | Batch: 76 / 301] loss:   1075.887\n",
            "[Epoch 44 / 50 | Batch: 77 / 301] loss:   1414.177\n",
            "[Epoch 44 / 50 | Batch: 78 / 301] loss:   1365.353\n",
            "[Epoch 44 / 50 | Batch: 79 / 301] loss:   1198.637\n",
            "[Epoch 44 / 50 | Batch: 80 / 301] loss:   1098.374\n",
            "[Epoch 44 / 50 | Batch: 81 / 301] loss:    944.428\n",
            "[Epoch 44 / 50 | Batch: 82 / 301] loss:   1185.569\n",
            "[Epoch 44 / 50 | Batch: 83 / 301] loss:   1222.516\n",
            "[Epoch 44 / 50 | Batch: 84 / 301] loss:   1280.835\n",
            "[Epoch 44 / 50 | Batch: 85 / 301] loss:   1251.449\n",
            "[Epoch 44 / 50 | Batch: 86 / 301] loss:    909.288\n",
            "[Epoch 44 / 50 | Batch: 87 / 301] loss:   1366.272\n",
            "[Epoch 44 / 50 | Batch: 88 / 301] loss:   1207.751\n",
            "[Epoch 44 / 50 | Batch: 89 / 301] loss:   1002.938\n",
            "[Epoch 44 / 50 | Batch: 90 / 301] loss:   1061.417\n",
            "[Epoch 44 / 50 | Batch: 91 / 301] loss:   1432.920\n",
            "[Epoch 44 / 50 | Batch: 92 / 301] loss:   1331.764\n",
            "[Epoch 44 / 50 | Batch: 93 / 301] loss:   1323.329\n",
            "[Epoch 44 / 50 | Batch: 94 / 301] loss:    996.533\n",
            "[Epoch 44 / 50 | Batch: 95 / 301] loss:   1299.208\n",
            "[Epoch 44 / 50 | Batch: 96 / 301] loss:    721.117\n",
            "[Epoch 44 / 50 | Batch: 97 / 301] loss:    877.953\n",
            "[Epoch 44 / 50 | Batch: 98 / 301] loss:   1100.300\n",
            "[Epoch 44 / 50 | Batch: 99 / 301] loss:   1242.800\n",
            "[Epoch 44 / 50 | Batch: 100 / 301] loss:   1377.648\n",
            "[Epoch 44 / 50 | Batch: 101 / 301] loss:   1555.507\n",
            "[Epoch 44 / 50 | Batch: 102 / 301] loss:   1245.107\n",
            "[Epoch 44 / 50 | Batch: 103 / 301] loss:   1254.852\n",
            "[Epoch 44 / 50 | Batch: 104 / 301] loss:   1154.392\n",
            "[Epoch 44 / 50 | Batch: 105 / 301] loss:    851.184\n",
            "[Epoch 44 / 50 | Batch: 106 / 301] loss:   1766.905\n",
            "[Epoch 44 / 50 | Batch: 107 / 301] loss:   1307.119\n",
            "[Epoch 44 / 50 | Batch: 108 / 301] loss:    881.134\n",
            "[Epoch 44 / 50 | Batch: 109 / 301] loss:   1040.870\n",
            "[Epoch 44 / 50 | Batch: 110 / 301] loss:   1229.004\n",
            "[Epoch 44 / 50 | Batch: 111 / 301] loss:   1067.286\n",
            "[Epoch 44 / 50 | Batch: 112 / 301] loss:    979.370\n",
            "[Epoch 44 / 50 | Batch: 113 / 301] loss:   1281.968\n",
            "[Epoch 44 / 50 | Batch: 114 / 301] loss:   1067.168\n",
            "[Epoch 44 / 50 | Batch: 115 / 301] loss:   1207.451\n",
            "[Epoch 44 / 50 | Batch: 116 / 301] loss:   1328.897\n",
            "[Epoch 44 / 50 | Batch: 117 / 301] loss:    900.278\n",
            "[Epoch 44 / 50 | Batch: 118 / 301] loss:   1210.797\n",
            "[Epoch 44 / 50 | Batch: 119 / 301] loss:   1253.830\n",
            "[Epoch 44 / 50 | Batch: 120 / 301] loss:   1037.374\n",
            "[Epoch 44 / 50 | Batch: 121 / 301] loss:   1297.937\n",
            "[Epoch 44 / 50 | Batch: 122 / 301] loss:   1270.372\n",
            "[Epoch 44 / 50 | Batch: 123 / 301] loss:    854.732\n",
            "[Epoch 44 / 50 | Batch: 124 / 301] loss:    765.947\n",
            "[Epoch 44 / 50 | Batch: 125 / 301] loss:   1008.989\n",
            "[Epoch 44 / 50 | Batch: 126 / 301] loss:    925.995\n",
            "[Epoch 44 / 50 | Batch: 127 / 301] loss:   1343.876\n",
            "[Epoch 44 / 50 | Batch: 128 / 301] loss:   1188.161\n",
            "[Epoch 44 / 50 | Batch: 129 / 301] loss:   1001.547\n",
            "[Epoch 44 / 50 | Batch: 130 / 301] loss:   1316.581\n",
            "[Epoch 44 / 50 | Batch: 131 / 301] loss:   1293.479\n",
            "[Epoch 44 / 50 | Batch: 132 / 301] loss:   1143.744\n",
            "[Epoch 44 / 50 | Batch: 133 / 301] loss:    847.648\n",
            "[Epoch 44 / 50 | Batch: 134 / 301] loss:   1109.533\n",
            "[Epoch 44 / 50 | Batch: 135 / 301] loss:   1047.330\n",
            "[Epoch 44 / 50 | Batch: 136 / 301] loss:    966.075\n",
            "[Epoch 44 / 50 | Batch: 137 / 301] loss:    871.768\n",
            "[Epoch 44 / 50 | Batch: 138 / 301] loss:   1414.949\n",
            "[Epoch 44 / 50 | Batch: 139 / 301] loss:   1349.243\n",
            "[Epoch 44 / 50 | Batch: 140 / 301] loss:    900.750\n",
            "[Epoch 44 / 50 | Batch: 141 / 301] loss:   1554.303\n",
            "[Epoch 44 / 50 | Batch: 142 / 301] loss:   1522.934\n",
            "[Epoch 44 / 50 | Batch: 143 / 301] loss:    898.814\n",
            "[Epoch 44 / 50 | Batch: 144 / 301] loss:   1185.508\n",
            "[Epoch 44 / 50 | Batch: 145 / 301] loss:    907.629\n",
            "[Epoch 44 / 50 | Batch: 146 / 301] loss:    959.391\n",
            "[Epoch 44 / 50 | Batch: 147 / 301] loss:   1202.309\n",
            "[Epoch 44 / 50 | Batch: 148 / 301] loss:   1132.700\n",
            "[Epoch 44 / 50 | Batch: 149 / 301] loss:   1298.765\n",
            "[Epoch 44 / 50 | Batch: 150 / 301] loss:   1103.686\n",
            "[Epoch 44 / 50 | Batch: 151 / 301] loss:   1117.921\n",
            "[Epoch 44 / 50 | Batch: 152 / 301] loss:   1112.413\n",
            "[Epoch 44 / 50 | Batch: 153 / 301] loss:    972.577\n",
            "[Epoch 44 / 50 | Batch: 154 / 301] loss:   1216.215\n",
            "[Epoch 44 / 50 | Batch: 155 / 301] loss:    954.050\n",
            "[Epoch 44 / 50 | Batch: 156 / 301] loss:   1090.203\n",
            "[Epoch 44 / 50 | Batch: 157 / 301] loss:   1190.998\n",
            "[Epoch 44 / 50 | Batch: 158 / 301] loss:   1148.669\n",
            "[Epoch 44 / 50 | Batch: 159 / 301] loss:   1343.375\n",
            "[Epoch 44 / 50 | Batch: 160 / 301] loss:   1137.117\n",
            "[Epoch 44 / 50 | Batch: 161 / 301] loss:   1362.726\n",
            "[Epoch 44 / 50 | Batch: 162 / 301] loss:   1064.210\n",
            "[Epoch 44 / 50 | Batch: 163 / 301] loss:   1077.907\n",
            "[Epoch 44 / 50 | Batch: 164 / 301] loss:   1341.978\n",
            "[Epoch 44 / 50 | Batch: 165 / 301] loss:   1244.530\n",
            "[Epoch 44 / 50 | Batch: 166 / 301] loss:   1089.509\n",
            "[Epoch 44 / 50 | Batch: 167 / 301] loss:   1408.268\n",
            "[Epoch 44 / 50 | Batch: 168 / 301] loss:   1184.221\n",
            "[Epoch 44 / 50 | Batch: 169 / 301] loss:   1375.891\n",
            "[Epoch 44 / 50 | Batch: 170 / 301] loss:    914.685\n",
            "[Epoch 44 / 50 | Batch: 171 / 301] loss:   1082.530\n",
            "[Epoch 44 / 50 | Batch: 172 / 301] loss:   1211.983\n",
            "[Epoch 44 / 50 | Batch: 173 / 301] loss:   1074.176\n",
            "[Epoch 44 / 50 | Batch: 174 / 301] loss:   1105.663\n",
            "[Epoch 44 / 50 | Batch: 175 / 301] loss:   1205.050\n",
            "[Epoch 44 / 50 | Batch: 176 / 301] loss:   1236.489\n",
            "[Epoch 44 / 50 | Batch: 177 / 301] loss:   1235.656\n",
            "[Epoch 44 / 50 | Batch: 178 / 301] loss:   1314.724\n",
            "[Epoch 44 / 50 | Batch: 179 / 301] loss:   1141.394\n",
            "[Epoch 44 / 50 | Batch: 180 / 301] loss:   1098.833\n",
            "[Epoch 44 / 50 | Batch: 181 / 301] loss:   1094.952\n",
            "[Epoch 44 / 50 | Batch: 182 / 301] loss:   1406.729\n",
            "[Epoch 44 / 50 | Batch: 183 / 301] loss:   1156.840\n",
            "[Epoch 44 / 50 | Batch: 184 / 301] loss:    932.310\n",
            "[Epoch 44 / 50 | Batch: 185 / 301] loss:   1218.385\n",
            "[Epoch 44 / 50 | Batch: 186 / 301] loss:   1030.122\n",
            "[Epoch 44 / 50 | Batch: 187 / 301] loss:   1033.607\n",
            "[Epoch 44 / 50 | Batch: 188 / 301] loss:   1317.280\n",
            "[Epoch 44 / 50 | Batch: 189 / 301] loss:   1728.189\n",
            "[Epoch 44 / 50 | Batch: 190 / 301] loss:   1123.654\n",
            "[Epoch 44 / 50 | Batch: 191 / 301] loss:   1428.310\n",
            "[Epoch 44 / 50 | Batch: 192 / 301] loss:   1053.065\n",
            "[Epoch 44 / 50 | Batch: 193 / 301] loss:   1245.536\n",
            "[Epoch 44 / 50 | Batch: 194 / 301] loss:   1143.410\n",
            "[Epoch 44 / 50 | Batch: 195 / 301] loss:    882.336\n",
            "[Epoch 44 / 50 | Batch: 196 / 301] loss:   1292.803\n",
            "[Epoch 44 / 50 | Batch: 197 / 301] loss:   1454.372\n",
            "[Epoch 44 / 50 | Batch: 198 / 301] loss:   1210.053\n",
            "[Epoch 44 / 50 | Batch: 199 / 301] loss:   1380.786\n",
            "[Epoch 44 / 50 | Batch: 200 / 301] loss:   1068.529\n",
            "[Epoch 44 / 50 | Batch: 201 / 301] loss:   1479.403\n",
            "[Epoch 44 / 50 | Batch: 202 / 301] loss:   1135.818\n",
            "[Epoch 44 / 50 | Batch: 203 / 301] loss:   1171.953\n",
            "[Epoch 44 / 50 | Batch: 204 / 301] loss:   1230.924\n",
            "[Epoch 44 / 50 | Batch: 205 / 301] loss:    965.101\n",
            "[Epoch 44 / 50 | Batch: 206 / 301] loss:   1594.577\n",
            "[Epoch 44 / 50 | Batch: 207 / 301] loss:   1381.685\n",
            "[Epoch 44 / 50 | Batch: 208 / 301] loss:    986.407\n",
            "[Epoch 44 / 50 | Batch: 209 / 301] loss:    831.643\n",
            "[Epoch 44 / 50 | Batch: 210 / 301] loss:   1219.175\n",
            "[Epoch 44 / 50 | Batch: 211 / 301] loss:   1411.301\n",
            "[Epoch 44 / 50 | Batch: 212 / 301] loss:   1302.237\n",
            "[Epoch 44 / 50 | Batch: 213 / 301] loss:   1239.537\n",
            "[Epoch 44 / 50 | Batch: 214 / 301] loss:    880.680\n",
            "[Epoch 44 / 50 | Batch: 215 / 301] loss:   1131.294\n",
            "[Epoch 44 / 50 | Batch: 216 / 301] loss:    940.387\n",
            "[Epoch 44 / 50 | Batch: 217 / 301] loss:    926.232\n",
            "[Epoch 44 / 50 | Batch: 218 / 301] loss:   1121.960\n",
            "[Epoch 44 / 50 | Batch: 219 / 301] loss:    920.795\n",
            "[Epoch 44 / 50 | Batch: 220 / 301] loss:   1257.669\n",
            "[Epoch 44 / 50 | Batch: 221 / 301] loss:   1194.985\n",
            "[Epoch 44 / 50 | Batch: 222 / 301] loss:   1350.699\n",
            "[Epoch 44 / 50 | Batch: 223 / 301] loss:   1161.129\n",
            "[Epoch 44 / 50 | Batch: 224 / 301] loss:   1005.145\n",
            "[Epoch 44 / 50 | Batch: 225 / 301] loss:   1144.631\n",
            "[Epoch 44 / 50 | Batch: 226 / 301] loss:   1134.000\n",
            "[Epoch 44 / 50 | Batch: 227 / 301] loss:   1026.607\n",
            "[Epoch 44 / 50 | Batch: 228 / 301] loss:   1090.689\n",
            "[Epoch 44 / 50 | Batch: 229 / 301] loss:    956.334\n",
            "[Epoch 44 / 50 | Batch: 230 / 301] loss:   1183.668\n",
            "[Epoch 44 / 50 | Batch: 231 / 301] loss:   1227.248\n",
            "[Epoch 44 / 50 | Batch: 232 / 301] loss:   1133.141\n",
            "[Epoch 44 / 50 | Batch: 233 / 301] loss:   1597.921\n",
            "[Epoch 44 / 50 | Batch: 234 / 301] loss:   1046.311\n",
            "[Epoch 44 / 50 | Batch: 235 / 301] loss:   1494.680\n",
            "[Epoch 44 / 50 | Batch: 236 / 301] loss:    776.487\n",
            "[Epoch 44 / 50 | Batch: 237 / 301] loss:   1570.105\n",
            "[Epoch 44 / 50 | Batch: 238 / 301] loss:   1318.536\n",
            "[Epoch 44 / 50 | Batch: 239 / 301] loss:    975.679\n",
            "[Epoch 44 / 50 | Batch: 240 / 301] loss:   1179.016\n",
            "[Epoch 44 / 50 | Batch: 241 / 301] loss:    867.734\n",
            "[Epoch 44 / 50 | Batch: 242 / 301] loss:    950.307\n",
            "[Epoch 44 / 50 | Batch: 243 / 301] loss:   1486.987\n",
            "[Epoch 44 / 50 | Batch: 244 / 301] loss:   1218.930\n",
            "[Epoch 44 / 50 | Batch: 245 / 301] loss:    939.758\n",
            "[Epoch 44 / 50 | Batch: 246 / 301] loss:   1023.219\n",
            "[Epoch 44 / 50 | Batch: 247 / 301] loss:   1076.705\n",
            "[Epoch 44 / 50 | Batch: 248 / 301] loss:    828.485\n",
            "[Epoch 44 / 50 | Batch: 249 / 301] loss:   1143.349\n",
            "[Epoch 44 / 50 | Batch: 250 / 301] loss:   1176.409\n",
            "[Epoch 44 / 50 | Batch: 251 / 301] loss:   1386.993\n",
            "[Epoch 44 / 50 | Batch: 252 / 301] loss:   1255.243\n",
            "[Epoch 44 / 50 | Batch: 253 / 301] loss:   1224.096\n",
            "[Epoch 44 / 50 | Batch: 254 / 301] loss:    880.301\n",
            "[Epoch 44 / 50 | Batch: 255 / 301] loss:   1094.129\n",
            "[Epoch 44 / 50 | Batch: 256 / 301] loss:   1438.481\n",
            "[Epoch 44 / 50 | Batch: 257 / 301] loss:   1239.656\n",
            "[Epoch 44 / 50 | Batch: 258 / 301] loss:   1331.508\n",
            "[Epoch 44 / 50 | Batch: 259 / 301] loss:   1849.466\n",
            "[Epoch 44 / 50 | Batch: 260 / 301] loss:   1023.597\n",
            "[Epoch 44 / 50 | Batch: 261 / 301] loss:   1339.684\n",
            "[Epoch 44 / 50 | Batch: 262 / 301] loss:   1033.237\n",
            "[Epoch 44 / 50 | Batch: 263 / 301] loss:   1526.219\n",
            "[Epoch 44 / 50 | Batch: 264 / 301] loss:   1218.387\n",
            "[Epoch 44 / 50 | Batch: 265 / 301] loss:   1293.769\n",
            "[Epoch 44 / 50 | Batch: 266 / 301] loss:   1116.501\n",
            "[Epoch 44 / 50 | Batch: 267 / 301] loss:   1512.279\n",
            "[Epoch 44 / 50 | Batch: 268 / 301] loss:   1037.990\n",
            "[Epoch 44 / 50 | Batch: 269 / 301] loss:    932.253\n",
            "[Epoch 44 / 50 | Batch: 270 / 301] loss:   1285.768\n",
            "[Epoch 44 / 50 | Batch: 271 / 301] loss:   1117.549\n",
            "[Epoch 44 / 50 | Batch: 272 / 301] loss:    892.425\n",
            "[Epoch 44 / 50 | Batch: 273 / 301] loss:   1297.569\n",
            "[Epoch 44 / 50 | Batch: 274 / 301] loss:   1138.336\n",
            "[Epoch 44 / 50 | Batch: 275 / 301] loss:   1083.395\n",
            "[Epoch 44 / 50 | Batch: 276 / 301] loss:   1045.513\n",
            "[Epoch 44 / 50 | Batch: 277 / 301] loss:    936.866\n",
            "[Epoch 44 / 50 | Batch: 278 / 301] loss:   1315.078\n",
            "[Epoch 44 / 50 | Batch: 279 / 301] loss:   1143.634\n",
            "[Epoch 44 / 50 | Batch: 280 / 301] loss:   1192.859\n",
            "[Epoch 44 / 50 | Batch: 281 / 301] loss:   1407.708\n",
            "[Epoch 44 / 50 | Batch: 282 / 301] loss:   1193.391\n",
            "[Epoch 44 / 50 | Batch: 283 / 301] loss:    903.069\n",
            "[Epoch 44 / 50 | Batch: 284 / 301] loss:   1292.195\n",
            "[Epoch 44 / 50 | Batch: 285 / 301] loss:    700.946\n",
            "[Epoch 44 / 50 | Batch: 286 / 301] loss:   1042.562\n",
            "[Epoch 44 / 50 | Batch: 287 / 301] loss:    932.019\n",
            "[Epoch 44 / 50 | Batch: 288 / 301] loss:   1029.447\n",
            "[Epoch 44 / 50 | Batch: 289 / 301] loss:   1201.917\n",
            "[Epoch 44 / 50 | Batch: 290 / 301] loss:   1215.230\n",
            "[Epoch 44 / 50 | Batch: 291 / 301] loss:    891.267\n",
            "[Epoch 44 / 50 | Batch: 292 / 301] loss:   1472.747\n",
            "[Epoch 44 / 50 | Batch: 293 / 301] loss:   1185.227\n",
            "[Epoch 44 / 50 | Batch: 294 / 301] loss:   1128.595\n",
            "[Epoch 44 / 50 | Batch: 295 / 301] loss:   1296.020\n",
            "[Epoch 44 / 50 | Batch: 296 / 301] loss:    937.854\n",
            "[Epoch 44 / 50 | Batch: 297 / 301] loss:   1618.551\n",
            "[Epoch 44 / 50 | Batch: 298 / 301] loss:   1084.054\n",
            "[Epoch 44 / 50 | Batch: 299 / 301] loss:   1173.307\n",
            "[Epoch 44 / 50 | Batch: 300 / 301] loss:   1610.780\n",
            "[Epoch 44 / 50 | Batch: 301 / 301] loss:    414.724\n",
            "Epoch loss: 1159.76725\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4270.897\n",
            "[Validation] [Batch  2 / 15] dev loss:   6504.674\n",
            "[Validation] [Batch  3 / 15] dev loss:   6796.883\n",
            "[Validation] [Batch  4 / 15] dev loss:   5082.102\n",
            "[Validation] [Batch  5 / 15] dev loss:   5595.918\n",
            "[Validation] [Batch  6 / 15] dev loss:   9240.216\n",
            "[Validation] [Batch  7 / 15] dev loss:   5995.778\n",
            "[Validation] [Batch  8 / 15] dev loss:   4223.309\n",
            "[Validation] [Batch  9 / 15] dev loss:   3609.835\n",
            "[Validation] [Batch 10 / 15] dev loss:  15766.265\n",
            "[Validation] [Batch 11 / 15] dev loss:  12720.711\n",
            "[Validation] [Batch 12 / 15] dev loss:  11356.217\n",
            "[Validation] [Batch 13 / 15] dev loss:   4565.729\n",
            "[Validation] [Batch 14 / 15] dev loss:  24803.887\n",
            "[Validation] [Batch 15 / 15] dev loss:   2074.692\n",
            "Dev loss 8173.80745\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 45 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 45 / 50 | Batch:  1 / 301] loss:   1112.616\n",
            "[Epoch 45 / 50 | Batch:  2 / 301] loss:   1183.389\n",
            "[Epoch 45 / 50 | Batch:  3 / 301] loss:   1087.650\n",
            "[Epoch 45 / 50 | Batch:  4 / 301] loss:    939.020\n",
            "[Epoch 45 / 50 | Batch:  5 / 301] loss:   1218.712\n",
            "[Epoch 45 / 50 | Batch:  6 / 301] loss:   1413.047\n",
            "[Epoch 45 / 50 | Batch:  7 / 301] loss:    965.427\n",
            "[Epoch 45 / 50 | Batch:  8 / 301] loss:   1502.887\n",
            "[Epoch 45 / 50 | Batch:  9 / 301] loss:   1432.547\n",
            "[Epoch 45 / 50 | Batch: 10 / 301] loss:   1545.645\n",
            "[Epoch 45 / 50 | Batch: 11 / 301] loss:   1081.304\n",
            "[Epoch 45 / 50 | Batch: 12 / 301] loss:   1204.840\n",
            "[Epoch 45 / 50 | Batch: 13 / 301] loss:   1346.856\n",
            "[Epoch 45 / 50 | Batch: 14 / 301] loss:   1238.961\n",
            "[Epoch 45 / 50 | Batch: 15 / 301] loss:    944.541\n",
            "[Epoch 45 / 50 | Batch: 16 / 301] loss:   1022.177\n",
            "[Epoch 45 / 50 | Batch: 17 / 301] loss:   1228.161\n",
            "[Epoch 45 / 50 | Batch: 18 / 301] loss:    918.225\n",
            "[Epoch 45 / 50 | Batch: 19 / 301] loss:    973.515\n",
            "[Epoch 45 / 50 | Batch: 20 / 301] loss:    729.474\n",
            "[Epoch 45 / 50 | Batch: 21 / 301] loss:   1029.657\n",
            "[Epoch 45 / 50 | Batch: 22 / 301] loss:   1103.047\n",
            "[Epoch 45 / 50 | Batch: 23 / 301] loss:    833.360\n",
            "[Epoch 45 / 50 | Batch: 24 / 301] loss:   1048.786\n",
            "[Epoch 45 / 50 | Batch: 25 / 301] loss:   1246.603\n",
            "[Epoch 45 / 50 | Batch: 26 / 301] loss:   1043.252\n",
            "[Epoch 45 / 50 | Batch: 27 / 301] loss:   1617.002\n",
            "[Epoch 45 / 50 | Batch: 28 / 301] loss:   1147.595\n",
            "[Epoch 45 / 50 | Batch: 29 / 301] loss:   1183.211\n",
            "[Epoch 45 / 50 | Batch: 30 / 301] loss:   1465.600\n",
            "[Epoch 45 / 50 | Batch: 31 / 301] loss:   1231.257\n",
            "[Epoch 45 / 50 | Batch: 32 / 301] loss:   1016.142\n",
            "[Epoch 45 / 50 | Batch: 33 / 301] loss:    854.839\n",
            "[Epoch 45 / 50 | Batch: 34 / 301] loss:   1190.779\n",
            "[Epoch 45 / 50 | Batch: 35 / 301] loss:   1285.964\n",
            "[Epoch 45 / 50 | Batch: 36 / 301] loss:   1491.794\n",
            "[Epoch 45 / 50 | Batch: 37 / 301] loss:   1014.148\n",
            "[Epoch 45 / 50 | Batch: 38 / 301] loss:   1250.944\n",
            "[Epoch 45 / 50 | Batch: 39 / 301] loss:   1012.989\n",
            "[Epoch 45 / 50 | Batch: 40 / 301] loss:    976.898\n",
            "[Epoch 45 / 50 | Batch: 41 / 301] loss:    849.241\n",
            "[Epoch 45 / 50 | Batch: 42 / 301] loss:    851.594\n",
            "[Epoch 45 / 50 | Batch: 43 / 301] loss:    934.524\n",
            "[Epoch 45 / 50 | Batch: 44 / 301] loss:   1408.460\n",
            "[Epoch 45 / 50 | Batch: 45 / 301] loss:   1481.330\n",
            "[Epoch 45 / 50 | Batch: 46 / 301] loss:   1187.381\n",
            "[Epoch 45 / 50 | Batch: 47 / 301] loss:   1211.200\n",
            "[Epoch 45 / 50 | Batch: 48 / 301] loss:    931.753\n",
            "[Epoch 45 / 50 | Batch: 49 / 301] loss:   1158.480\n",
            "[Epoch 45 / 50 | Batch: 50 / 301] loss:   1240.549\n",
            "[Epoch 45 / 50 | Batch: 51 / 301] loss:   1336.716\n",
            "[Epoch 45 / 50 | Batch: 52 / 301] loss:   1237.797\n",
            "[Epoch 45 / 50 | Batch: 53 / 301] loss:    953.378\n",
            "[Epoch 45 / 50 | Batch: 54 / 301] loss:   1215.346\n",
            "[Epoch 45 / 50 | Batch: 55 / 301] loss:   1116.500\n",
            "[Epoch 45 / 50 | Batch: 56 / 301] loss:    980.066\n",
            "[Epoch 45 / 50 | Batch: 57 / 301] loss:   1104.489\n",
            "[Epoch 45 / 50 | Batch: 58 / 301] loss:   1313.520\n",
            "[Epoch 45 / 50 | Batch: 59 / 301] loss:   1198.991\n",
            "[Epoch 45 / 50 | Batch: 60 / 301] loss:   1023.127\n",
            "[Epoch 45 / 50 | Batch: 61 / 301] loss:   1046.929\n",
            "[Epoch 45 / 50 | Batch: 62 / 301] loss:   1218.081\n",
            "[Epoch 45 / 50 | Batch: 63 / 301] loss:   1061.631\n",
            "[Epoch 45 / 50 | Batch: 64 / 301] loss:   1268.718\n",
            "[Epoch 45 / 50 | Batch: 65 / 301] loss:   1104.383\n",
            "[Epoch 45 / 50 | Batch: 66 / 301] loss:   1319.388\n",
            "[Epoch 45 / 50 | Batch: 67 / 301] loss:    997.809\n",
            "[Epoch 45 / 50 | Batch: 68 / 301] loss:    933.078\n",
            "[Epoch 45 / 50 | Batch: 69 / 301] loss:   1377.583\n",
            "[Epoch 45 / 50 | Batch: 70 / 301] loss:    895.984\n",
            "[Epoch 45 / 50 | Batch: 71 / 301] loss:   1337.730\n",
            "[Epoch 45 / 50 | Batch: 72 / 301] loss:   1578.124\n",
            "[Epoch 45 / 50 | Batch: 73 / 301] loss:   1277.510\n",
            "[Epoch 45 / 50 | Batch: 74 / 301] loss:    779.055\n",
            "[Epoch 45 / 50 | Batch: 75 / 301] loss:   1021.854\n",
            "[Epoch 45 / 50 | Batch: 76 / 301] loss:   1016.965\n",
            "[Epoch 45 / 50 | Batch: 77 / 301] loss:   1117.242\n",
            "[Epoch 45 / 50 | Batch: 78 / 301] loss:   1188.257\n",
            "[Epoch 45 / 50 | Batch: 79 / 301] loss:    912.665\n",
            "[Epoch 45 / 50 | Batch: 80 / 301] loss:   1123.531\n",
            "[Epoch 45 / 50 | Batch: 81 / 301] loss:   1154.595\n",
            "[Epoch 45 / 50 | Batch: 82 / 301] loss:   1118.103\n",
            "[Epoch 45 / 50 | Batch: 83 / 301] loss:   1151.031\n",
            "[Epoch 45 / 50 | Batch: 84 / 301] loss:    832.926\n",
            "[Epoch 45 / 50 | Batch: 85 / 301] loss:   1162.772\n",
            "[Epoch 45 / 50 | Batch: 86 / 301] loss:    815.763\n",
            "[Epoch 45 / 50 | Batch: 87 / 301] loss:   1063.198\n",
            "[Epoch 45 / 50 | Batch: 88 / 301] loss:   1043.244\n",
            "[Epoch 45 / 50 | Batch: 89 / 301] loss:    926.723\n",
            "[Epoch 45 / 50 | Batch: 90 / 301] loss:   1015.837\n",
            "[Epoch 45 / 50 | Batch: 91 / 301] loss:   1069.997\n",
            "[Epoch 45 / 50 | Batch: 92 / 301] loss:   1202.812\n",
            "[Epoch 45 / 50 | Batch: 93 / 301] loss:    983.309\n",
            "[Epoch 45 / 50 | Batch: 94 / 301] loss:   1264.811\n",
            "[Epoch 45 / 50 | Batch: 95 / 301] loss:   1497.080\n",
            "[Epoch 45 / 50 | Batch: 96 / 301] loss:   1041.576\n",
            "[Epoch 45 / 50 | Batch: 97 / 301] loss:    931.741\n",
            "[Epoch 45 / 50 | Batch: 98 / 301] loss:   1313.409\n",
            "[Epoch 45 / 50 | Batch: 99 / 301] loss:   1172.395\n",
            "[Epoch 45 / 50 | Batch: 100 / 301] loss:   1097.885\n",
            "[Epoch 45 / 50 | Batch: 101 / 301] loss:   1425.301\n",
            "[Epoch 45 / 50 | Batch: 102 / 301] loss:   1024.193\n",
            "[Epoch 45 / 50 | Batch: 103 / 301] loss:   1060.578\n",
            "[Epoch 45 / 50 | Batch: 104 / 301] loss:   1090.715\n",
            "[Epoch 45 / 50 | Batch: 105 / 301] loss:   1297.391\n",
            "[Epoch 45 / 50 | Batch: 106 / 301] loss:   1056.050\n",
            "[Epoch 45 / 50 | Batch: 107 / 301] loss:    987.005\n",
            "[Epoch 45 / 50 | Batch: 108 / 301] loss:   1537.153\n",
            "[Epoch 45 / 50 | Batch: 109 / 301] loss:   1310.067\n",
            "[Epoch 45 / 50 | Batch: 110 / 301] loss:    963.920\n",
            "[Epoch 45 / 50 | Batch: 111 / 301] loss:    880.257\n",
            "[Epoch 45 / 50 | Batch: 112 / 301] loss:   1240.158\n",
            "[Epoch 45 / 50 | Batch: 113 / 301] loss:   1189.811\n",
            "[Epoch 45 / 50 | Batch: 114 / 301] loss:    962.476\n",
            "[Epoch 45 / 50 | Batch: 115 / 301] loss:    936.893\n",
            "[Epoch 45 / 50 | Batch: 116 / 301] loss:   1080.671\n",
            "[Epoch 45 / 50 | Batch: 117 / 301] loss:   1044.232\n",
            "[Epoch 45 / 50 | Batch: 118 / 301] loss:   1171.470\n",
            "[Epoch 45 / 50 | Batch: 119 / 301] loss:   1283.487\n",
            "[Epoch 45 / 50 | Batch: 120 / 301] loss:   1500.225\n",
            "[Epoch 45 / 50 | Batch: 121 / 301] loss:   1209.839\n",
            "[Epoch 45 / 50 | Batch: 122 / 301] loss:   1279.202\n",
            "[Epoch 45 / 50 | Batch: 123 / 301] loss:   1489.878\n",
            "[Epoch 45 / 50 | Batch: 124 / 301] loss:   1071.224\n",
            "[Epoch 45 / 50 | Batch: 125 / 301] loss:   1183.747\n",
            "[Epoch 45 / 50 | Batch: 126 / 301] loss:   1042.176\n",
            "[Epoch 45 / 50 | Batch: 127 / 301] loss:   1452.725\n",
            "[Epoch 45 / 50 | Batch: 128 / 301] loss:   1191.876\n",
            "[Epoch 45 / 50 | Batch: 129 / 301] loss:   1056.734\n",
            "[Epoch 45 / 50 | Batch: 130 / 301] loss:   1017.449\n",
            "[Epoch 45 / 50 | Batch: 131 / 301] loss:    989.539\n",
            "[Epoch 45 / 50 | Batch: 132 / 301] loss:   1250.436\n",
            "[Epoch 45 / 50 | Batch: 133 / 301] loss:   1184.777\n",
            "[Epoch 45 / 50 | Batch: 134 / 301] loss:   1160.373\n",
            "[Epoch 45 / 50 | Batch: 135 / 301] loss:    989.994\n",
            "[Epoch 45 / 50 | Batch: 136 / 301] loss:    957.410\n",
            "[Epoch 45 / 50 | Batch: 137 / 301] loss:    858.777\n",
            "[Epoch 45 / 50 | Batch: 138 / 301] loss:   1316.397\n",
            "[Epoch 45 / 50 | Batch: 139 / 301] loss:   1218.407\n",
            "[Epoch 45 / 50 | Batch: 140 / 301] loss:   1105.808\n",
            "[Epoch 45 / 50 | Batch: 141 / 301] loss:    988.684\n",
            "[Epoch 45 / 50 | Batch: 142 / 301] loss:   1254.226\n",
            "[Epoch 45 / 50 | Batch: 143 / 301] loss:   1017.677\n",
            "[Epoch 45 / 50 | Batch: 144 / 301] loss:   1193.970\n",
            "[Epoch 45 / 50 | Batch: 145 / 301] loss:    871.160\n",
            "[Epoch 45 / 50 | Batch: 146 / 301] loss:   1329.564\n",
            "[Epoch 45 / 50 | Batch: 147 / 301] loss:   1055.499\n",
            "[Epoch 45 / 50 | Batch: 148 / 301] loss:    994.370\n",
            "[Epoch 45 / 50 | Batch: 149 / 301] loss:   1205.469\n",
            "[Epoch 45 / 50 | Batch: 150 / 301] loss:   1376.234\n",
            "[Epoch 45 / 50 | Batch: 151 / 301] loss:    888.498\n",
            "[Epoch 45 / 50 | Batch: 152 / 301] loss:    982.905\n",
            "[Epoch 45 / 50 | Batch: 153 / 301] loss:   1579.062\n",
            "[Epoch 45 / 50 | Batch: 154 / 301] loss:   1535.258\n",
            "[Epoch 45 / 50 | Batch: 155 / 301] loss:   1123.326\n",
            "[Epoch 45 / 50 | Batch: 156 / 301] loss:    797.534\n",
            "[Epoch 45 / 50 | Batch: 157 / 301] loss:   1027.664\n",
            "[Epoch 45 / 50 | Batch: 158 / 301] loss:   1341.684\n",
            "[Epoch 45 / 50 | Batch: 159 / 301] loss:    839.605\n",
            "[Epoch 45 / 50 | Batch: 160 / 301] loss:   1333.720\n",
            "[Epoch 45 / 50 | Batch: 161 / 301] loss:    982.785\n",
            "[Epoch 45 / 50 | Batch: 162 / 301] loss:   1168.974\n",
            "[Epoch 45 / 50 | Batch: 163 / 301] loss:   1529.704\n",
            "[Epoch 45 / 50 | Batch: 164 / 301] loss:   1904.211\n",
            "[Epoch 45 / 50 | Batch: 165 / 301] loss:   1083.753\n",
            "[Epoch 45 / 50 | Batch: 166 / 301] loss:   1262.641\n",
            "[Epoch 45 / 50 | Batch: 167 / 301] loss:   1064.489\n",
            "[Epoch 45 / 50 | Batch: 168 / 301] loss:   1573.403\n",
            "[Epoch 45 / 50 | Batch: 169 / 301] loss:   1345.440\n",
            "[Epoch 45 / 50 | Batch: 170 / 301] loss:    855.159\n",
            "[Epoch 45 / 50 | Batch: 171 / 301] loss:   1038.373\n",
            "[Epoch 45 / 50 | Batch: 172 / 301] loss:   1300.109\n",
            "[Epoch 45 / 50 | Batch: 173 / 301] loss:   1203.564\n",
            "[Epoch 45 / 50 | Batch: 174 / 301] loss:   1168.144\n",
            "[Epoch 45 / 50 | Batch: 175 / 301] loss:   1188.557\n",
            "[Epoch 45 / 50 | Batch: 176 / 301] loss:   1279.892\n",
            "[Epoch 45 / 50 | Batch: 177 / 301] loss:   1082.906\n",
            "[Epoch 45 / 50 | Batch: 178 / 301] loss:   1686.827\n",
            "[Epoch 45 / 50 | Batch: 179 / 301] loss:    905.894\n",
            "[Epoch 45 / 50 | Batch: 180 / 301] loss:   1452.174\n",
            "[Epoch 45 / 50 | Batch: 181 / 301] loss:   1312.994\n",
            "[Epoch 45 / 50 | Batch: 182 / 301] loss:   1015.544\n",
            "[Epoch 45 / 50 | Batch: 183 / 301] loss:   1457.064\n",
            "[Epoch 45 / 50 | Batch: 184 / 301] loss:   1092.312\n",
            "[Epoch 45 / 50 | Batch: 185 / 301] loss:    924.558\n",
            "[Epoch 45 / 50 | Batch: 186 / 301] loss:   1107.964\n",
            "[Epoch 45 / 50 | Batch: 187 / 301] loss:   1290.186\n",
            "[Epoch 45 / 50 | Batch: 188 / 301] loss:    875.662\n",
            "[Epoch 45 / 50 | Batch: 189 / 301] loss:    759.479\n",
            "[Epoch 45 / 50 | Batch: 190 / 301] loss:    765.944\n",
            "[Epoch 45 / 50 | Batch: 191 / 301] loss:   1058.301\n",
            "[Epoch 45 / 50 | Batch: 192 / 301] loss:   1094.829\n",
            "[Epoch 45 / 50 | Batch: 193 / 301] loss:   1023.640\n",
            "[Epoch 45 / 50 | Batch: 194 / 301] loss:   1137.617\n",
            "[Epoch 45 / 50 | Batch: 195 / 301] loss:   1065.445\n",
            "[Epoch 45 / 50 | Batch: 196 / 301] loss:    845.926\n",
            "[Epoch 45 / 50 | Batch: 197 / 301] loss:   1026.671\n",
            "[Epoch 45 / 50 | Batch: 198 / 301] loss:    923.400\n",
            "[Epoch 45 / 50 | Batch: 199 / 301] loss:   1394.686\n",
            "[Epoch 45 / 50 | Batch: 200 / 301] loss:   1453.974\n",
            "[Epoch 45 / 50 | Batch: 201 / 301] loss:   1006.533\n",
            "[Epoch 45 / 50 | Batch: 202 / 301] loss:   1127.112\n",
            "[Epoch 45 / 50 | Batch: 203 / 301] loss:   1026.343\n",
            "[Epoch 45 / 50 | Batch: 204 / 301] loss:    878.859\n",
            "[Epoch 45 / 50 | Batch: 205 / 301] loss:   1453.907\n",
            "[Epoch 45 / 50 | Batch: 206 / 301] loss:    865.503\n",
            "[Epoch 45 / 50 | Batch: 207 / 301] loss:   1076.990\n",
            "[Epoch 45 / 50 | Batch: 208 / 301] loss:   1152.599\n",
            "[Epoch 45 / 50 | Batch: 209 / 301] loss:   1121.893\n",
            "[Epoch 45 / 50 | Batch: 210 / 301] loss:   1040.046\n",
            "[Epoch 45 / 50 | Batch: 211 / 301] loss:   1032.257\n",
            "[Epoch 45 / 50 | Batch: 212 / 301] loss:   1126.879\n",
            "[Epoch 45 / 50 | Batch: 213 / 301] loss:   1395.680\n",
            "[Epoch 45 / 50 | Batch: 214 / 301] loss:   1186.784\n",
            "[Epoch 45 / 50 | Batch: 215 / 301] loss:   1000.027\n",
            "[Epoch 45 / 50 | Batch: 216 / 301] loss:    937.257\n",
            "[Epoch 45 / 50 | Batch: 217 / 301] loss:    814.670\n",
            "[Epoch 45 / 50 | Batch: 218 / 301] loss:   1040.108\n",
            "[Epoch 45 / 50 | Batch: 219 / 301] loss:   1354.312\n",
            "[Epoch 45 / 50 | Batch: 220 / 301] loss:   1094.181\n",
            "[Epoch 45 / 50 | Batch: 221 / 301] loss:    987.263\n",
            "[Epoch 45 / 50 | Batch: 222 / 301] loss:    927.121\n",
            "[Epoch 45 / 50 | Batch: 223 / 301] loss:   1533.261\n",
            "[Epoch 45 / 50 | Batch: 224 / 301] loss:   1351.205\n",
            "[Epoch 45 / 50 | Batch: 225 / 301] loss:   1464.671\n",
            "[Epoch 45 / 50 | Batch: 226 / 301] loss:   1269.114\n",
            "[Epoch 45 / 50 | Batch: 227 / 301] loss:   1219.448\n",
            "[Epoch 45 / 50 | Batch: 228 / 301] loss:   1194.398\n",
            "[Epoch 45 / 50 | Batch: 229 / 301] loss:   1268.937\n",
            "[Epoch 45 / 50 | Batch: 230 / 301] loss:    976.145\n",
            "[Epoch 45 / 50 | Batch: 231 / 301] loss:   1032.864\n",
            "[Epoch 45 / 50 | Batch: 232 / 301] loss:   1118.676\n",
            "[Epoch 45 / 50 | Batch: 233 / 301] loss:   1137.106\n",
            "[Epoch 45 / 50 | Batch: 234 / 301] loss:   1084.877\n",
            "[Epoch 45 / 50 | Batch: 235 / 301] loss:    909.873\n",
            "[Epoch 45 / 50 | Batch: 236 / 301] loss:   1597.264\n",
            "[Epoch 45 / 50 | Batch: 237 / 301] loss:   1301.337\n",
            "[Epoch 45 / 50 | Batch: 238 / 301] loss:   1215.941\n",
            "[Epoch 45 / 50 | Batch: 239 / 301] loss:   1253.298\n",
            "[Epoch 45 / 50 | Batch: 240 / 301] loss:    874.157\n",
            "[Epoch 45 / 50 | Batch: 241 / 301] loss:   1010.555\n",
            "[Epoch 45 / 50 | Batch: 242 / 301] loss:   1249.335\n",
            "[Epoch 45 / 50 | Batch: 243 / 301] loss:   1231.993\n",
            "[Epoch 45 / 50 | Batch: 244 / 301] loss:   1366.214\n",
            "[Epoch 45 / 50 | Batch: 245 / 301] loss:   1132.560\n",
            "[Epoch 45 / 50 | Batch: 246 / 301] loss:   1361.775\n",
            "[Epoch 45 / 50 | Batch: 247 / 301] loss:   1065.629\n",
            "[Epoch 45 / 50 | Batch: 248 / 301] loss:   1151.391\n",
            "[Epoch 45 / 50 | Batch: 249 / 301] loss:   1116.779\n",
            "[Epoch 45 / 50 | Batch: 250 / 301] loss:   1349.815\n",
            "[Epoch 45 / 50 | Batch: 251 / 301] loss:    992.793\n",
            "[Epoch 45 / 50 | Batch: 252 / 301] loss:   1482.287\n",
            "[Epoch 45 / 50 | Batch: 253 / 301] loss:   1438.287\n",
            "[Epoch 45 / 50 | Batch: 254 / 301] loss:   1540.952\n",
            "[Epoch 45 / 50 | Batch: 255 / 301] loss:   1314.598\n",
            "[Epoch 45 / 50 | Batch: 256 / 301] loss:   1493.019\n",
            "[Epoch 45 / 50 | Batch: 257 / 301] loss:   1043.171\n",
            "[Epoch 45 / 50 | Batch: 258 / 301] loss:   1495.140\n",
            "[Epoch 45 / 50 | Batch: 259 / 301] loss:   1110.432\n",
            "[Epoch 45 / 50 | Batch: 260 / 301] loss:   1279.011\n",
            "[Epoch 45 / 50 | Batch: 261 / 301] loss:   1388.770\n",
            "[Epoch 45 / 50 | Batch: 262 / 301] loss:    987.180\n",
            "[Epoch 45 / 50 | Batch: 263 / 301] loss:   1148.427\n",
            "[Epoch 45 / 50 | Batch: 264 / 301] loss:    925.563\n",
            "[Epoch 45 / 50 | Batch: 265 / 301] loss:   1125.270\n",
            "[Epoch 45 / 50 | Batch: 266 / 301] loss:   1291.690\n",
            "[Epoch 45 / 50 | Batch: 267 / 301] loss:   1555.230\n",
            "[Epoch 45 / 50 | Batch: 268 / 301] loss:   1091.169\n",
            "[Epoch 45 / 50 | Batch: 269 / 301] loss:    973.232\n",
            "[Epoch 45 / 50 | Batch: 270 / 301] loss:   1173.932\n",
            "[Epoch 45 / 50 | Batch: 271 / 301] loss:   1346.224\n",
            "[Epoch 45 / 50 | Batch: 272 / 301] loss:   1153.292\n",
            "[Epoch 45 / 50 | Batch: 273 / 301] loss:   1248.236\n",
            "[Epoch 45 / 50 | Batch: 274 / 301] loss:   1483.012\n",
            "[Epoch 45 / 50 | Batch: 275 / 301] loss:   1174.647\n",
            "[Epoch 45 / 50 | Batch: 276 / 301] loss:   1163.856\n",
            "[Epoch 45 / 50 | Batch: 277 / 301] loss:   1393.701\n",
            "[Epoch 45 / 50 | Batch: 278 / 301] loss:   1139.648\n",
            "[Epoch 45 / 50 | Batch: 279 / 301] loss:   1279.026\n",
            "[Epoch 45 / 50 | Batch: 280 / 301] loss:   1163.646\n",
            "[Epoch 45 / 50 | Batch: 281 / 301] loss:   1107.752\n",
            "[Epoch 45 / 50 | Batch: 282 / 301] loss:   1193.265\n",
            "[Epoch 45 / 50 | Batch: 283 / 301] loss:    947.138\n",
            "[Epoch 45 / 50 | Batch: 284 / 301] loss:   1235.184\n",
            "[Epoch 45 / 50 | Batch: 285 / 301] loss:   1115.843\n",
            "[Epoch 45 / 50 | Batch: 286 / 301] loss:    990.688\n",
            "[Epoch 45 / 50 | Batch: 287 / 301] loss:   1848.653\n",
            "[Epoch 45 / 50 | Batch: 288 / 301] loss:   1099.164\n",
            "[Epoch 45 / 50 | Batch: 289 / 301] loss:   1252.274\n",
            "[Epoch 45 / 50 | Batch: 290 / 301] loss:   1220.120\n",
            "[Epoch 45 / 50 | Batch: 291 / 301] loss:   1905.094\n",
            "[Epoch 45 / 50 | Batch: 292 / 301] loss:   1503.905\n",
            "[Epoch 45 / 50 | Batch: 293 / 301] loss:   1262.354\n",
            "[Epoch 45 / 50 | Batch: 294 / 301] loss:   1434.575\n",
            "[Epoch 45 / 50 | Batch: 295 / 301] loss:   1132.036\n",
            "[Epoch 45 / 50 | Batch: 296 / 301] loss:   1192.101\n",
            "[Epoch 45 / 50 | Batch: 297 / 301] loss:    976.878\n",
            "[Epoch 45 / 50 | Batch: 298 / 301] loss:   1109.945\n",
            "[Epoch 45 / 50 | Batch: 299 / 301] loss:   1139.661\n",
            "[Epoch 45 / 50 | Batch: 300 / 301] loss:    943.512\n",
            "[Epoch 45 / 50 | Batch: 301 / 301] loss:    642.784\n",
            "Epoch loss: 1159.28392\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4285.943\n",
            "[Validation] [Batch  2 / 15] dev loss:   6058.318\n",
            "[Validation] [Batch  3 / 15] dev loss:   6699.837\n",
            "[Validation] [Batch  4 / 15] dev loss:   5440.120\n",
            "[Validation] [Batch  5 / 15] dev loss:   5388.854\n",
            "[Validation] [Batch  6 / 15] dev loss:   9487.990\n",
            "[Validation] [Batch  7 / 15] dev loss:   6503.253\n",
            "[Validation] [Batch  8 / 15] dev loss:   4275.619\n",
            "[Validation] [Batch  9 / 15] dev loss:   3577.436\n",
            "[Validation] [Batch 10 / 15] dev loss:  16085.164\n",
            "[Validation] [Batch 11 / 15] dev loss:  12826.966\n",
            "[Validation] [Batch 12 / 15] dev loss:  10663.657\n",
            "[Validation] [Batch 13 / 15] dev loss:   4410.665\n",
            "[Validation] [Batch 14 / 15] dev loss:  23627.854\n",
            "[Validation] [Batch 15 / 15] dev loss:   2163.843\n",
            "Dev loss 8099.70122\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 46 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 46 / 50 | Batch:  1 / 301] loss:   1071.546\n",
            "[Epoch 46 / 50 | Batch:  2 / 301] loss:   1478.894\n",
            "[Epoch 46 / 50 | Batch:  3 / 301] loss:    943.346\n",
            "[Epoch 46 / 50 | Batch:  4 / 301] loss:   1064.605\n",
            "[Epoch 46 / 50 | Batch:  5 / 301] loss:    866.508\n",
            "[Epoch 46 / 50 | Batch:  6 / 301] loss:   1451.998\n",
            "[Epoch 46 / 50 | Batch:  7 / 301] loss:   1092.226\n",
            "[Epoch 46 / 50 | Batch:  8 / 301] loss:   1034.315\n",
            "[Epoch 46 / 50 | Batch:  9 / 301] loss:   1010.894\n",
            "[Epoch 46 / 50 | Batch: 10 / 301] loss:   1151.653\n",
            "[Epoch 46 / 50 | Batch: 11 / 301] loss:   1152.070\n",
            "[Epoch 46 / 50 | Batch: 12 / 301] loss:   1160.783\n",
            "[Epoch 46 / 50 | Batch: 13 / 301] loss:   1014.792\n",
            "[Epoch 46 / 50 | Batch: 14 / 301] loss:   1354.942\n",
            "[Epoch 46 / 50 | Batch: 15 / 301] loss:   1415.738\n",
            "[Epoch 46 / 50 | Batch: 16 / 301] loss:    797.688\n",
            "[Epoch 46 / 50 | Batch: 17 / 301] loss:   1437.771\n",
            "[Epoch 46 / 50 | Batch: 18 / 301] loss:   1108.902\n",
            "[Epoch 46 / 50 | Batch: 19 / 301] loss:    782.132\n",
            "[Epoch 46 / 50 | Batch: 20 / 301] loss:   1586.088\n",
            "[Epoch 46 / 50 | Batch: 21 / 301] loss:    998.617\n",
            "[Epoch 46 / 50 | Batch: 22 / 301] loss:   1314.422\n",
            "[Epoch 46 / 50 | Batch: 23 / 301] loss:   1238.192\n",
            "[Epoch 46 / 50 | Batch: 24 / 301] loss:    973.922\n",
            "[Epoch 46 / 50 | Batch: 25 / 301] loss:   1205.091\n",
            "[Epoch 46 / 50 | Batch: 26 / 301] loss:    824.322\n",
            "[Epoch 46 / 50 | Batch: 27 / 301] loss:   1197.016\n",
            "[Epoch 46 / 50 | Batch: 28 / 301] loss:   1183.494\n",
            "[Epoch 46 / 50 | Batch: 29 / 301] loss:    979.451\n",
            "[Epoch 46 / 50 | Batch: 30 / 301] loss:   1243.919\n",
            "[Epoch 46 / 50 | Batch: 31 / 301] loss:    728.761\n",
            "[Epoch 46 / 50 | Batch: 32 / 301] loss:    790.440\n",
            "[Epoch 46 / 50 | Batch: 33 / 301] loss:   1371.651\n",
            "[Epoch 46 / 50 | Batch: 34 / 301] loss:   1429.439\n",
            "[Epoch 46 / 50 | Batch: 35 / 301] loss:   1033.159\n",
            "[Epoch 46 / 50 | Batch: 36 / 301] loss:   1138.491\n",
            "[Epoch 46 / 50 | Batch: 37 / 301] loss:   1048.171\n",
            "[Epoch 46 / 50 | Batch: 38 / 301] loss:    973.817\n",
            "[Epoch 46 / 50 | Batch: 39 / 301] loss:    747.932\n",
            "[Epoch 46 / 50 | Batch: 40 / 301] loss:   1737.129\n",
            "[Epoch 46 / 50 | Batch: 41 / 301] loss:   1261.367\n",
            "[Epoch 46 / 50 | Batch: 42 / 301] loss:   1205.654\n",
            "[Epoch 46 / 50 | Batch: 43 / 301] loss:   1030.511\n",
            "[Epoch 46 / 50 | Batch: 44 / 301] loss:   1237.801\n",
            "[Epoch 46 / 50 | Batch: 45 / 301] loss:   1053.830\n",
            "[Epoch 46 / 50 | Batch: 46 / 301] loss:   1087.128\n",
            "[Epoch 46 / 50 | Batch: 47 / 301] loss:    787.104\n",
            "[Epoch 46 / 50 | Batch: 48 / 301] loss:   1198.042\n",
            "[Epoch 46 / 50 | Batch: 49 / 301] loss:   1141.105\n",
            "[Epoch 46 / 50 | Batch: 50 / 301] loss:    835.930\n",
            "[Epoch 46 / 50 | Batch: 51 / 301] loss:   1199.055\n",
            "[Epoch 46 / 50 | Batch: 52 / 301] loss:    865.138\n",
            "[Epoch 46 / 50 | Batch: 53 / 301] loss:    891.767\n",
            "[Epoch 46 / 50 | Batch: 54 / 301] loss:   1238.590\n",
            "[Epoch 46 / 50 | Batch: 55 / 301] loss:   1216.440\n",
            "[Epoch 46 / 50 | Batch: 56 / 301] loss:   1376.948\n",
            "[Epoch 46 / 50 | Batch: 57 / 301] loss:   1240.545\n",
            "[Epoch 46 / 50 | Batch: 58 / 301] loss:   1181.006\n",
            "[Epoch 46 / 50 | Batch: 59 / 301] loss:   1797.530\n",
            "[Epoch 46 / 50 | Batch: 60 / 301] loss:   1205.912\n",
            "[Epoch 46 / 50 | Batch: 61 / 301] loss:   1014.275\n",
            "[Epoch 46 / 50 | Batch: 62 / 301] loss:    889.072\n",
            "[Epoch 46 / 50 | Batch: 63 / 301] loss:   1099.809\n",
            "[Epoch 46 / 50 | Batch: 64 / 301] loss:   1004.824\n",
            "[Epoch 46 / 50 | Batch: 65 / 301] loss:   1143.218\n",
            "[Epoch 46 / 50 | Batch: 66 / 301] loss:    903.474\n",
            "[Epoch 46 / 50 | Batch: 67 / 301] loss:   1152.548\n",
            "[Epoch 46 / 50 | Batch: 68 / 301] loss:   1263.842\n",
            "[Epoch 46 / 50 | Batch: 69 / 301] loss:    909.468\n",
            "[Epoch 46 / 50 | Batch: 70 / 301] loss:    969.511\n",
            "[Epoch 46 / 50 | Batch: 71 / 301] loss:   1138.965\n",
            "[Epoch 46 / 50 | Batch: 72 / 301] loss:   1284.404\n",
            "[Epoch 46 / 50 | Batch: 73 / 301] loss:   1376.744\n",
            "[Epoch 46 / 50 | Batch: 74 / 301] loss:    938.596\n",
            "[Epoch 46 / 50 | Batch: 75 / 301] loss:    887.927\n",
            "[Epoch 46 / 50 | Batch: 76 / 301] loss:    784.233\n",
            "[Epoch 46 / 50 | Batch: 77 / 301] loss:   1271.850\n",
            "[Epoch 46 / 50 | Batch: 78 / 301] loss:    936.968\n",
            "[Epoch 46 / 50 | Batch: 79 / 301] loss:   1149.475\n",
            "[Epoch 46 / 50 | Batch: 80 / 301] loss:   1240.456\n",
            "[Epoch 46 / 50 | Batch: 81 / 301] loss:   1040.104\n",
            "[Epoch 46 / 50 | Batch: 82 / 301] loss:   1383.765\n",
            "[Epoch 46 / 50 | Batch: 83 / 301] loss:   1284.397\n",
            "[Epoch 46 / 50 | Batch: 84 / 301] loss:   1022.849\n",
            "[Epoch 46 / 50 | Batch: 85 / 301] loss:    877.953\n",
            "[Epoch 46 / 50 | Batch: 86 / 301] loss:   1052.974\n",
            "[Epoch 46 / 50 | Batch: 87 / 301] loss:   1088.709\n",
            "[Epoch 46 / 50 | Batch: 88 / 301] loss:    828.270\n",
            "[Epoch 46 / 50 | Batch: 89 / 301] loss:   1209.217\n",
            "[Epoch 46 / 50 | Batch: 90 / 301] loss:   1032.702\n",
            "[Epoch 46 / 50 | Batch: 91 / 301] loss:   1122.054\n",
            "[Epoch 46 / 50 | Batch: 92 / 301] loss:   1111.264\n",
            "[Epoch 46 / 50 | Batch: 93 / 301] loss:   1334.087\n",
            "[Epoch 46 / 50 | Batch: 94 / 301] loss:    905.501\n",
            "[Epoch 46 / 50 | Batch: 95 / 301] loss:    870.337\n",
            "[Epoch 46 / 50 | Batch: 96 / 301] loss:   1270.677\n",
            "[Epoch 46 / 50 | Batch: 97 / 301] loss:   1412.459\n",
            "[Epoch 46 / 50 | Batch: 98 / 301] loss:   1355.675\n",
            "[Epoch 46 / 50 | Batch: 99 / 301] loss:   1069.717\n",
            "[Epoch 46 / 50 | Batch: 100 / 301] loss:   1411.312\n",
            "[Epoch 46 / 50 | Batch: 101 / 301] loss:   1137.687\n",
            "[Epoch 46 / 50 | Batch: 102 / 301] loss:   1114.193\n",
            "[Epoch 46 / 50 | Batch: 103 / 301] loss:   1390.352\n",
            "[Epoch 46 / 50 | Batch: 104 / 301] loss:   1295.259\n",
            "[Epoch 46 / 50 | Batch: 105 / 301] loss:   1140.312\n",
            "[Epoch 46 / 50 | Batch: 106 / 301] loss:   1208.025\n",
            "[Epoch 46 / 50 | Batch: 107 / 301] loss:   1171.906\n",
            "[Epoch 46 / 50 | Batch: 108 / 301] loss:   1231.381\n",
            "[Epoch 46 / 50 | Batch: 109 / 301] loss:   1055.590\n",
            "[Epoch 46 / 50 | Batch: 110 / 301] loss:   1175.149\n",
            "[Epoch 46 / 50 | Batch: 111 / 301] loss:   1242.195\n",
            "[Epoch 46 / 50 | Batch: 112 / 301] loss:   1320.771\n",
            "[Epoch 46 / 50 | Batch: 113 / 301] loss:   1296.647\n",
            "[Epoch 46 / 50 | Batch: 114 / 301] loss:   1063.232\n",
            "[Epoch 46 / 50 | Batch: 115 / 301] loss:   1186.481\n",
            "[Epoch 46 / 50 | Batch: 116 / 301] loss:   1028.924\n",
            "[Epoch 46 / 50 | Batch: 117 / 301] loss:   1646.368\n",
            "[Epoch 46 / 50 | Batch: 118 / 301] loss:    969.421\n",
            "[Epoch 46 / 50 | Batch: 119 / 301] loss:   1729.661\n",
            "[Epoch 46 / 50 | Batch: 120 / 301] loss:   1593.733\n",
            "[Epoch 46 / 50 | Batch: 121 / 301] loss:   1291.917\n",
            "[Epoch 46 / 50 | Batch: 122 / 301] loss:    916.478\n",
            "[Epoch 46 / 50 | Batch: 123 / 301] loss:    951.553\n",
            "[Epoch 46 / 50 | Batch: 124 / 301] loss:    765.591\n",
            "[Epoch 46 / 50 | Batch: 125 / 301] loss:    910.435\n",
            "[Epoch 46 / 50 | Batch: 126 / 301] loss:   1243.777\n",
            "[Epoch 46 / 50 | Batch: 127 / 301] loss:    987.421\n",
            "[Epoch 46 / 50 | Batch: 128 / 301] loss:   1133.530\n",
            "[Epoch 46 / 50 | Batch: 129 / 301] loss:   1166.831\n",
            "[Epoch 46 / 50 | Batch: 130 / 301] loss:   1218.151\n",
            "[Epoch 46 / 50 | Batch: 131 / 301] loss:   1245.058\n",
            "[Epoch 46 / 50 | Batch: 132 / 301] loss:   1136.405\n",
            "[Epoch 46 / 50 | Batch: 133 / 301] loss:   1034.414\n",
            "[Epoch 46 / 50 | Batch: 134 / 301] loss:   1089.560\n",
            "[Epoch 46 / 50 | Batch: 135 / 301] loss:   1364.632\n",
            "[Epoch 46 / 50 | Batch: 136 / 301] loss:    841.414\n",
            "[Epoch 46 / 50 | Batch: 137 / 301] loss:   1429.581\n",
            "[Epoch 46 / 50 | Batch: 138 / 301] loss:   1070.463\n",
            "[Epoch 46 / 50 | Batch: 139 / 301] loss:   1146.841\n",
            "[Epoch 46 / 50 | Batch: 140 / 301] loss:   1109.912\n",
            "[Epoch 46 / 50 | Batch: 141 / 301] loss:    996.846\n",
            "[Epoch 46 / 50 | Batch: 142 / 301] loss:   1420.858\n",
            "[Epoch 46 / 50 | Batch: 143 / 301] loss:   1098.474\n",
            "[Epoch 46 / 50 | Batch: 144 / 301] loss:   1280.377\n",
            "[Epoch 46 / 50 | Batch: 145 / 301] loss:   1254.621\n",
            "[Epoch 46 / 50 | Batch: 146 / 301] loss:   1178.516\n",
            "[Epoch 46 / 50 | Batch: 147 / 301] loss:   1043.707\n",
            "[Epoch 46 / 50 | Batch: 148 / 301] loss:   1322.275\n",
            "[Epoch 46 / 50 | Batch: 149 / 301] loss:    947.042\n",
            "[Epoch 46 / 50 | Batch: 150 / 301] loss:    846.005\n",
            "[Epoch 46 / 50 | Batch: 151 / 301] loss:    982.284\n",
            "[Epoch 46 / 50 | Batch: 152 / 301] loss:   1350.901\n",
            "[Epoch 46 / 50 | Batch: 153 / 301] loss:    967.847\n",
            "[Epoch 46 / 50 | Batch: 154 / 301] loss:   1012.705\n",
            "[Epoch 46 / 50 | Batch: 155 / 301] loss:   1203.613\n",
            "[Epoch 46 / 50 | Batch: 156 / 301] loss:   1220.031\n",
            "[Epoch 46 / 50 | Batch: 157 / 301] loss:   1352.559\n",
            "[Epoch 46 / 50 | Batch: 158 / 301] loss:   1209.276\n",
            "[Epoch 46 / 50 | Batch: 159 / 301] loss:   1254.691\n",
            "[Epoch 46 / 50 | Batch: 160 / 301] loss:   1196.211\n",
            "[Epoch 46 / 50 | Batch: 161 / 301] loss:   1124.885\n",
            "[Epoch 46 / 50 | Batch: 162 / 301] loss:   1358.982\n",
            "[Epoch 46 / 50 | Batch: 163 / 301] loss:   1207.654\n",
            "[Epoch 46 / 50 | Batch: 164 / 301] loss:    974.390\n",
            "[Epoch 46 / 50 | Batch: 165 / 301] loss:   1252.671\n",
            "[Epoch 46 / 50 | Batch: 166 / 301] loss:   1028.992\n",
            "[Epoch 46 / 50 | Batch: 167 / 301] loss:   1171.230\n",
            "[Epoch 46 / 50 | Batch: 168 / 301] loss:   1148.840\n",
            "[Epoch 46 / 50 | Batch: 169 / 301] loss:    893.990\n",
            "[Epoch 46 / 50 | Batch: 170 / 301] loss:   1229.058\n",
            "[Epoch 46 / 50 | Batch: 171 / 301] loss:   1121.402\n",
            "[Epoch 46 / 50 | Batch: 172 / 301] loss:   1440.444\n",
            "[Epoch 46 / 50 | Batch: 173 / 301] loss:    977.226\n",
            "[Epoch 46 / 50 | Batch: 174 / 301] loss:   1437.479\n",
            "[Epoch 46 / 50 | Batch: 175 / 301] loss:    930.107\n",
            "[Epoch 46 / 50 | Batch: 176 / 301] loss:   1172.731\n",
            "[Epoch 46 / 50 | Batch: 177 / 301] loss:   1022.160\n",
            "[Epoch 46 / 50 | Batch: 178 / 301] loss:   1419.474\n",
            "[Epoch 46 / 50 | Batch: 179 / 301] loss:   1362.441\n",
            "[Epoch 46 / 50 | Batch: 180 / 301] loss:   1302.508\n",
            "[Epoch 46 / 50 | Batch: 181 / 301] loss:   1135.087\n",
            "[Epoch 46 / 50 | Batch: 182 / 301] loss:   1061.859\n",
            "[Epoch 46 / 50 | Batch: 183 / 301] loss:   1208.833\n",
            "[Epoch 46 / 50 | Batch: 184 / 301] loss:   1082.642\n",
            "[Epoch 46 / 50 | Batch: 185 / 301] loss:   1345.750\n",
            "[Epoch 46 / 50 | Batch: 186 / 301] loss:   1424.662\n",
            "[Epoch 46 / 50 | Batch: 187 / 301] loss:   1425.971\n",
            "[Epoch 46 / 50 | Batch: 188 / 301] loss:   1226.448\n",
            "[Epoch 46 / 50 | Batch: 189 / 301] loss:   1003.015\n",
            "[Epoch 46 / 50 | Batch: 190 / 301] loss:   1145.349\n",
            "[Epoch 46 / 50 | Batch: 191 / 301] loss:   1138.783\n",
            "[Epoch 46 / 50 | Batch: 192 / 301] loss:   1211.331\n",
            "[Epoch 46 / 50 | Batch: 193 / 301] loss:    963.434\n",
            "[Epoch 46 / 50 | Batch: 194 / 301] loss:   1377.391\n",
            "[Epoch 46 / 50 | Batch: 195 / 301] loss:    928.992\n",
            "[Epoch 46 / 50 | Batch: 196 / 301] loss:   1278.539\n",
            "[Epoch 46 / 50 | Batch: 197 / 301] loss:   1063.671\n",
            "[Epoch 46 / 50 | Batch: 198 / 301] loss:   1256.369\n",
            "[Epoch 46 / 50 | Batch: 199 / 301] loss:   1459.157\n",
            "[Epoch 46 / 50 | Batch: 200 / 301] loss:   1207.486\n",
            "[Epoch 46 / 50 | Batch: 201 / 301] loss:   1363.773\n",
            "[Epoch 46 / 50 | Batch: 202 / 301] loss:   1060.976\n",
            "[Epoch 46 / 50 | Batch: 203 / 301] loss:   1231.857\n",
            "[Epoch 46 / 50 | Batch: 204 / 301] loss:   1156.997\n",
            "[Epoch 46 / 50 | Batch: 205 / 301] loss:   1498.352\n",
            "[Epoch 46 / 50 | Batch: 206 / 301] loss:    867.678\n",
            "[Epoch 46 / 50 | Batch: 207 / 301] loss:    980.315\n",
            "[Epoch 46 / 50 | Batch: 208 / 301] loss:   1044.640\n",
            "[Epoch 46 / 50 | Batch: 209 / 301] loss:   1184.567\n",
            "[Epoch 46 / 50 | Batch: 210 / 301] loss:    963.678\n",
            "[Epoch 46 / 50 | Batch: 211 / 301] loss:   1236.742\n",
            "[Epoch 46 / 50 | Batch: 212 / 301] loss:   1161.147\n",
            "[Epoch 46 / 50 | Batch: 213 / 301] loss:   1545.726\n",
            "[Epoch 46 / 50 | Batch: 214 / 301] loss:   1481.098\n",
            "[Epoch 46 / 50 | Batch: 215 / 301] loss:   1394.245\n",
            "[Epoch 46 / 50 | Batch: 216 / 301] loss:   1620.451\n",
            "[Epoch 46 / 50 | Batch: 217 / 301] loss:    990.571\n",
            "[Epoch 46 / 50 | Batch: 218 / 301] loss:   1563.448\n",
            "[Epoch 46 / 50 | Batch: 219 / 301] loss:    839.906\n",
            "[Epoch 46 / 50 | Batch: 220 / 301] loss:   1080.501\n",
            "[Epoch 46 / 50 | Batch: 221 / 301] loss:   1204.668\n",
            "[Epoch 46 / 50 | Batch: 222 / 301] loss:   1260.494\n",
            "[Epoch 46 / 50 | Batch: 223 / 301] loss:   1026.755\n",
            "[Epoch 46 / 50 | Batch: 224 / 301] loss:   1471.498\n",
            "[Epoch 46 / 50 | Batch: 225 / 301] loss:    926.865\n",
            "[Epoch 46 / 50 | Batch: 226 / 301] loss:   1163.685\n",
            "[Epoch 46 / 50 | Batch: 227 / 301] loss:    853.704\n",
            "[Epoch 46 / 50 | Batch: 228 / 301] loss:   1210.941\n",
            "[Epoch 46 / 50 | Batch: 229 / 301] loss:   1267.368\n",
            "[Epoch 46 / 50 | Batch: 230 / 301] loss:   1001.229\n",
            "[Epoch 46 / 50 | Batch: 231 / 301] loss:    791.073\n",
            "[Epoch 46 / 50 | Batch: 232 / 301] loss:   1482.175\n",
            "[Epoch 46 / 50 | Batch: 233 / 301] loss:   1005.884\n",
            "[Epoch 46 / 50 | Batch: 234 / 301] loss:   1016.714\n",
            "[Epoch 46 / 50 | Batch: 235 / 301] loss:   1392.280\n",
            "[Epoch 46 / 50 | Batch: 236 / 301] loss:   1408.897\n",
            "[Epoch 46 / 50 | Batch: 237 / 301] loss:   1216.049\n",
            "[Epoch 46 / 50 | Batch: 238 / 301] loss:    856.563\n",
            "[Epoch 46 / 50 | Batch: 239 / 301] loss:    991.933\n",
            "[Epoch 46 / 50 | Batch: 240 / 301] loss:   1261.416\n",
            "[Epoch 46 / 50 | Batch: 241 / 301] loss:   1128.028\n",
            "[Epoch 46 / 50 | Batch: 242 / 301] loss:   1085.735\n",
            "[Epoch 46 / 50 | Batch: 243 / 301] loss:   1203.743\n",
            "[Epoch 46 / 50 | Batch: 244 / 301] loss:   1578.908\n",
            "[Epoch 46 / 50 | Batch: 245 / 301] loss:   1192.048\n",
            "[Epoch 46 / 50 | Batch: 246 / 301] loss:   1065.102\n",
            "[Epoch 46 / 50 | Batch: 247 / 301] loss:   1094.164\n",
            "[Epoch 46 / 50 | Batch: 248 / 301] loss:    821.559\n",
            "[Epoch 46 / 50 | Batch: 249 / 301] loss:   1284.130\n",
            "[Epoch 46 / 50 | Batch: 250 / 301] loss:   1469.706\n",
            "[Epoch 46 / 50 | Batch: 251 / 301] loss:   1056.588\n",
            "[Epoch 46 / 50 | Batch: 252 / 301] loss:   1363.783\n",
            "[Epoch 46 / 50 | Batch: 253 / 301] loss:   1352.396\n",
            "[Epoch 46 / 50 | Batch: 254 / 301] loss:   1197.486\n",
            "[Epoch 46 / 50 | Batch: 255 / 301] loss:   1197.306\n",
            "[Epoch 46 / 50 | Batch: 256 / 301] loss:   1112.214\n",
            "[Epoch 46 / 50 | Batch: 257 / 301] loss:    955.388\n",
            "[Epoch 46 / 50 | Batch: 258 / 301] loss:   1515.501\n",
            "[Epoch 46 / 50 | Batch: 259 / 301] loss:   1202.367\n",
            "[Epoch 46 / 50 | Batch: 260 / 301] loss:   1033.510\n",
            "[Epoch 46 / 50 | Batch: 261 / 301] loss:   1006.350\n",
            "[Epoch 46 / 50 | Batch: 262 / 301] loss:   1032.348\n",
            "[Epoch 46 / 50 | Batch: 263 / 301] loss:    987.138\n",
            "[Epoch 46 / 50 | Batch: 264 / 301] loss:   1007.927\n",
            "[Epoch 46 / 50 | Batch: 265 / 301] loss:   1090.626\n",
            "[Epoch 46 / 50 | Batch: 266 / 301] loss:   1249.125\n",
            "[Epoch 46 / 50 | Batch: 267 / 301] loss:   1410.763\n",
            "[Epoch 46 / 50 | Batch: 268 / 301] loss:   1245.008\n",
            "[Epoch 46 / 50 | Batch: 269 / 301] loss:    906.956\n",
            "[Epoch 46 / 50 | Batch: 270 / 301] loss:   1474.847\n",
            "[Epoch 46 / 50 | Batch: 271 / 301] loss:   1488.624\n",
            "[Epoch 46 / 50 | Batch: 272 / 301] loss:   1127.920\n",
            "[Epoch 46 / 50 | Batch: 273 / 301] loss:    950.936\n",
            "[Epoch 46 / 50 | Batch: 274 / 301] loss:    870.343\n",
            "[Epoch 46 / 50 | Batch: 275 / 301] loss:   1055.038\n",
            "[Epoch 46 / 50 | Batch: 276 / 301] loss:   1926.862\n",
            "[Epoch 46 / 50 | Batch: 277 / 301] loss:   1096.313\n",
            "[Epoch 46 / 50 | Batch: 278 / 301] loss:   1227.965\n",
            "[Epoch 46 / 50 | Batch: 279 / 301] loss:    849.301\n",
            "[Epoch 46 / 50 | Batch: 280 / 301] loss:   1086.535\n",
            "[Epoch 46 / 50 | Batch: 281 / 301] loss:   1523.021\n",
            "[Epoch 46 / 50 | Batch: 282 / 301] loss:   1464.018\n",
            "[Epoch 46 / 50 | Batch: 283 / 301] loss:   1171.366\n",
            "[Epoch 46 / 50 | Batch: 284 / 301] loss:   1204.386\n",
            "[Epoch 46 / 50 | Batch: 285 / 301] loss:   1319.099\n",
            "[Epoch 46 / 50 | Batch: 286 / 301] loss:    998.824\n",
            "[Epoch 46 / 50 | Batch: 287 / 301] loss:    988.318\n",
            "[Epoch 46 / 50 | Batch: 288 / 301] loss:   1036.446\n",
            "[Epoch 46 / 50 | Batch: 289 / 301] loss:   1229.657\n",
            "[Epoch 46 / 50 | Batch: 290 / 301] loss:   1478.391\n",
            "[Epoch 46 / 50 | Batch: 291 / 301] loss:   1132.253\n",
            "[Epoch 46 / 50 | Batch: 292 / 301] loss:   1270.464\n",
            "[Epoch 46 / 50 | Batch: 293 / 301] loss:   1106.229\n",
            "[Epoch 46 / 50 | Batch: 294 / 301] loss:    986.653\n",
            "[Epoch 46 / 50 | Batch: 295 / 301] loss:   1079.286\n",
            "[Epoch 46 / 50 | Batch: 296 / 301] loss:    970.915\n",
            "[Epoch 46 / 50 | Batch: 297 / 301] loss:    929.600\n",
            "[Epoch 46 / 50 | Batch: 298 / 301] loss:   1993.817\n",
            "[Epoch 46 / 50 | Batch: 299 / 301] loss:   1238.772\n",
            "[Epoch 46 / 50 | Batch: 300 / 301] loss:   1214.680\n",
            "[Epoch 46 / 50 | Batch: 301 / 301] loss:    590.393\n",
            "Epoch loss: 1159.28561\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4260.037\n",
            "[Validation] [Batch  2 / 15] dev loss:   6227.633\n",
            "[Validation] [Batch  3 / 15] dev loss:   7013.326\n",
            "[Validation] [Batch  4 / 15] dev loss:   5476.439\n",
            "[Validation] [Batch  5 / 15] dev loss:   5643.678\n",
            "[Validation] [Batch  6 / 15] dev loss:   9070.849\n",
            "[Validation] [Batch  7 / 15] dev loss:   6145.790\n",
            "[Validation] [Batch  8 / 15] dev loss:   4240.611\n",
            "[Validation] [Batch  9 / 15] dev loss:   3860.177\n",
            "[Validation] [Batch 10 / 15] dev loss:  16314.939\n",
            "[Validation] [Batch 11 / 15] dev loss:  12959.045\n",
            "[Validation] [Batch 12 / 15] dev loss:  10668.422\n",
            "[Validation] [Batch 13 / 15] dev loss:   4396.229\n",
            "[Validation] [Batch 14 / 15] dev loss:  22826.043\n",
            "[Validation] [Batch 15 / 15] dev loss:   2060.199\n",
            "Dev loss 8077.56113\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 47 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 47 / 50 | Batch:  1 / 301] loss:   1132.707\n",
            "[Epoch 47 / 50 | Batch:  2 / 301] loss:   1104.443\n",
            "[Epoch 47 / 50 | Batch:  3 / 301] loss:   1355.377\n",
            "[Epoch 47 / 50 | Batch:  4 / 301] loss:   1297.787\n",
            "[Epoch 47 / 50 | Batch:  5 / 301] loss:   1194.099\n",
            "[Epoch 47 / 50 | Batch:  6 / 301] loss:    896.021\n",
            "[Epoch 47 / 50 | Batch:  7 / 301] loss:    858.441\n",
            "[Epoch 47 / 50 | Batch:  8 / 301] loss:   1045.858\n",
            "[Epoch 47 / 50 | Batch:  9 / 301] loss:   1310.497\n",
            "[Epoch 47 / 50 | Batch: 10 / 301] loss:    937.265\n",
            "[Epoch 47 / 50 | Batch: 11 / 301] loss:   1093.305\n",
            "[Epoch 47 / 50 | Batch: 12 / 301] loss:    976.077\n",
            "[Epoch 47 / 50 | Batch: 13 / 301] loss:   1032.370\n",
            "[Epoch 47 / 50 | Batch: 14 / 301] loss:   1231.094\n",
            "[Epoch 47 / 50 | Batch: 15 / 301] loss:    994.104\n",
            "[Epoch 47 / 50 | Batch: 16 / 301] loss:   1144.341\n",
            "[Epoch 47 / 50 | Batch: 17 / 301] loss:   1223.216\n",
            "[Epoch 47 / 50 | Batch: 18 / 301] loss:   1347.765\n",
            "[Epoch 47 / 50 | Batch: 19 / 301] loss:   1093.702\n",
            "[Epoch 47 / 50 | Batch: 20 / 301] loss:   1176.622\n",
            "[Epoch 47 / 50 | Batch: 21 / 301] loss:   1022.599\n",
            "[Epoch 47 / 50 | Batch: 22 / 301] loss:   1040.662\n",
            "[Epoch 47 / 50 | Batch: 23 / 301] loss:   1344.822\n",
            "[Epoch 47 / 50 | Batch: 24 / 301] loss:   1228.240\n",
            "[Epoch 47 / 50 | Batch: 25 / 301] loss:   1096.389\n",
            "[Epoch 47 / 50 | Batch: 26 / 301] loss:   1181.378\n",
            "[Epoch 47 / 50 | Batch: 27 / 301] loss:    998.144\n",
            "[Epoch 47 / 50 | Batch: 28 / 301] loss:   1565.211\n",
            "[Epoch 47 / 50 | Batch: 29 / 301] loss:    866.265\n",
            "[Epoch 47 / 50 | Batch: 30 / 301] loss:   1203.533\n",
            "[Epoch 47 / 50 | Batch: 31 / 301] loss:   1093.775\n",
            "[Epoch 47 / 50 | Batch: 32 / 301] loss:   1140.099\n",
            "[Epoch 47 / 50 | Batch: 33 / 301] loss:   1158.133\n",
            "[Epoch 47 / 50 | Batch: 34 / 301] loss:   1052.006\n",
            "[Epoch 47 / 50 | Batch: 35 / 301] loss:   1315.892\n",
            "[Epoch 47 / 50 | Batch: 36 / 301] loss:    874.352\n",
            "[Epoch 47 / 50 | Batch: 37 / 301] loss:   1021.063\n",
            "[Epoch 47 / 50 | Batch: 38 / 301] loss:   1014.704\n",
            "[Epoch 47 / 50 | Batch: 39 / 301] loss:   1046.800\n",
            "[Epoch 47 / 50 | Batch: 40 / 301] loss:   1048.505\n",
            "[Epoch 47 / 50 | Batch: 41 / 301] loss:   1514.210\n",
            "[Epoch 47 / 50 | Batch: 42 / 301] loss:   1128.117\n",
            "[Epoch 47 / 50 | Batch: 43 / 301] loss:    816.857\n",
            "[Epoch 47 / 50 | Batch: 44 / 301] loss:   1603.060\n",
            "[Epoch 47 / 50 | Batch: 45 / 301] loss:   1545.037\n",
            "[Epoch 47 / 50 | Batch: 46 / 301] loss:   1210.950\n",
            "[Epoch 47 / 50 | Batch: 47 / 301] loss:   1220.130\n",
            "[Epoch 47 / 50 | Batch: 48 / 301] loss:   1110.188\n",
            "[Epoch 47 / 50 | Batch: 49 / 301] loss:   1282.810\n",
            "[Epoch 47 / 50 | Batch: 50 / 301] loss:   1390.166\n",
            "[Epoch 47 / 50 | Batch: 51 / 301] loss:    894.163\n",
            "[Epoch 47 / 50 | Batch: 52 / 301] loss:   1584.042\n",
            "[Epoch 47 / 50 | Batch: 53 / 301] loss:   1001.748\n",
            "[Epoch 47 / 50 | Batch: 54 / 301] loss:    920.985\n",
            "[Epoch 47 / 50 | Batch: 55 / 301] loss:   1129.276\n",
            "[Epoch 47 / 50 | Batch: 56 / 301] loss:   1182.762\n",
            "[Epoch 47 / 50 | Batch: 57 / 301] loss:   1742.618\n",
            "[Epoch 47 / 50 | Batch: 58 / 301] loss:   1205.619\n",
            "[Epoch 47 / 50 | Batch: 59 / 301] loss:   1276.961\n",
            "[Epoch 47 / 50 | Batch: 60 / 301] loss:   1079.654\n",
            "[Epoch 47 / 50 | Batch: 61 / 301] loss:   1077.820\n",
            "[Epoch 47 / 50 | Batch: 62 / 301] loss:   1015.115\n",
            "[Epoch 47 / 50 | Batch: 63 / 301] loss:   1459.849\n",
            "[Epoch 47 / 50 | Batch: 64 / 301] loss:   1215.730\n",
            "[Epoch 47 / 50 | Batch: 65 / 301] loss:   1313.825\n",
            "[Epoch 47 / 50 | Batch: 66 / 301] loss:   1094.119\n",
            "[Epoch 47 / 50 | Batch: 67 / 301] loss:   1039.490\n",
            "[Epoch 47 / 50 | Batch: 68 / 301] loss:   1192.766\n",
            "[Epoch 47 / 50 | Batch: 69 / 301] loss:    834.335\n",
            "[Epoch 47 / 50 | Batch: 70 / 301] loss:   1208.460\n",
            "[Epoch 47 / 50 | Batch: 71 / 301] loss:   1300.521\n",
            "[Epoch 47 / 50 | Batch: 72 / 301] loss:   1042.329\n",
            "[Epoch 47 / 50 | Batch: 73 / 301] loss:    970.893\n",
            "[Epoch 47 / 50 | Batch: 74 / 301] loss:   1458.567\n",
            "[Epoch 47 / 50 | Batch: 75 / 301] loss:   1142.915\n",
            "[Epoch 47 / 50 | Batch: 76 / 301] loss:   1387.406\n",
            "[Epoch 47 / 50 | Batch: 77 / 301] loss:    893.121\n",
            "[Epoch 47 / 50 | Batch: 78 / 301] loss:   1187.208\n",
            "[Epoch 47 / 50 | Batch: 79 / 301] loss:   1122.988\n",
            "[Epoch 47 / 50 | Batch: 80 / 301] loss:   1097.306\n",
            "[Epoch 47 / 50 | Batch: 81 / 301] loss:    861.630\n",
            "[Epoch 47 / 50 | Batch: 82 / 301] loss:   1218.203\n",
            "[Epoch 47 / 50 | Batch: 83 / 301] loss:   1159.700\n",
            "[Epoch 47 / 50 | Batch: 84 / 301] loss:   1678.664\n",
            "[Epoch 47 / 50 | Batch: 85 / 301] loss:   1711.132\n",
            "[Epoch 47 / 50 | Batch: 86 / 301] loss:   1503.922\n",
            "[Epoch 47 / 50 | Batch: 87 / 301] loss:   1115.171\n",
            "[Epoch 47 / 50 | Batch: 88 / 301] loss:   1556.185\n",
            "[Epoch 47 / 50 | Batch: 89 / 301] loss:    951.645\n",
            "[Epoch 47 / 50 | Batch: 90 / 301] loss:   1275.348\n",
            "[Epoch 47 / 50 | Batch: 91 / 301] loss:    890.560\n",
            "[Epoch 47 / 50 | Batch: 92 / 301] loss:    906.202\n",
            "[Epoch 47 / 50 | Batch: 93 / 301] loss:    905.497\n",
            "[Epoch 47 / 50 | Batch: 94 / 301] loss:   1076.216\n",
            "[Epoch 47 / 50 | Batch: 95 / 301] loss:    811.122\n",
            "[Epoch 47 / 50 | Batch: 96 / 301] loss:   1127.178\n",
            "[Epoch 47 / 50 | Batch: 97 / 301] loss:   1135.514\n",
            "[Epoch 47 / 50 | Batch: 98 / 301] loss:   1177.837\n",
            "[Epoch 47 / 50 | Batch: 99 / 301] loss:    785.473\n",
            "[Epoch 47 / 50 | Batch: 100 / 301] loss:   1570.426\n",
            "[Epoch 47 / 50 | Batch: 101 / 301] loss:   1552.144\n",
            "[Epoch 47 / 50 | Batch: 102 / 301] loss:   1199.535\n",
            "[Epoch 47 / 50 | Batch: 103 / 301] loss:    967.867\n",
            "[Epoch 47 / 50 | Batch: 104 / 301] loss:   1072.047\n",
            "[Epoch 47 / 50 | Batch: 105 / 301] loss:   1041.856\n",
            "[Epoch 47 / 50 | Batch: 106 / 301] loss:   1129.470\n",
            "[Epoch 47 / 50 | Batch: 107 / 301] loss:   1064.646\n",
            "[Epoch 47 / 50 | Batch: 108 / 301] loss:   1390.114\n",
            "[Epoch 47 / 50 | Batch: 109 / 301] loss:   1186.247\n",
            "[Epoch 47 / 50 | Batch: 110 / 301] loss:   1321.194\n",
            "[Epoch 47 / 50 | Batch: 111 / 301] loss:   1153.371\n",
            "[Epoch 47 / 50 | Batch: 112 / 301] loss:    912.049\n",
            "[Epoch 47 / 50 | Batch: 113 / 301] loss:   1459.381\n",
            "[Epoch 47 / 50 | Batch: 114 / 301] loss:   1461.785\n",
            "[Epoch 47 / 50 | Batch: 115 / 301] loss:   1159.724\n",
            "[Epoch 47 / 50 | Batch: 116 / 301] loss:    831.543\n",
            "[Epoch 47 / 50 | Batch: 117 / 301] loss:   1055.113\n",
            "[Epoch 47 / 50 | Batch: 118 / 301] loss:   1530.514\n",
            "[Epoch 47 / 50 | Batch: 119 / 301] loss:   1015.049\n",
            "[Epoch 47 / 50 | Batch: 120 / 301] loss:    941.077\n",
            "[Epoch 47 / 50 | Batch: 121 / 301] loss:   1291.645\n",
            "[Epoch 47 / 50 | Batch: 122 / 301] loss:   1125.564\n",
            "[Epoch 47 / 50 | Batch: 123 / 301] loss:    845.963\n",
            "[Epoch 47 / 50 | Batch: 124 / 301] loss:    999.900\n",
            "[Epoch 47 / 50 | Batch: 125 / 301] loss:   1160.884\n",
            "[Epoch 47 / 50 | Batch: 126 / 301] loss:   1014.959\n",
            "[Epoch 47 / 50 | Batch: 127 / 301] loss:   1127.292\n",
            "[Epoch 47 / 50 | Batch: 128 / 301] loss:   1300.368\n",
            "[Epoch 47 / 50 | Batch: 129 / 301] loss:   1072.730\n",
            "[Epoch 47 / 50 | Batch: 130 / 301] loss:   1277.597\n",
            "[Epoch 47 / 50 | Batch: 131 / 301] loss:    850.117\n",
            "[Epoch 47 / 50 | Batch: 132 / 301] loss:   1510.521\n",
            "[Epoch 47 / 50 | Batch: 133 / 301] loss:   1146.430\n",
            "[Epoch 47 / 50 | Batch: 134 / 301] loss:    935.008\n",
            "[Epoch 47 / 50 | Batch: 135 / 301] loss:   1087.897\n",
            "[Epoch 47 / 50 | Batch: 136 / 301] loss:   1190.190\n",
            "[Epoch 47 / 50 | Batch: 137 / 301] loss:   1120.718\n",
            "[Epoch 47 / 50 | Batch: 138 / 301] loss:    993.436\n",
            "[Epoch 47 / 50 | Batch: 139 / 301] loss:   1038.725\n",
            "[Epoch 47 / 50 | Batch: 140 / 301] loss:   1270.069\n",
            "[Epoch 47 / 50 | Batch: 141 / 301] loss:   1618.457\n",
            "[Epoch 47 / 50 | Batch: 142 / 301] loss:   1152.897\n",
            "[Epoch 47 / 50 | Batch: 143 / 301] loss:   1008.643\n",
            "[Epoch 47 / 50 | Batch: 144 / 301] loss:   1337.578\n",
            "[Epoch 47 / 50 | Batch: 145 / 301] loss:   1258.038\n",
            "[Epoch 47 / 50 | Batch: 146 / 301] loss:    834.201\n",
            "[Epoch 47 / 50 | Batch: 147 / 301] loss:   1232.798\n",
            "[Epoch 47 / 50 | Batch: 148 / 301] loss:   1000.921\n",
            "[Epoch 47 / 50 | Batch: 149 / 301] loss:   1324.251\n",
            "[Epoch 47 / 50 | Batch: 150 / 301] loss:   1264.955\n",
            "[Epoch 47 / 50 | Batch: 151 / 301] loss:   1134.885\n",
            "[Epoch 47 / 50 | Batch: 152 / 301] loss:   1224.019\n",
            "[Epoch 47 / 50 | Batch: 153 / 301] loss:   1056.751\n",
            "[Epoch 47 / 50 | Batch: 154 / 301] loss:   1109.879\n",
            "[Epoch 47 / 50 | Batch: 155 / 301] loss:   1267.172\n",
            "[Epoch 47 / 50 | Batch: 156 / 301] loss:   1165.733\n",
            "[Epoch 47 / 50 | Batch: 157 / 301] loss:   1057.307\n",
            "[Epoch 47 / 50 | Batch: 158 / 301] loss:    777.958\n",
            "[Epoch 47 / 50 | Batch: 159 / 301] loss:    925.689\n",
            "[Epoch 47 / 50 | Batch: 160 / 301] loss:   1373.684\n",
            "[Epoch 47 / 50 | Batch: 161 / 301] loss:   1090.577\n",
            "[Epoch 47 / 50 | Batch: 162 / 301] loss:   1067.663\n",
            "[Epoch 47 / 50 | Batch: 163 / 301] loss:   1439.894\n",
            "[Epoch 47 / 50 | Batch: 164 / 301] loss:    890.109\n",
            "[Epoch 47 / 50 | Batch: 165 / 301] loss:    970.893\n",
            "[Epoch 47 / 50 | Batch: 166 / 301] loss:   1015.421\n",
            "[Epoch 47 / 50 | Batch: 167 / 301] loss:   1221.872\n",
            "[Epoch 47 / 50 | Batch: 168 / 301] loss:   1316.769\n",
            "[Epoch 47 / 50 | Batch: 169 / 301] loss:   1349.830\n",
            "[Epoch 47 / 50 | Batch: 170 / 301] loss:   1178.049\n",
            "[Epoch 47 / 50 | Batch: 171 / 301] loss:   1192.794\n",
            "[Epoch 47 / 50 | Batch: 172 / 301] loss:   1282.101\n",
            "[Epoch 47 / 50 | Batch: 173 / 301] loss:   1256.184\n",
            "[Epoch 47 / 50 | Batch: 174 / 301] loss:   1141.568\n",
            "[Epoch 47 / 50 | Batch: 175 / 301] loss:   1302.413\n",
            "[Epoch 47 / 50 | Batch: 176 / 301] loss:   1119.568\n",
            "[Epoch 47 / 50 | Batch: 177 / 301] loss:    908.065\n",
            "[Epoch 47 / 50 | Batch: 178 / 301] loss:   1707.786\n",
            "[Epoch 47 / 50 | Batch: 179 / 301] loss:   1162.852\n",
            "[Epoch 47 / 50 | Batch: 180 / 301] loss:   1195.027\n",
            "[Epoch 47 / 50 | Batch: 181 / 301] loss:   1093.006\n",
            "[Epoch 47 / 50 | Batch: 182 / 301] loss:   1146.812\n",
            "[Epoch 47 / 50 | Batch: 183 / 301] loss:    947.071\n",
            "[Epoch 47 / 50 | Batch: 184 / 301] loss:   1202.552\n",
            "[Epoch 47 / 50 | Batch: 185 / 301] loss:    980.335\n",
            "[Epoch 47 / 50 | Batch: 186 / 301] loss:   1456.479\n",
            "[Epoch 47 / 50 | Batch: 187 / 301] loss:   1494.550\n",
            "[Epoch 47 / 50 | Batch: 188 / 301] loss:   1069.235\n",
            "[Epoch 47 / 50 | Batch: 189 / 301] loss:   1268.451\n",
            "[Epoch 47 / 50 | Batch: 190 / 301] loss:   1675.481\n",
            "[Epoch 47 / 50 | Batch: 191 / 301] loss:   1037.137\n",
            "[Epoch 47 / 50 | Batch: 192 / 301] loss:   1326.261\n",
            "[Epoch 47 / 50 | Batch: 193 / 301] loss:   1570.566\n",
            "[Epoch 47 / 50 | Batch: 194 / 301] loss:   1240.375\n",
            "[Epoch 47 / 50 | Batch: 195 / 301] loss:   1002.128\n",
            "[Epoch 47 / 50 | Batch: 196 / 301] loss:   1151.094\n",
            "[Epoch 47 / 50 | Batch: 197 / 301] loss:   1507.761\n",
            "[Epoch 47 / 50 | Batch: 198 / 301] loss:   1179.119\n",
            "[Epoch 47 / 50 | Batch: 199 / 301] loss:   1578.099\n",
            "[Epoch 47 / 50 | Batch: 200 / 301] loss:   1533.551\n",
            "[Epoch 47 / 50 | Batch: 201 / 301] loss:    923.993\n",
            "[Epoch 47 / 50 | Batch: 202 / 301] loss:   1316.579\n",
            "[Epoch 47 / 50 | Batch: 203 / 301] loss:   1181.668\n",
            "[Epoch 47 / 50 | Batch: 204 / 301] loss:   1110.959\n",
            "[Epoch 47 / 50 | Batch: 205 / 301] loss:   1224.548\n",
            "[Epoch 47 / 50 | Batch: 206 / 301] loss:    956.533\n",
            "[Epoch 47 / 50 | Batch: 207 / 301] loss:    880.391\n",
            "[Epoch 47 / 50 | Batch: 208 / 301] loss:   1135.186\n",
            "[Epoch 47 / 50 | Batch: 209 / 301] loss:   1430.297\n",
            "[Epoch 47 / 50 | Batch: 210 / 301] loss:    750.360\n",
            "[Epoch 47 / 50 | Batch: 211 / 301] loss:   1072.765\n",
            "[Epoch 47 / 50 | Batch: 212 / 301] loss:   1202.211\n",
            "[Epoch 47 / 50 | Batch: 213 / 301] loss:   1366.192\n",
            "[Epoch 47 / 50 | Batch: 214 / 301] loss:   1532.227\n",
            "[Epoch 47 / 50 | Batch: 215 / 301] loss:   1559.901\n",
            "[Epoch 47 / 50 | Batch: 216 / 301] loss:   1654.539\n",
            "[Epoch 47 / 50 | Batch: 217 / 301] loss:   1423.068\n",
            "[Epoch 47 / 50 | Batch: 218 / 301] loss:   1033.376\n",
            "[Epoch 47 / 50 | Batch: 219 / 301] loss:    915.163\n",
            "[Epoch 47 / 50 | Batch: 220 / 301] loss:   1068.278\n",
            "[Epoch 47 / 50 | Batch: 221 / 301] loss:   1073.510\n",
            "[Epoch 47 / 50 | Batch: 222 / 301] loss:   1092.162\n",
            "[Epoch 47 / 50 | Batch: 223 / 301] loss:    953.556\n",
            "[Epoch 47 / 50 | Batch: 224 / 301] loss:    947.706\n",
            "[Epoch 47 / 50 | Batch: 225 / 301] loss:   1489.006\n",
            "[Epoch 47 / 50 | Batch: 226 / 301] loss:   1451.372\n",
            "[Epoch 47 / 50 | Batch: 227 / 301] loss:   1396.288\n",
            "[Epoch 47 / 50 | Batch: 228 / 301] loss:   1204.082\n",
            "[Epoch 47 / 50 | Batch: 229 / 301] loss:   1048.405\n",
            "[Epoch 47 / 50 | Batch: 230 / 301] loss:   1063.568\n",
            "[Epoch 47 / 50 | Batch: 231 / 301] loss:    937.517\n",
            "[Epoch 47 / 50 | Batch: 232 / 301] loss:   1074.270\n",
            "[Epoch 47 / 50 | Batch: 233 / 301] loss:    882.203\n",
            "[Epoch 47 / 50 | Batch: 234 / 301] loss:   1068.761\n",
            "[Epoch 47 / 50 | Batch: 235 / 301] loss:   1661.339\n",
            "[Epoch 47 / 50 | Batch: 236 / 301] loss:   1187.184\n",
            "[Epoch 47 / 50 | Batch: 237 / 301] loss:    981.824\n",
            "[Epoch 47 / 50 | Batch: 238 / 301] loss:   1124.676\n",
            "[Epoch 47 / 50 | Batch: 239 / 301] loss:    952.113\n",
            "[Epoch 47 / 50 | Batch: 240 / 301] loss:   1175.767\n",
            "[Epoch 47 / 50 | Batch: 241 / 301] loss:   1238.391\n",
            "[Epoch 47 / 50 | Batch: 242 / 301] loss:    872.383\n",
            "[Epoch 47 / 50 | Batch: 243 / 301] loss:   1115.374\n",
            "[Epoch 47 / 50 | Batch: 244 / 301] loss:   1410.584\n",
            "[Epoch 47 / 50 | Batch: 245 / 301] loss:   1208.131\n",
            "[Epoch 47 / 50 | Batch: 246 / 301] loss:   1167.030\n",
            "[Epoch 47 / 50 | Batch: 247 / 301] loss:   1113.594\n",
            "[Epoch 47 / 50 | Batch: 248 / 301] loss:   1262.940\n",
            "[Epoch 47 / 50 | Batch: 249 / 301] loss:   1176.757\n",
            "[Epoch 47 / 50 | Batch: 250 / 301] loss:    962.792\n",
            "[Epoch 47 / 50 | Batch: 251 / 301] loss:    885.043\n",
            "[Epoch 47 / 50 | Batch: 252 / 301] loss:   1102.271\n",
            "[Epoch 47 / 50 | Batch: 253 / 301] loss:   1083.457\n",
            "[Epoch 47 / 50 | Batch: 254 / 301] loss:   1164.477\n",
            "[Epoch 47 / 50 | Batch: 255 / 301] loss:    936.182\n",
            "[Epoch 47 / 50 | Batch: 256 / 301] loss:    854.531\n",
            "[Epoch 47 / 50 | Batch: 257 / 301] loss:    987.274\n",
            "[Epoch 47 / 50 | Batch: 258 / 301] loss:   1007.526\n",
            "[Epoch 47 / 50 | Batch: 259 / 301] loss:   1276.792\n",
            "[Epoch 47 / 50 | Batch: 260 / 301] loss:   1487.353\n",
            "[Epoch 47 / 50 | Batch: 261 / 301] loss:   1777.141\n",
            "[Epoch 47 / 50 | Batch: 262 / 301] loss:   1495.323\n",
            "[Epoch 47 / 50 | Batch: 263 / 301] loss:    966.374\n",
            "[Epoch 47 / 50 | Batch: 264 / 301] loss:   1147.420\n",
            "[Epoch 47 / 50 | Batch: 265 / 301] loss:   1312.286\n",
            "[Epoch 47 / 50 | Batch: 266 / 301] loss:   1099.378\n",
            "[Epoch 47 / 50 | Batch: 267 / 301] loss:   1230.581\n",
            "[Epoch 47 / 50 | Batch: 268 / 301] loss:    963.134\n",
            "[Epoch 47 / 50 | Batch: 269 / 301] loss:   1328.671\n",
            "[Epoch 47 / 50 | Batch: 270 / 301] loss:   1048.445\n",
            "[Epoch 47 / 50 | Batch: 271 / 301] loss:    999.223\n",
            "[Epoch 47 / 50 | Batch: 272 / 301] loss:   1154.251\n",
            "[Epoch 47 / 50 | Batch: 273 / 301] loss:   1070.343\n",
            "[Epoch 47 / 50 | Batch: 274 / 301] loss:   1212.145\n",
            "[Epoch 47 / 50 | Batch: 275 / 301] loss:    910.351\n",
            "[Epoch 47 / 50 | Batch: 276 / 301] loss:   1148.634\n",
            "[Epoch 47 / 50 | Batch: 277 / 301] loss:   1004.598\n",
            "[Epoch 47 / 50 | Batch: 278 / 301] loss:   1451.498\n",
            "[Epoch 47 / 50 | Batch: 279 / 301] loss:   1053.270\n",
            "[Epoch 47 / 50 | Batch: 280 / 301] loss:   1014.234\n",
            "[Epoch 47 / 50 | Batch: 281 / 301] loss:   1157.009\n",
            "[Epoch 47 / 50 | Batch: 282 / 301] loss:   1008.470\n",
            "[Epoch 47 / 50 | Batch: 283 / 301] loss:   1179.043\n",
            "[Epoch 47 / 50 | Batch: 284 / 301] loss:   1051.151\n",
            "[Epoch 47 / 50 | Batch: 285 / 301] loss:   1054.697\n",
            "[Epoch 47 / 50 | Batch: 286 / 301] loss:    882.791\n",
            "[Epoch 47 / 50 | Batch: 287 / 301] loss:   1277.031\n",
            "[Epoch 47 / 50 | Batch: 288 / 301] loss:   1048.842\n",
            "[Epoch 47 / 50 | Batch: 289 / 301] loss:   1052.700\n",
            "[Epoch 47 / 50 | Batch: 290 / 301] loss:   1060.199\n",
            "[Epoch 47 / 50 | Batch: 291 / 301] loss:   1243.669\n",
            "[Epoch 47 / 50 | Batch: 292 / 301] loss:   1465.167\n",
            "[Epoch 47 / 50 | Batch: 293 / 301] loss:   1065.057\n",
            "[Epoch 47 / 50 | Batch: 294 / 301] loss:   1014.890\n",
            "[Epoch 47 / 50 | Batch: 295 / 301] loss:   1060.837\n",
            "[Epoch 47 / 50 | Batch: 296 / 301] loss:   1158.807\n",
            "[Epoch 47 / 50 | Batch: 297 / 301] loss:   1272.068\n",
            "[Epoch 47 / 50 | Batch: 298 / 301] loss:    848.048\n",
            "[Epoch 47 / 50 | Batch: 299 / 301] loss:   1414.453\n",
            "[Epoch 47 / 50 | Batch: 300 / 301] loss:   1320.044\n",
            "[Epoch 47 / 50 | Batch: 301 / 301] loss:    615.954\n",
            "Epoch loss: 1161.87415\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4755.748\n",
            "[Validation] [Batch  2 / 15] dev loss:   6589.601\n",
            "[Validation] [Batch  3 / 15] dev loss:   7125.356\n",
            "[Validation] [Batch  4 / 15] dev loss:   5052.767\n",
            "[Validation] [Batch  5 / 15] dev loss:   5713.368\n",
            "[Validation] [Batch  6 / 15] dev loss:  10643.179\n",
            "[Validation] [Batch  7 / 15] dev loss:   5996.923\n",
            "[Validation] [Batch  8 / 15] dev loss:   4430.528\n",
            "[Validation] [Batch  9 / 15] dev loss:   3758.244\n",
            "[Validation] [Batch 10 / 15] dev loss:  15908.760\n",
            "[Validation] [Batch 11 / 15] dev loss:  13226.926\n",
            "[Validation] [Batch 12 / 15] dev loss:  11046.358\n",
            "[Validation] [Batch 13 / 15] dev loss:   4722.309\n",
            "[Validation] [Batch 14 / 15] dev loss:  23234.520\n",
            "[Validation] [Batch 15 / 15] dev loss:   2381.058\n",
            "Dev loss 8305.70957\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 48 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 48 / 50 | Batch:  1 / 301] loss:   1041.211\n",
            "[Epoch 48 / 50 | Batch:  2 / 301] loss:   1223.616\n",
            "[Epoch 48 / 50 | Batch:  3 / 301] loss:   1155.873\n",
            "[Epoch 48 / 50 | Batch:  4 / 301] loss:   1245.510\n",
            "[Epoch 48 / 50 | Batch:  5 / 301] loss:   1198.884\n",
            "[Epoch 48 / 50 | Batch:  6 / 301] loss:   1297.821\n",
            "[Epoch 48 / 50 | Batch:  7 / 301] loss:   1136.396\n",
            "[Epoch 48 / 50 | Batch:  8 / 301] loss:    870.552\n",
            "[Epoch 48 / 50 | Batch:  9 / 301] loss:    817.758\n",
            "[Epoch 48 / 50 | Batch: 10 / 301] loss:   1108.808\n",
            "[Epoch 48 / 50 | Batch: 11 / 301] loss:   1100.049\n",
            "[Epoch 48 / 50 | Batch: 12 / 301] loss:   1001.052\n",
            "[Epoch 48 / 50 | Batch: 13 / 301] loss:   1185.634\n",
            "[Epoch 48 / 50 | Batch: 14 / 301] loss:   1552.724\n",
            "[Epoch 48 / 50 | Batch: 15 / 301] loss:   1517.941\n",
            "[Epoch 48 / 50 | Batch: 16 / 301] loss:   1104.545\n",
            "[Epoch 48 / 50 | Batch: 17 / 301] loss:   1053.200\n",
            "[Epoch 48 / 50 | Batch: 18 / 301] loss:   1309.446\n",
            "[Epoch 48 / 50 | Batch: 19 / 301] loss:    960.205\n",
            "[Epoch 48 / 50 | Batch: 20 / 301] loss:   1264.793\n",
            "[Epoch 48 / 50 | Batch: 21 / 301] loss:   1233.694\n",
            "[Epoch 48 / 50 | Batch: 22 / 301] loss:   1147.312\n",
            "[Epoch 48 / 50 | Batch: 23 / 301] loss:   1139.619\n",
            "[Epoch 48 / 50 | Batch: 24 / 301] loss:   1254.835\n",
            "[Epoch 48 / 50 | Batch: 25 / 301] loss:   1072.098\n",
            "[Epoch 48 / 50 | Batch: 26 / 301] loss:   1059.177\n",
            "[Epoch 48 / 50 | Batch: 27 / 301] loss:    933.057\n",
            "[Epoch 48 / 50 | Batch: 28 / 301] loss:   1213.305\n",
            "[Epoch 48 / 50 | Batch: 29 / 301] loss:   1285.738\n",
            "[Epoch 48 / 50 | Batch: 30 / 301] loss:    938.687\n",
            "[Epoch 48 / 50 | Batch: 31 / 301] loss:    935.664\n",
            "[Epoch 48 / 50 | Batch: 32 / 301] loss:   1066.764\n",
            "[Epoch 48 / 50 | Batch: 33 / 301] loss:   1297.423\n",
            "[Epoch 48 / 50 | Batch: 34 / 301] loss:   1428.396\n",
            "[Epoch 48 / 50 | Batch: 35 / 301] loss:   1212.458\n",
            "[Epoch 48 / 50 | Batch: 36 / 301] loss:   1123.460\n",
            "[Epoch 48 / 50 | Batch: 37 / 301] loss:   1220.999\n",
            "[Epoch 48 / 50 | Batch: 38 / 301] loss:   1017.600\n",
            "[Epoch 48 / 50 | Batch: 39 / 301] loss:   1206.132\n",
            "[Epoch 48 / 50 | Batch: 40 / 301] loss:   1076.477\n",
            "[Epoch 48 / 50 | Batch: 41 / 301] loss:   1212.048\n",
            "[Epoch 48 / 50 | Batch: 42 / 301] loss:   1153.232\n",
            "[Epoch 48 / 50 | Batch: 43 / 301] loss:   1111.565\n",
            "[Epoch 48 / 50 | Batch: 44 / 301] loss:    870.561\n",
            "[Epoch 48 / 50 | Batch: 45 / 301] loss:   1207.650\n",
            "[Epoch 48 / 50 | Batch: 46 / 301] loss:   1102.567\n",
            "[Epoch 48 / 50 | Batch: 47 / 301] loss:   1512.731\n",
            "[Epoch 48 / 50 | Batch: 48 / 301] loss:   1123.282\n",
            "[Epoch 48 / 50 | Batch: 49 / 301] loss:   1321.828\n",
            "[Epoch 48 / 50 | Batch: 50 / 301] loss:   1288.142\n",
            "[Epoch 48 / 50 | Batch: 51 / 301] loss:   1137.241\n",
            "[Epoch 48 / 50 | Batch: 52 / 301] loss:   1034.154\n",
            "[Epoch 48 / 50 | Batch: 53 / 301] loss:   1412.637\n",
            "[Epoch 48 / 50 | Batch: 54 / 301] loss:   1107.058\n",
            "[Epoch 48 / 50 | Batch: 55 / 301] loss:   1415.442\n",
            "[Epoch 48 / 50 | Batch: 56 / 301] loss:   1280.676\n",
            "[Epoch 48 / 50 | Batch: 57 / 301] loss:   1061.279\n",
            "[Epoch 48 / 50 | Batch: 58 / 301] loss:    935.486\n",
            "[Epoch 48 / 50 | Batch: 59 / 301] loss:   1419.969\n",
            "[Epoch 48 / 50 | Batch: 60 / 301] loss:   1258.021\n",
            "[Epoch 48 / 50 | Batch: 61 / 301] loss:   1166.761\n",
            "[Epoch 48 / 50 | Batch: 62 / 301] loss:    983.469\n",
            "[Epoch 48 / 50 | Batch: 63 / 301] loss:   1474.245\n",
            "[Epoch 48 / 50 | Batch: 64 / 301] loss:   1172.123\n",
            "[Epoch 48 / 50 | Batch: 65 / 301] loss:   1284.642\n",
            "[Epoch 48 / 50 | Batch: 66 / 301] loss:   1058.430\n",
            "[Epoch 48 / 50 | Batch: 67 / 301] loss:   1397.143\n",
            "[Epoch 48 / 50 | Batch: 68 / 301] loss:   1317.084\n",
            "[Epoch 48 / 50 | Batch: 69 / 301] loss:   1213.956\n",
            "[Epoch 48 / 50 | Batch: 70 / 301] loss:   1004.581\n",
            "[Epoch 48 / 50 | Batch: 71 / 301] loss:   1271.618\n",
            "[Epoch 48 / 50 | Batch: 72 / 301] loss:    931.742\n",
            "[Epoch 48 / 50 | Batch: 73 / 301] loss:   1161.902\n",
            "[Epoch 48 / 50 | Batch: 74 / 301] loss:   1150.583\n",
            "[Epoch 48 / 50 | Batch: 75 / 301] loss:   1166.929\n",
            "[Epoch 48 / 50 | Batch: 76 / 301] loss:   1894.142\n",
            "[Epoch 48 / 50 | Batch: 77 / 301] loss:   1295.562\n",
            "[Epoch 48 / 50 | Batch: 78 / 301] loss:   1036.852\n",
            "[Epoch 48 / 50 | Batch: 79 / 301] loss:   1388.395\n",
            "[Epoch 48 / 50 | Batch: 80 / 301] loss:    984.679\n",
            "[Epoch 48 / 50 | Batch: 81 / 301] loss:    947.165\n",
            "[Epoch 48 / 50 | Batch: 82 / 301] loss:   1202.669\n",
            "[Epoch 48 / 50 | Batch: 83 / 301] loss:    915.272\n",
            "[Epoch 48 / 50 | Batch: 84 / 301] loss:   1058.158\n",
            "[Epoch 48 / 50 | Batch: 85 / 301] loss:   1268.002\n",
            "[Epoch 48 / 50 | Batch: 86 / 301] loss:   1005.085\n",
            "[Epoch 48 / 50 | Batch: 87 / 301] loss:   1050.530\n",
            "[Epoch 48 / 50 | Batch: 88 / 301] loss:   1389.705\n",
            "[Epoch 48 / 50 | Batch: 89 / 301] loss:   1309.331\n",
            "[Epoch 48 / 50 | Batch: 90 / 301] loss:   1332.259\n",
            "[Epoch 48 / 50 | Batch: 91 / 301] loss:   1044.073\n",
            "[Epoch 48 / 50 | Batch: 92 / 301] loss:   1189.163\n",
            "[Epoch 48 / 50 | Batch: 93 / 301] loss:   1238.267\n",
            "[Epoch 48 / 50 | Batch: 94 / 301] loss:   1288.552\n",
            "[Epoch 48 / 50 | Batch: 95 / 301] loss:   1264.457\n",
            "[Epoch 48 / 50 | Batch: 96 / 301] loss:   1060.862\n",
            "[Epoch 48 / 50 | Batch: 97 / 301] loss:   1033.416\n",
            "[Epoch 48 / 50 | Batch: 98 / 301] loss:   1063.749\n",
            "[Epoch 48 / 50 | Batch: 99 / 301] loss:   1037.985\n",
            "[Epoch 48 / 50 | Batch: 100 / 301] loss:   1325.632\n",
            "[Epoch 48 / 50 | Batch: 101 / 301] loss:   1107.945\n",
            "[Epoch 48 / 50 | Batch: 102 / 301] loss:    978.710\n",
            "[Epoch 48 / 50 | Batch: 103 / 301] loss:   1345.402\n",
            "[Epoch 48 / 50 | Batch: 104 / 301] loss:   1169.650\n",
            "[Epoch 48 / 50 | Batch: 105 / 301] loss:   1165.542\n",
            "[Epoch 48 / 50 | Batch: 106 / 301] loss:   1142.346\n",
            "[Epoch 48 / 50 | Batch: 107 / 301] loss:   1202.665\n",
            "[Epoch 48 / 50 | Batch: 108 / 301] loss:   1350.402\n",
            "[Epoch 48 / 50 | Batch: 109 / 301] loss:   1686.088\n",
            "[Epoch 48 / 50 | Batch: 110 / 301] loss:    890.318\n",
            "[Epoch 48 / 50 | Batch: 111 / 301] loss:   1183.900\n",
            "[Epoch 48 / 50 | Batch: 112 / 301] loss:    957.584\n",
            "[Epoch 48 / 50 | Batch: 113 / 301] loss:   1234.765\n",
            "[Epoch 48 / 50 | Batch: 114 / 301] loss:   1249.603\n",
            "[Epoch 48 / 50 | Batch: 115 / 301] loss:    931.647\n",
            "[Epoch 48 / 50 | Batch: 116 / 301] loss:   1270.196\n",
            "[Epoch 48 / 50 | Batch: 117 / 301] loss:   1351.580\n",
            "[Epoch 48 / 50 | Batch: 118 / 301] loss:   1134.034\n",
            "[Epoch 48 / 50 | Batch: 119 / 301] loss:    818.502\n",
            "[Epoch 48 / 50 | Batch: 120 / 301] loss:   1190.878\n",
            "[Epoch 48 / 50 | Batch: 121 / 301] loss:    961.467\n",
            "[Epoch 48 / 50 | Batch: 122 / 301] loss:   1126.813\n",
            "[Epoch 48 / 50 | Batch: 123 / 301] loss:    997.207\n",
            "[Epoch 48 / 50 | Batch: 124 / 301] loss:   1070.205\n",
            "[Epoch 48 / 50 | Batch: 125 / 301] loss:   1158.932\n",
            "[Epoch 48 / 50 | Batch: 126 / 301] loss:   1335.785\n",
            "[Epoch 48 / 50 | Batch: 127 / 301] loss:    983.528\n",
            "[Epoch 48 / 50 | Batch: 128 / 301] loss:   1037.720\n",
            "[Epoch 48 / 50 | Batch: 129 / 301] loss:    970.158\n",
            "[Epoch 48 / 50 | Batch: 130 / 301] loss:   1192.556\n",
            "[Epoch 48 / 50 | Batch: 131 / 301] loss:   1085.409\n",
            "[Epoch 48 / 50 | Batch: 132 / 301] loss:   1402.592\n",
            "[Epoch 48 / 50 | Batch: 133 / 301] loss:   1176.330\n",
            "[Epoch 48 / 50 | Batch: 134 / 301] loss:   1314.593\n",
            "[Epoch 48 / 50 | Batch: 135 / 301] loss:   1041.271\n",
            "[Epoch 48 / 50 | Batch: 136 / 301] loss:   1406.594\n",
            "[Epoch 48 / 50 | Batch: 137 / 301] loss:   1219.009\n",
            "[Epoch 48 / 50 | Batch: 138 / 301] loss:   1062.565\n",
            "[Epoch 48 / 50 | Batch: 139 / 301] loss:   1253.688\n",
            "[Epoch 48 / 50 | Batch: 140 / 301] loss:   1042.008\n",
            "[Epoch 48 / 50 | Batch: 141 / 301] loss:   1190.410\n",
            "[Epoch 48 / 50 | Batch: 142 / 301] loss:   1109.228\n",
            "[Epoch 48 / 50 | Batch: 143 / 301] loss:    994.587\n",
            "[Epoch 48 / 50 | Batch: 144 / 301] loss:   1159.871\n",
            "[Epoch 48 / 50 | Batch: 145 / 301] loss:   1342.060\n",
            "[Epoch 48 / 50 | Batch: 146 / 301] loss:   1080.711\n",
            "[Epoch 48 / 50 | Batch: 147 / 301] loss:    958.042\n",
            "[Epoch 48 / 50 | Batch: 148 / 301] loss:   1297.247\n",
            "[Epoch 48 / 50 | Batch: 149 / 301] loss:   1123.552\n",
            "[Epoch 48 / 50 | Batch: 150 / 301] loss:   1384.896\n",
            "[Epoch 48 / 50 | Batch: 151 / 301] loss:   1235.499\n",
            "[Epoch 48 / 50 | Batch: 152 / 301] loss:    769.599\n",
            "[Epoch 48 / 50 | Batch: 153 / 301] loss:   1102.953\n",
            "[Epoch 48 / 50 | Batch: 154 / 301] loss:   1221.203\n",
            "[Epoch 48 / 50 | Batch: 155 / 301] loss:    944.546\n",
            "[Epoch 48 / 50 | Batch: 156 / 301] loss:   1000.012\n",
            "[Epoch 48 / 50 | Batch: 157 / 301] loss:   1211.896\n",
            "[Epoch 48 / 50 | Batch: 158 / 301] loss:   1201.832\n",
            "[Epoch 48 / 50 | Batch: 159 / 301] loss:    986.039\n",
            "[Epoch 48 / 50 | Batch: 160 / 301] loss:    856.906\n",
            "[Epoch 48 / 50 | Batch: 161 / 301] loss:   1091.499\n",
            "[Epoch 48 / 50 | Batch: 162 / 301] loss:    990.159\n",
            "[Epoch 48 / 50 | Batch: 163 / 301] loss:    888.811\n",
            "[Epoch 48 / 50 | Batch: 164 / 301] loss:   1023.001\n",
            "[Epoch 48 / 50 | Batch: 165 / 301] loss:   1230.851\n",
            "[Epoch 48 / 50 | Batch: 166 / 301] loss:    829.792\n",
            "[Epoch 48 / 50 | Batch: 167 / 301] loss:    859.928\n",
            "[Epoch 48 / 50 | Batch: 168 / 301] loss:    982.428\n",
            "[Epoch 48 / 50 | Batch: 169 / 301] loss:   1286.979\n",
            "[Epoch 48 / 50 | Batch: 170 / 301] loss:    951.210\n",
            "[Epoch 48 / 50 | Batch: 171 / 301] loss:    946.271\n",
            "[Epoch 48 / 50 | Batch: 172 / 301] loss:   1183.008\n",
            "[Epoch 48 / 50 | Batch: 173 / 301] loss:    965.175\n",
            "[Epoch 48 / 50 | Batch: 174 / 301] loss:   1199.747\n",
            "[Epoch 48 / 50 | Batch: 175 / 301] loss:   1698.763\n",
            "[Epoch 48 / 50 | Batch: 176 / 301] loss:    756.048\n",
            "[Epoch 48 / 50 | Batch: 177 / 301] loss:   1583.009\n",
            "[Epoch 48 / 50 | Batch: 178 / 301] loss:   1048.287\n",
            "[Epoch 48 / 50 | Batch: 179 / 301] loss:   1511.962\n",
            "[Epoch 48 / 50 | Batch: 180 / 301] loss:    891.593\n",
            "[Epoch 48 / 50 | Batch: 181 / 301] loss:   1150.345\n",
            "[Epoch 48 / 50 | Batch: 182 / 301] loss:   1168.894\n",
            "[Epoch 48 / 50 | Batch: 183 / 301] loss:   1149.642\n",
            "[Epoch 48 / 50 | Batch: 184 / 301] loss:   1246.538\n",
            "[Epoch 48 / 50 | Batch: 185 / 301] loss:   1143.422\n",
            "[Epoch 48 / 50 | Batch: 186 / 301] loss:   1104.150\n",
            "[Epoch 48 / 50 | Batch: 187 / 301] loss:   1572.730\n",
            "[Epoch 48 / 50 | Batch: 188 / 301] loss:   1143.710\n",
            "[Epoch 48 / 50 | Batch: 189 / 301] loss:   1074.323\n",
            "[Epoch 48 / 50 | Batch: 190 / 301] loss:   1488.790\n",
            "[Epoch 48 / 50 | Batch: 191 / 301] loss:   1185.833\n",
            "[Epoch 48 / 50 | Batch: 192 / 301] loss:    975.960\n",
            "[Epoch 48 / 50 | Batch: 193 / 301] loss:   1161.477\n",
            "[Epoch 48 / 50 | Batch: 194 / 301] loss:   1112.248\n",
            "[Epoch 48 / 50 | Batch: 195 / 301] loss:   1121.435\n",
            "[Epoch 48 / 50 | Batch: 196 / 301] loss:    915.412\n",
            "[Epoch 48 / 50 | Batch: 197 / 301] loss:   1199.424\n",
            "[Epoch 48 / 50 | Batch: 198 / 301] loss:   1010.738\n",
            "[Epoch 48 / 50 | Batch: 199 / 301] loss:   1526.044\n",
            "[Epoch 48 / 50 | Batch: 200 / 301] loss:   1230.977\n",
            "[Epoch 48 / 50 | Batch: 201 / 301] loss:    987.589\n",
            "[Epoch 48 / 50 | Batch: 202 / 301] loss:    935.099\n",
            "[Epoch 48 / 50 | Batch: 203 / 301] loss:   1084.187\n",
            "[Epoch 48 / 50 | Batch: 204 / 301] loss:   1226.903\n",
            "[Epoch 48 / 50 | Batch: 205 / 301] loss:   1023.331\n",
            "[Epoch 48 / 50 | Batch: 206 / 301] loss:   1568.340\n",
            "[Epoch 48 / 50 | Batch: 207 / 301] loss:    965.860\n",
            "[Epoch 48 / 50 | Batch: 208 / 301] loss:   1194.879\n",
            "[Epoch 48 / 50 | Batch: 209 / 301] loss:   1132.148\n",
            "[Epoch 48 / 50 | Batch: 210 / 301] loss:   1450.568\n",
            "[Epoch 48 / 50 | Batch: 211 / 301] loss:   1330.121\n",
            "[Epoch 48 / 50 | Batch: 212 / 301] loss:   1364.071\n",
            "[Epoch 48 / 50 | Batch: 213 / 301] loss:   1220.999\n",
            "[Epoch 48 / 50 | Batch: 214 / 301] loss:   1186.409\n",
            "[Epoch 48 / 50 | Batch: 215 / 301] loss:   1206.516\n",
            "[Epoch 48 / 50 | Batch: 216 / 301] loss:   1314.685\n",
            "[Epoch 48 / 50 | Batch: 217 / 301] loss:   1512.140\n",
            "[Epoch 48 / 50 | Batch: 218 / 301] loss:   1498.240\n",
            "[Epoch 48 / 50 | Batch: 219 / 301] loss:   1333.157\n",
            "[Epoch 48 / 50 | Batch: 220 / 301] loss:   1105.254\n",
            "[Epoch 48 / 50 | Batch: 221 / 301] loss:   1020.653\n",
            "[Epoch 48 / 50 | Batch: 222 / 301] loss:   1387.848\n",
            "[Epoch 48 / 50 | Batch: 223 / 301] loss:   1107.736\n",
            "[Epoch 48 / 50 | Batch: 224 / 301] loss:    950.066\n",
            "[Epoch 48 / 50 | Batch: 225 / 301] loss:   1000.144\n",
            "[Epoch 48 / 50 | Batch: 226 / 301] loss:    968.439\n",
            "[Epoch 48 / 50 | Batch: 227 / 301] loss:   1480.503\n",
            "[Epoch 48 / 50 | Batch: 228 / 301] loss:   1055.473\n",
            "[Epoch 48 / 50 | Batch: 229 / 301] loss:    807.542\n",
            "[Epoch 48 / 50 | Batch: 230 / 301] loss:   1159.000\n",
            "[Epoch 48 / 50 | Batch: 231 / 301] loss:   1135.024\n",
            "[Epoch 48 / 50 | Batch: 232 / 301] loss:   1031.246\n",
            "[Epoch 48 / 50 | Batch: 233 / 301] loss:   1058.591\n",
            "[Epoch 48 / 50 | Batch: 234 / 301] loss:   1266.579\n",
            "[Epoch 48 / 50 | Batch: 235 / 301] loss:   1434.017\n",
            "[Epoch 48 / 50 | Batch: 236 / 301] loss:   1188.682\n",
            "[Epoch 48 / 50 | Batch: 237 / 301] loss:   1132.056\n",
            "[Epoch 48 / 50 | Batch: 238 / 301] loss:   1066.855\n",
            "[Epoch 48 / 50 | Batch: 239 / 301] loss:   1111.520\n",
            "[Epoch 48 / 50 | Batch: 240 / 301] loss:   1258.212\n",
            "[Epoch 48 / 50 | Batch: 241 / 301] loss:    974.867\n",
            "[Epoch 48 / 50 | Batch: 242 / 301] loss:   1093.753\n",
            "[Epoch 48 / 50 | Batch: 243 / 301] loss:    915.991\n",
            "[Epoch 48 / 50 | Batch: 244 / 301] loss:   1362.127\n",
            "[Epoch 48 / 50 | Batch: 245 / 301] loss:    976.183\n",
            "[Epoch 48 / 50 | Batch: 246 / 301] loss:   1134.447\n",
            "[Epoch 48 / 50 | Batch: 247 / 301] loss:    967.138\n",
            "[Epoch 48 / 50 | Batch: 248 / 301] loss:   1113.354\n",
            "[Epoch 48 / 50 | Batch: 249 / 301] loss:   1552.975\n",
            "[Epoch 48 / 50 | Batch: 250 / 301] loss:    994.244\n",
            "[Epoch 48 / 50 | Batch: 251 / 301] loss:   1355.730\n",
            "[Epoch 48 / 50 | Batch: 252 / 301] loss:   1208.674\n",
            "[Epoch 48 / 50 | Batch: 253 / 301] loss:    886.554\n",
            "[Epoch 48 / 50 | Batch: 254 / 301] loss:   1252.233\n",
            "[Epoch 48 / 50 | Batch: 255 / 301] loss:    936.999\n",
            "[Epoch 48 / 50 | Batch: 256 / 301] loss:   1159.687\n",
            "[Epoch 48 / 50 | Batch: 257 / 301] loss:    900.448\n",
            "[Epoch 48 / 50 | Batch: 258 / 301] loss:   1208.497\n",
            "[Epoch 48 / 50 | Batch: 259 / 301] loss:   1707.476\n",
            "[Epoch 48 / 50 | Batch: 260 / 301] loss:   1096.359\n",
            "[Epoch 48 / 50 | Batch: 261 / 301] loss:   1046.705\n",
            "[Epoch 48 / 50 | Batch: 262 / 301] loss:    970.206\n",
            "[Epoch 48 / 50 | Batch: 263 / 301] loss:    945.517\n",
            "[Epoch 48 / 50 | Batch: 264 / 301] loss:   1073.903\n",
            "[Epoch 48 / 50 | Batch: 265 / 301] loss:   1178.824\n",
            "[Epoch 48 / 50 | Batch: 266 / 301] loss:    999.044\n",
            "[Epoch 48 / 50 | Batch: 267 / 301] loss:   1004.062\n",
            "[Epoch 48 / 50 | Batch: 268 / 301] loss:   1000.774\n",
            "[Epoch 48 / 50 | Batch: 269 / 301] loss:   1098.133\n",
            "[Epoch 48 / 50 | Batch: 270 / 301] loss:   1500.297\n",
            "[Epoch 48 / 50 | Batch: 271 / 301] loss:   1274.907\n",
            "[Epoch 48 / 50 | Batch: 272 / 301] loss:   1324.740\n",
            "[Epoch 48 / 50 | Batch: 273 / 301] loss:   1102.588\n",
            "[Epoch 48 / 50 | Batch: 274 / 301] loss:   1113.228\n",
            "[Epoch 48 / 50 | Batch: 275 / 301] loss:   1191.476\n",
            "[Epoch 48 / 50 | Batch: 276 / 301] loss:    914.362\n",
            "[Epoch 48 / 50 | Batch: 277 / 301] loss:   1050.398\n",
            "[Epoch 48 / 50 | Batch: 278 / 301] loss:   1777.743\n",
            "[Epoch 48 / 50 | Batch: 279 / 301] loss:   1219.333\n",
            "[Epoch 48 / 50 | Batch: 280 / 301] loss:    990.966\n",
            "[Epoch 48 / 50 | Batch: 281 / 301] loss:   1254.543\n",
            "[Epoch 48 / 50 | Batch: 282 / 301] loss:   1179.548\n",
            "[Epoch 48 / 50 | Batch: 283 / 301] loss:   1050.089\n",
            "[Epoch 48 / 50 | Batch: 284 / 301] loss:   1280.454\n",
            "[Epoch 48 / 50 | Batch: 285 / 301] loss:   1361.633\n",
            "[Epoch 48 / 50 | Batch: 286 / 301] loss:   1697.339\n",
            "[Epoch 48 / 50 | Batch: 287 / 301] loss:   1500.982\n",
            "[Epoch 48 / 50 | Batch: 288 / 301] loss:    867.824\n",
            "[Epoch 48 / 50 | Batch: 289 / 301] loss:   1378.968\n",
            "[Epoch 48 / 50 | Batch: 290 / 301] loss:   1250.321\n",
            "[Epoch 48 / 50 | Batch: 291 / 301] loss:   1344.234\n",
            "[Epoch 48 / 50 | Batch: 292 / 301] loss:   1177.287\n",
            "[Epoch 48 / 50 | Batch: 293 / 301] loss:   1254.634\n",
            "[Epoch 48 / 50 | Batch: 294 / 301] loss:   1576.715\n",
            "[Epoch 48 / 50 | Batch: 295 / 301] loss:    928.813\n",
            "[Epoch 48 / 50 | Batch: 296 / 301] loss:   1221.833\n",
            "[Epoch 48 / 50 | Batch: 297 / 301] loss:   1271.122\n",
            "[Epoch 48 / 50 | Batch: 298 / 301] loss:    958.240\n",
            "[Epoch 48 / 50 | Batch: 299 / 301] loss:   1265.951\n",
            "[Epoch 48 / 50 | Batch: 300 / 301] loss:   1284.657\n",
            "[Epoch 48 / 50 | Batch: 301 / 301] loss:    690.227\n",
            "Epoch loss: 1162.64645\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4524.301\n",
            "[Validation] [Batch  2 / 15] dev loss:   6047.256\n",
            "[Validation] [Batch  3 / 15] dev loss:   7101.395\n",
            "[Validation] [Batch  4 / 15] dev loss:   5657.055\n",
            "[Validation] [Batch  5 / 15] dev loss:   5681.849\n",
            "[Validation] [Batch  6 / 15] dev loss:   8941.485\n",
            "[Validation] [Batch  7 / 15] dev loss:   6240.252\n",
            "[Validation] [Batch  8 / 15] dev loss:   4375.496\n",
            "[Validation] [Batch  9 / 15] dev loss:   3595.368\n",
            "[Validation] [Batch 10 / 15] dev loss:  15951.876\n",
            "[Validation] [Batch 11 / 15] dev loss:  13230.910\n",
            "[Validation] [Batch 12 / 15] dev loss:  10643.903\n",
            "[Validation] [Batch 13 / 15] dev loss:   4560.813\n",
            "[Validation] [Batch 14 / 15] dev loss:  21901.998\n",
            "[Validation] [Batch 15 / 15] dev loss:   1834.731\n",
            "Dev loss 8019.24583\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 49 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 49 / 50 | Batch:  1 / 301] loss:   1272.152\n",
            "[Epoch 49 / 50 | Batch:  2 / 301] loss:   1372.523\n",
            "[Epoch 49 / 50 | Batch:  3 / 301] loss:    991.139\n",
            "[Epoch 49 / 50 | Batch:  4 / 301] loss:   1398.315\n",
            "[Epoch 49 / 50 | Batch:  5 / 301] loss:   1322.080\n",
            "[Epoch 49 / 50 | Batch:  6 / 301] loss:   1283.484\n",
            "[Epoch 49 / 50 | Batch:  7 / 301] loss:   1254.710\n",
            "[Epoch 49 / 50 | Batch:  8 / 301] loss:   1347.697\n",
            "[Epoch 49 / 50 | Batch:  9 / 301] loss:   1057.234\n",
            "[Epoch 49 / 50 | Batch: 10 / 301] loss:   1345.390\n",
            "[Epoch 49 / 50 | Batch: 11 / 301] loss:    950.549\n",
            "[Epoch 49 / 50 | Batch: 12 / 301] loss:   1037.714\n",
            "[Epoch 49 / 50 | Batch: 13 / 301] loss:    923.029\n",
            "[Epoch 49 / 50 | Batch: 14 / 301] loss:   1315.024\n",
            "[Epoch 49 / 50 | Batch: 15 / 301] loss:   1203.402\n",
            "[Epoch 49 / 50 | Batch: 16 / 301] loss:   1163.183\n",
            "[Epoch 49 / 50 | Batch: 17 / 301] loss:   1569.174\n",
            "[Epoch 49 / 50 | Batch: 18 / 301] loss:    972.786\n",
            "[Epoch 49 / 50 | Batch: 19 / 301] loss:   1432.865\n",
            "[Epoch 49 / 50 | Batch: 20 / 301] loss:   1336.559\n",
            "[Epoch 49 / 50 | Batch: 21 / 301] loss:   1208.157\n",
            "[Epoch 49 / 50 | Batch: 22 / 301] loss:    791.731\n",
            "[Epoch 49 / 50 | Batch: 23 / 301] loss:   1384.900\n",
            "[Epoch 49 / 50 | Batch: 24 / 301] loss:    966.625\n",
            "[Epoch 49 / 50 | Batch: 25 / 301] loss:   1129.877\n",
            "[Epoch 49 / 50 | Batch: 26 / 301] loss:   1567.750\n",
            "[Epoch 49 / 50 | Batch: 27 / 301] loss:   1157.978\n",
            "[Epoch 49 / 50 | Batch: 28 / 301] loss:   1390.477\n",
            "[Epoch 49 / 50 | Batch: 29 / 301] loss:   1358.785\n",
            "[Epoch 49 / 50 | Batch: 30 / 301] loss:   1106.053\n",
            "[Epoch 49 / 50 | Batch: 31 / 301] loss:   1079.066\n",
            "[Epoch 49 / 50 | Batch: 32 / 301] loss:   1005.669\n",
            "[Epoch 49 / 50 | Batch: 33 / 301] loss:   1415.369\n",
            "[Epoch 49 / 50 | Batch: 34 / 301] loss:   1394.048\n",
            "[Epoch 49 / 50 | Batch: 35 / 301] loss:    923.753\n",
            "[Epoch 49 / 50 | Batch: 36 / 301] loss:    962.161\n",
            "[Epoch 49 / 50 | Batch: 37 / 301] loss:   1536.228\n",
            "[Epoch 49 / 50 | Batch: 38 / 301] loss:   1095.348\n",
            "[Epoch 49 / 50 | Batch: 39 / 301] loss:    864.636\n",
            "[Epoch 49 / 50 | Batch: 40 / 301] loss:    897.737\n",
            "[Epoch 49 / 50 | Batch: 41 / 301] loss:   1012.026\n",
            "[Epoch 49 / 50 | Batch: 42 / 301] loss:   1550.905\n",
            "[Epoch 49 / 50 | Batch: 43 / 301] loss:   1171.730\n",
            "[Epoch 49 / 50 | Batch: 44 / 301] loss:    891.002\n",
            "[Epoch 49 / 50 | Batch: 45 / 301] loss:   1378.238\n",
            "[Epoch 49 / 50 | Batch: 46 / 301] loss:   1861.280\n",
            "[Epoch 49 / 50 | Batch: 47 / 301] loss:   1055.796\n",
            "[Epoch 49 / 50 | Batch: 48 / 301] loss:   1087.763\n",
            "[Epoch 49 / 50 | Batch: 49 / 301] loss:   1041.718\n",
            "[Epoch 49 / 50 | Batch: 50 / 301] loss:   1111.897\n",
            "[Epoch 49 / 50 | Batch: 51 / 301] loss:   1036.898\n",
            "[Epoch 49 / 50 | Batch: 52 / 301] loss:   1014.979\n",
            "[Epoch 49 / 50 | Batch: 53 / 301] loss:   1113.898\n",
            "[Epoch 49 / 50 | Batch: 54 / 301] loss:    911.472\n",
            "[Epoch 49 / 50 | Batch: 55 / 301] loss:   1291.281\n",
            "[Epoch 49 / 50 | Batch: 56 / 301] loss:   1100.487\n",
            "[Epoch 49 / 50 | Batch: 57 / 301] loss:   1175.910\n",
            "[Epoch 49 / 50 | Batch: 58 / 301] loss:   1110.809\n",
            "[Epoch 49 / 50 | Batch: 59 / 301] loss:   1086.491\n",
            "[Epoch 49 / 50 | Batch: 60 / 301] loss:   1617.503\n",
            "[Epoch 49 / 50 | Batch: 61 / 301] loss:   1307.914\n",
            "[Epoch 49 / 50 | Batch: 62 / 301] loss:    850.255\n",
            "[Epoch 49 / 50 | Batch: 63 / 301] loss:   1065.075\n",
            "[Epoch 49 / 50 | Batch: 64 / 301] loss:    958.176\n",
            "[Epoch 49 / 50 | Batch: 65 / 301] loss:   1275.064\n",
            "[Epoch 49 / 50 | Batch: 66 / 301] loss:   1007.204\n",
            "[Epoch 49 / 50 | Batch: 67 / 301] loss:   1423.662\n",
            "[Epoch 49 / 50 | Batch: 68 / 301] loss:   1081.475\n",
            "[Epoch 49 / 50 | Batch: 69 / 301] loss:   1157.207\n",
            "[Epoch 49 / 50 | Batch: 70 / 301] loss:   1289.883\n",
            "[Epoch 49 / 50 | Batch: 71 / 301] loss:   1287.220\n",
            "[Epoch 49 / 50 | Batch: 72 / 301] loss:   1076.109\n",
            "[Epoch 49 / 50 | Batch: 73 / 301] loss:   1031.984\n",
            "[Epoch 49 / 50 | Batch: 74 / 301] loss:   1224.207\n",
            "[Epoch 49 / 50 | Batch: 75 / 301] loss:   1594.573\n",
            "[Epoch 49 / 50 | Batch: 76 / 301] loss:   1118.691\n",
            "[Epoch 49 / 50 | Batch: 77 / 301] loss:    999.033\n",
            "[Epoch 49 / 50 | Batch: 78 / 301] loss:   1446.686\n",
            "[Epoch 49 / 50 | Batch: 79 / 301] loss:   1013.515\n",
            "[Epoch 49 / 50 | Batch: 80 / 301] loss:   1406.288\n",
            "[Epoch 49 / 50 | Batch: 81 / 301] loss:   1549.160\n",
            "[Epoch 49 / 50 | Batch: 82 / 301] loss:   1018.582\n",
            "[Epoch 49 / 50 | Batch: 83 / 301] loss:   1274.821\n",
            "[Epoch 49 / 50 | Batch: 84 / 301] loss:   1034.230\n",
            "[Epoch 49 / 50 | Batch: 85 / 301] loss:    857.389\n",
            "[Epoch 49 / 50 | Batch: 86 / 301] loss:    991.348\n",
            "[Epoch 49 / 50 | Batch: 87 / 301] loss:    827.093\n",
            "[Epoch 49 / 50 | Batch: 88 / 301] loss:   1279.403\n",
            "[Epoch 49 / 50 | Batch: 89 / 301] loss:   1252.464\n",
            "[Epoch 49 / 50 | Batch: 90 / 301] loss:   1400.880\n",
            "[Epoch 49 / 50 | Batch: 91 / 301] loss:    916.311\n",
            "[Epoch 49 / 50 | Batch: 92 / 301] loss:   1479.781\n",
            "[Epoch 49 / 50 | Batch: 93 / 301] loss:   1192.611\n",
            "[Epoch 49 / 50 | Batch: 94 / 301] loss:   1036.419\n",
            "[Epoch 49 / 50 | Batch: 95 / 301] loss:   1288.367\n",
            "[Epoch 49 / 50 | Batch: 96 / 301] loss:   1126.747\n",
            "[Epoch 49 / 50 | Batch: 97 / 301] loss:   1275.602\n",
            "[Epoch 49 / 50 | Batch: 98 / 301] loss:   1201.318\n",
            "[Epoch 49 / 50 | Batch: 99 / 301] loss:   1327.875\n",
            "[Epoch 49 / 50 | Batch: 100 / 301] loss:    790.599\n",
            "[Epoch 49 / 50 | Batch: 101 / 301] loss:    918.661\n",
            "[Epoch 49 / 50 | Batch: 102 / 301] loss:   1310.164\n",
            "[Epoch 49 / 50 | Batch: 103 / 301] loss:   1274.531\n",
            "[Epoch 49 / 50 | Batch: 104 / 301] loss:   1065.464\n",
            "[Epoch 49 / 50 | Batch: 105 / 301] loss:   1024.904\n",
            "[Epoch 49 / 50 | Batch: 106 / 301] loss:   1000.810\n",
            "[Epoch 49 / 50 | Batch: 107 / 301] loss:   1225.001\n",
            "[Epoch 49 / 50 | Batch: 108 / 301] loss:   1223.466\n",
            "[Epoch 49 / 50 | Batch: 109 / 301] loss:    914.842\n",
            "[Epoch 49 / 50 | Batch: 110 / 301] loss:   1165.463\n",
            "[Epoch 49 / 50 | Batch: 111 / 301] loss:   1283.825\n",
            "[Epoch 49 / 50 | Batch: 112 / 301] loss:    874.840\n",
            "[Epoch 49 / 50 | Batch: 113 / 301] loss:   1013.019\n",
            "[Epoch 49 / 50 | Batch: 114 / 301] loss:   1381.701\n",
            "[Epoch 49 / 50 | Batch: 115 / 301] loss:   1307.626\n",
            "[Epoch 49 / 50 | Batch: 116 / 301] loss:   1114.375\n",
            "[Epoch 49 / 50 | Batch: 117 / 301] loss:    989.243\n",
            "[Epoch 49 / 50 | Batch: 118 / 301] loss:    863.204\n",
            "[Epoch 49 / 50 | Batch: 119 / 301] loss:    827.970\n",
            "[Epoch 49 / 50 | Batch: 120 / 301] loss:   1329.342\n",
            "[Epoch 49 / 50 | Batch: 121 / 301] loss:   1172.970\n",
            "[Epoch 49 / 50 | Batch: 122 / 301] loss:   1045.692\n",
            "[Epoch 49 / 50 | Batch: 123 / 301] loss:   1365.927\n",
            "[Epoch 49 / 50 | Batch: 124 / 301] loss:   1136.057\n",
            "[Epoch 49 / 50 | Batch: 125 / 301] loss:   1034.603\n",
            "[Epoch 49 / 50 | Batch: 126 / 301] loss:   1245.087\n",
            "[Epoch 49 / 50 | Batch: 127 / 301] loss:   1414.656\n",
            "[Epoch 49 / 50 | Batch: 128 / 301] loss:   1133.519\n",
            "[Epoch 49 / 50 | Batch: 129 / 301] loss:   1510.962\n",
            "[Epoch 49 / 50 | Batch: 130 / 301] loss:   1072.197\n",
            "[Epoch 49 / 50 | Batch: 131 / 301] loss:   1306.249\n",
            "[Epoch 49 / 50 | Batch: 132 / 301] loss:   1554.741\n",
            "[Epoch 49 / 50 | Batch: 133 / 301] loss:   1136.712\n",
            "[Epoch 49 / 50 | Batch: 134 / 301] loss:   1088.731\n",
            "[Epoch 49 / 50 | Batch: 135 / 301] loss:   1283.509\n",
            "[Epoch 49 / 50 | Batch: 136 / 301] loss:   1599.645\n",
            "[Epoch 49 / 50 | Batch: 137 / 301] loss:    999.666\n",
            "[Epoch 49 / 50 | Batch: 138 / 301] loss:   1162.055\n",
            "[Epoch 49 / 50 | Batch: 139 / 301] loss:    924.692\n",
            "[Epoch 49 / 50 | Batch: 140 / 301] loss:   1320.319\n",
            "[Epoch 49 / 50 | Batch: 141 / 301] loss:    932.997\n",
            "[Epoch 49 / 50 | Batch: 142 / 301] loss:    960.099\n",
            "[Epoch 49 / 50 | Batch: 143 / 301] loss:   1229.804\n",
            "[Epoch 49 / 50 | Batch: 144 / 301] loss:   1532.817\n",
            "[Epoch 49 / 50 | Batch: 145 / 301] loss:    952.777\n",
            "[Epoch 49 / 50 | Batch: 146 / 301] loss:   1118.215\n",
            "[Epoch 49 / 50 | Batch: 147 / 301] loss:   1169.888\n",
            "[Epoch 49 / 50 | Batch: 148 / 301] loss:   1745.912\n",
            "[Epoch 49 / 50 | Batch: 149 / 301] loss:   1082.966\n",
            "[Epoch 49 / 50 | Batch: 150 / 301] loss:   1046.016\n",
            "[Epoch 49 / 50 | Batch: 151 / 301] loss:    987.105\n",
            "[Epoch 49 / 50 | Batch: 152 / 301] loss:   1273.178\n",
            "[Epoch 49 / 50 | Batch: 153 / 301] loss:   1115.001\n",
            "[Epoch 49 / 50 | Batch: 154 / 301] loss:   1267.574\n",
            "[Epoch 49 / 50 | Batch: 155 / 301] loss:   1355.836\n",
            "[Epoch 49 / 50 | Batch: 156 / 301] loss:    861.050\n",
            "[Epoch 49 / 50 | Batch: 157 / 301] loss:    896.137\n",
            "[Epoch 49 / 50 | Batch: 158 / 301] loss:    957.265\n",
            "[Epoch 49 / 50 | Batch: 159 / 301] loss:   1324.735\n",
            "[Epoch 49 / 50 | Batch: 160 / 301] loss:    980.288\n",
            "[Epoch 49 / 50 | Batch: 161 / 301] loss:    948.323\n",
            "[Epoch 49 / 50 | Batch: 162 / 301] loss:   1217.607\n",
            "[Epoch 49 / 50 | Batch: 163 / 301] loss:   1230.004\n",
            "[Epoch 49 / 50 | Batch: 164 / 301] loss:   1204.935\n",
            "[Epoch 49 / 50 | Batch: 165 / 301] loss:   1103.041\n",
            "[Epoch 49 / 50 | Batch: 166 / 301] loss:   1425.744\n",
            "[Epoch 49 / 50 | Batch: 167 / 301] loss:   1265.056\n",
            "[Epoch 49 / 50 | Batch: 168 / 301] loss:   1311.164\n",
            "[Epoch 49 / 50 | Batch: 169 / 301] loss:    852.336\n",
            "[Epoch 49 / 50 | Batch: 170 / 301] loss:    987.838\n",
            "[Epoch 49 / 50 | Batch: 171 / 301] loss:   1081.181\n",
            "[Epoch 49 / 50 | Batch: 172 / 301] loss:   1171.391\n",
            "[Epoch 49 / 50 | Batch: 173 / 301] loss:   1289.467\n",
            "[Epoch 49 / 50 | Batch: 174 / 301] loss:   1412.487\n",
            "[Epoch 49 / 50 | Batch: 175 / 301] loss:    869.771\n",
            "[Epoch 49 / 50 | Batch: 176 / 301] loss:    835.869\n",
            "[Epoch 49 / 50 | Batch: 177 / 301] loss:   1214.759\n",
            "[Epoch 49 / 50 | Batch: 178 / 301] loss:   1203.508\n",
            "[Epoch 49 / 50 | Batch: 179 / 301] loss:   1746.883\n",
            "[Epoch 49 / 50 | Batch: 180 / 301] loss:    973.710\n",
            "[Epoch 49 / 50 | Batch: 181 / 301] loss:   1087.996\n",
            "[Epoch 49 / 50 | Batch: 182 / 301] loss:   1288.313\n",
            "[Epoch 49 / 50 | Batch: 183 / 301] loss:   1060.694\n",
            "[Epoch 49 / 50 | Batch: 184 / 301] loss:    983.018\n",
            "[Epoch 49 / 50 | Batch: 185 / 301] loss:    924.319\n",
            "[Epoch 49 / 50 | Batch: 186 / 301] loss:    966.369\n",
            "[Epoch 49 / 50 | Batch: 187 / 301] loss:   1134.606\n",
            "[Epoch 49 / 50 | Batch: 188 / 301] loss:   1404.075\n",
            "[Epoch 49 / 50 | Batch: 189 / 301] loss:   1146.535\n",
            "[Epoch 49 / 50 | Batch: 190 / 301] loss:   1321.351\n",
            "[Epoch 49 / 50 | Batch: 191 / 301] loss:   1134.042\n",
            "[Epoch 49 / 50 | Batch: 192 / 301] loss:   1507.132\n",
            "[Epoch 49 / 50 | Batch: 193 / 301] loss:   1359.116\n",
            "[Epoch 49 / 50 | Batch: 194 / 301] loss:   1265.766\n",
            "[Epoch 49 / 50 | Batch: 195 / 301] loss:   1048.115\n",
            "[Epoch 49 / 50 | Batch: 196 / 301] loss:   1038.376\n",
            "[Epoch 49 / 50 | Batch: 197 / 301] loss:   1040.254\n",
            "[Epoch 49 / 50 | Batch: 198 / 301] loss:    807.189\n",
            "[Epoch 49 / 50 | Batch: 199 / 301] loss:   1165.049\n",
            "[Epoch 49 / 50 | Batch: 200 / 301] loss:   1108.588\n",
            "[Epoch 49 / 50 | Batch: 201 / 301] loss:   1067.742\n",
            "[Epoch 49 / 50 | Batch: 202 / 301] loss:   1358.729\n",
            "[Epoch 49 / 50 | Batch: 203 / 301] loss:   1207.429\n",
            "[Epoch 49 / 50 | Batch: 204 / 301] loss:   1167.195\n",
            "[Epoch 49 / 50 | Batch: 205 / 301] loss:   1700.785\n",
            "[Epoch 49 / 50 | Batch: 206 / 301] loss:   1219.052\n",
            "[Epoch 49 / 50 | Batch: 207 / 301] loss:   1168.610\n",
            "[Epoch 49 / 50 | Batch: 208 / 301] loss:   1182.287\n",
            "[Epoch 49 / 50 | Batch: 209 / 301] loss:   1143.641\n",
            "[Epoch 49 / 50 | Batch: 210 / 301] loss:   1047.252\n",
            "[Epoch 49 / 50 | Batch: 211 / 301] loss:   1130.974\n",
            "[Epoch 49 / 50 | Batch: 212 / 301] loss:   1228.121\n",
            "[Epoch 49 / 50 | Batch: 213 / 301] loss:    860.371\n",
            "[Epoch 49 / 50 | Batch: 214 / 301] loss:   1389.197\n",
            "[Epoch 49 / 50 | Batch: 215 / 301] loss:    961.076\n",
            "[Epoch 49 / 50 | Batch: 216 / 301] loss:   1361.032\n",
            "[Epoch 49 / 50 | Batch: 217 / 301] loss:   1064.611\n",
            "[Epoch 49 / 50 | Batch: 218 / 301] loss:   1006.291\n",
            "[Epoch 49 / 50 | Batch: 219 / 301] loss:   1465.424\n",
            "[Epoch 49 / 50 | Batch: 220 / 301] loss:    984.094\n",
            "[Epoch 49 / 50 | Batch: 221 / 301] loss:    976.365\n",
            "[Epoch 49 / 50 | Batch: 222 / 301] loss:   1576.243\n",
            "[Epoch 49 / 50 | Batch: 223 / 301] loss:    947.143\n",
            "[Epoch 49 / 50 | Batch: 224 / 301] loss:   1321.458\n",
            "[Epoch 49 / 50 | Batch: 225 / 301] loss:   1143.866\n",
            "[Epoch 49 / 50 | Batch: 226 / 301] loss:   1192.363\n",
            "[Epoch 49 / 50 | Batch: 227 / 301] loss:   1390.063\n",
            "[Epoch 49 / 50 | Batch: 228 / 301] loss:   1453.684\n",
            "[Epoch 49 / 50 | Batch: 229 / 301] loss:   1239.157\n",
            "[Epoch 49 / 50 | Batch: 230 / 301] loss:    915.827\n",
            "[Epoch 49 / 50 | Batch: 231 / 301] loss:   1303.517\n",
            "[Epoch 49 / 50 | Batch: 232 / 301] loss:    930.727\n",
            "[Epoch 49 / 50 | Batch: 233 / 301] loss:    706.412\n",
            "[Epoch 49 / 50 | Batch: 234 / 301] loss:   1102.542\n",
            "[Epoch 49 / 50 | Batch: 235 / 301] loss:    967.093\n",
            "[Epoch 49 / 50 | Batch: 236 / 301] loss:   1192.316\n",
            "[Epoch 49 / 50 | Batch: 237 / 301] loss:    815.651\n",
            "[Epoch 49 / 50 | Batch: 238 / 301] loss:   1109.908\n",
            "[Epoch 49 / 50 | Batch: 239 / 301] loss:    982.571\n",
            "[Epoch 49 / 50 | Batch: 240 / 301] loss:    957.150\n",
            "[Epoch 49 / 50 | Batch: 241 / 301] loss:   1324.234\n",
            "[Epoch 49 / 50 | Batch: 242 / 301] loss:   1266.877\n",
            "[Epoch 49 / 50 | Batch: 243 / 301] loss:   1053.836\n",
            "[Epoch 49 / 50 | Batch: 244 / 301] loss:    838.842\n",
            "[Epoch 49 / 50 | Batch: 245 / 301] loss:    825.959\n",
            "[Epoch 49 / 50 | Batch: 246 / 301] loss:   1359.153\n",
            "[Epoch 49 / 50 | Batch: 247 / 301] loss:   1203.147\n",
            "[Epoch 49 / 50 | Batch: 248 / 301] loss:    934.010\n",
            "[Epoch 49 / 50 | Batch: 249 / 301] loss:    865.739\n",
            "[Epoch 49 / 50 | Batch: 250 / 301] loss:   1022.491\n",
            "[Epoch 49 / 50 | Batch: 251 / 301] loss:   1213.526\n",
            "[Epoch 49 / 50 | Batch: 252 / 301] loss:   1074.154\n",
            "[Epoch 49 / 50 | Batch: 253 / 301] loss:   1026.195\n",
            "[Epoch 49 / 50 | Batch: 254 / 301] loss:   1391.778\n",
            "[Epoch 49 / 50 | Batch: 255 / 301] loss:   1246.724\n",
            "[Epoch 49 / 50 | Batch: 256 / 301] loss:    789.755\n",
            "[Epoch 49 / 50 | Batch: 257 / 301] loss:   1276.798\n",
            "[Epoch 49 / 50 | Batch: 258 / 301] loss:    784.964\n",
            "[Epoch 49 / 50 | Batch: 259 / 301] loss:   1365.778\n",
            "[Epoch 49 / 50 | Batch: 260 / 301] loss:   1055.188\n",
            "[Epoch 49 / 50 | Batch: 261 / 301] loss:   1205.263\n",
            "[Epoch 49 / 50 | Batch: 262 / 301] loss:   1191.261\n",
            "[Epoch 49 / 50 | Batch: 263 / 301] loss:   1100.167\n",
            "[Epoch 49 / 50 | Batch: 264 / 301] loss:    918.908\n",
            "[Epoch 49 / 50 | Batch: 265 / 301] loss:    913.546\n",
            "[Epoch 49 / 50 | Batch: 266 / 301] loss:   1101.912\n",
            "[Epoch 49 / 50 | Batch: 267 / 301] loss:   1138.282\n",
            "[Epoch 49 / 50 | Batch: 268 / 301] loss:   1297.335\n",
            "[Epoch 49 / 50 | Batch: 269 / 301] loss:   1427.697\n",
            "[Epoch 49 / 50 | Batch: 270 / 301] loss:   1107.210\n",
            "[Epoch 49 / 50 | Batch: 271 / 301] loss:   1140.102\n",
            "[Epoch 49 / 50 | Batch: 272 / 301] loss:   1163.831\n",
            "[Epoch 49 / 50 | Batch: 273 / 301] loss:   1504.234\n",
            "[Epoch 49 / 50 | Batch: 274 / 301] loss:   1285.214\n",
            "[Epoch 49 / 50 | Batch: 275 / 301] loss:   1304.939\n",
            "[Epoch 49 / 50 | Batch: 276 / 301] loss:   1141.996\n",
            "[Epoch 49 / 50 | Batch: 277 / 301] loss:    692.662\n",
            "[Epoch 49 / 50 | Batch: 278 / 301] loss:   1169.891\n",
            "[Epoch 49 / 50 | Batch: 279 / 301] loss:   1122.459\n",
            "[Epoch 49 / 50 | Batch: 280 / 301] loss:   1085.584\n",
            "[Epoch 49 / 50 | Batch: 281 / 301] loss:   1330.764\n",
            "[Epoch 49 / 50 | Batch: 282 / 301] loss:    885.697\n",
            "[Epoch 49 / 50 | Batch: 283 / 301] loss:   1594.136\n",
            "[Epoch 49 / 50 | Batch: 284 / 301] loss:   1381.451\n",
            "[Epoch 49 / 50 | Batch: 285 / 301] loss:   1002.328\n",
            "[Epoch 49 / 50 | Batch: 286 / 301] loss:    997.820\n",
            "[Epoch 49 / 50 | Batch: 287 / 301] loss:   1157.548\n",
            "[Epoch 49 / 50 | Batch: 288 / 301] loss:    926.420\n",
            "[Epoch 49 / 50 | Batch: 289 / 301] loss:    953.355\n",
            "[Epoch 49 / 50 | Batch: 290 / 301] loss:    750.172\n",
            "[Epoch 49 / 50 | Batch: 291 / 301] loss:   1235.226\n",
            "[Epoch 49 / 50 | Batch: 292 / 301] loss:   1123.889\n",
            "[Epoch 49 / 50 | Batch: 293 / 301] loss:   1256.957\n",
            "[Epoch 49 / 50 | Batch: 294 / 301] loss:   1219.281\n",
            "[Epoch 49 / 50 | Batch: 295 / 301] loss:   1052.350\n",
            "[Epoch 49 / 50 | Batch: 296 / 301] loss:   1173.301\n",
            "[Epoch 49 / 50 | Batch: 297 / 301] loss:   1459.930\n",
            "[Epoch 49 / 50 | Batch: 298 / 301] loss:   1465.417\n",
            "[Epoch 49 / 50 | Batch: 299 / 301] loss:    857.760\n",
            "[Epoch 49 / 50 | Batch: 300 / 301] loss:   1374.775\n",
            "[Epoch 49 / 50 | Batch: 301 / 301] loss:    610.270\n",
            "Epoch loss: 1157.35279\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4359.584\n",
            "[Validation] [Batch  2 / 15] dev loss:   6550.907\n",
            "[Validation] [Batch  3 / 15] dev loss:   6823.421\n",
            "[Validation] [Batch  4 / 15] dev loss:   5264.937\n",
            "[Validation] [Batch  5 / 15] dev loss:   5558.489\n",
            "[Validation] [Batch  6 / 15] dev loss:   9712.125\n",
            "[Validation] [Batch  7 / 15] dev loss:   6896.835\n",
            "[Validation] [Batch  8 / 15] dev loss:   4224.660\n",
            "[Validation] [Batch  9 / 15] dev loss:   3744.622\n",
            "[Validation] [Batch 10 / 15] dev loss:  15951.168\n",
            "[Validation] [Batch 11 / 15] dev loss:  13335.508\n",
            "[Validation] [Batch 12 / 15] dev loss:  11697.185\n",
            "[Validation] [Batch 13 / 15] dev loss:   4288.531\n",
            "[Validation] [Batch 14 / 15] dev loss:  23780.900\n",
            "[Validation] [Batch 15 / 15] dev loss:   2201.312\n",
            "Dev loss 8292.67892\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "-----------------------------------------------\n",
            "Epoch 50 / 50\n",
            "-----------------------------------------------\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet.pt\n",
            "[Epoch 50 / 50 | Batch:  1 / 301] loss:    724.636\n",
            "[Epoch 50 / 50 | Batch:  2 / 301] loss:   1425.524\n",
            "[Epoch 50 / 50 | Batch:  3 / 301] loss:   1317.183\n",
            "[Epoch 50 / 50 | Batch:  4 / 301] loss:    830.007\n",
            "[Epoch 50 / 50 | Batch:  5 / 301] loss:    921.245\n",
            "[Epoch 50 / 50 | Batch:  6 / 301] loss:    912.748\n",
            "[Epoch 50 / 50 | Batch:  7 / 301] loss:   1039.940\n",
            "[Epoch 50 / 50 | Batch:  8 / 301] loss:   1315.177\n",
            "[Epoch 50 / 50 | Batch:  9 / 301] loss:   1007.416\n",
            "[Epoch 50 / 50 | Batch: 10 / 301] loss:   1049.871\n",
            "[Epoch 50 / 50 | Batch: 11 / 301] loss:   1108.964\n",
            "[Epoch 50 / 50 | Batch: 12 / 301] loss:    851.181\n",
            "[Epoch 50 / 50 | Batch: 13 / 301] loss:   1079.648\n",
            "[Epoch 50 / 50 | Batch: 14 / 301] loss:   1198.273\n",
            "[Epoch 50 / 50 | Batch: 15 / 301] loss:    884.663\n",
            "[Epoch 50 / 50 | Batch: 16 / 301] loss:    884.853\n",
            "[Epoch 50 / 50 | Batch: 17 / 301] loss:   1470.063\n",
            "[Epoch 50 / 50 | Batch: 18 / 301] loss:    784.136\n",
            "[Epoch 50 / 50 | Batch: 19 / 301] loss:   1197.176\n",
            "[Epoch 50 / 50 | Batch: 20 / 301] loss:   1021.778\n",
            "[Epoch 50 / 50 | Batch: 21 / 301] loss:   1273.251\n",
            "[Epoch 50 / 50 | Batch: 22 / 301] loss:   1421.727\n",
            "[Epoch 50 / 50 | Batch: 23 / 301] loss:   1027.693\n",
            "[Epoch 50 / 50 | Batch: 24 / 301] loss:   1239.837\n",
            "[Epoch 50 / 50 | Batch: 25 / 301] loss:   1183.203\n",
            "[Epoch 50 / 50 | Batch: 26 / 301] loss:   1289.519\n",
            "[Epoch 50 / 50 | Batch: 27 / 301] loss:   1292.298\n",
            "[Epoch 50 / 50 | Batch: 28 / 301] loss:   1348.211\n",
            "[Epoch 50 / 50 | Batch: 29 / 301] loss:   1343.408\n",
            "[Epoch 50 / 50 | Batch: 30 / 301] loss:    851.595\n",
            "[Epoch 50 / 50 | Batch: 31 / 301] loss:   1111.283\n",
            "[Epoch 50 / 50 | Batch: 32 / 301] loss:   1097.078\n",
            "[Epoch 50 / 50 | Batch: 33 / 301] loss:    980.031\n",
            "[Epoch 50 / 50 | Batch: 34 / 301] loss:   1224.773\n",
            "[Epoch 50 / 50 | Batch: 35 / 301] loss:    924.486\n",
            "[Epoch 50 / 50 | Batch: 36 / 301] loss:    980.061\n",
            "[Epoch 50 / 50 | Batch: 37 / 301] loss:   1257.901\n",
            "[Epoch 50 / 50 | Batch: 38 / 301] loss:   1381.221\n",
            "[Epoch 50 / 50 | Batch: 39 / 301] loss:   1127.573\n",
            "[Epoch 50 / 50 | Batch: 40 / 301] loss:   1242.384\n",
            "[Epoch 50 / 50 | Batch: 41 / 301] loss:   1361.266\n",
            "[Epoch 50 / 50 | Batch: 42 / 301] loss:   1146.781\n",
            "[Epoch 50 / 50 | Batch: 43 / 301] loss:   1143.496\n",
            "[Epoch 50 / 50 | Batch: 44 / 301] loss:   1310.505\n",
            "[Epoch 50 / 50 | Batch: 45 / 301] loss:   1450.777\n",
            "[Epoch 50 / 50 | Batch: 46 / 301] loss:    972.436\n",
            "[Epoch 50 / 50 | Batch: 47 / 301] loss:    960.999\n",
            "[Epoch 50 / 50 | Batch: 48 / 301] loss:    810.344\n",
            "[Epoch 50 / 50 | Batch: 49 / 301] loss:   1025.766\n",
            "[Epoch 50 / 50 | Batch: 50 / 301] loss:   1273.097\n",
            "[Epoch 50 / 50 | Batch: 51 / 301] loss:    838.541\n",
            "[Epoch 50 / 50 | Batch: 52 / 301] loss:   1206.377\n",
            "[Epoch 50 / 50 | Batch: 53 / 301] loss:   1138.696\n",
            "[Epoch 50 / 50 | Batch: 54 / 301] loss:   1172.617\n",
            "[Epoch 50 / 50 | Batch: 55 / 301] loss:   1262.272\n",
            "[Epoch 50 / 50 | Batch: 56 / 301] loss:    974.463\n",
            "[Epoch 50 / 50 | Batch: 57 / 301] loss:    851.354\n",
            "[Epoch 50 / 50 | Batch: 58 / 301] loss:   1333.967\n",
            "[Epoch 50 / 50 | Batch: 59 / 301] loss:   1057.845\n",
            "[Epoch 50 / 50 | Batch: 60 / 301] loss:   1219.137\n",
            "[Epoch 50 / 50 | Batch: 61 / 301] loss:   1180.891\n",
            "[Epoch 50 / 50 | Batch: 62 / 301] loss:   1139.704\n",
            "[Epoch 50 / 50 | Batch: 63 / 301] loss:    996.658\n",
            "[Epoch 50 / 50 | Batch: 64 / 301] loss:   1272.210\n",
            "[Epoch 50 / 50 | Batch: 65 / 301] loss:   1178.478\n",
            "[Epoch 50 / 50 | Batch: 66 / 301] loss:   1070.010\n",
            "[Epoch 50 / 50 | Batch: 67 / 301] loss:   1261.541\n",
            "[Epoch 50 / 50 | Batch: 68 / 301] loss:   1352.194\n",
            "[Epoch 50 / 50 | Batch: 69 / 301] loss:   1493.853\n",
            "[Epoch 50 / 50 | Batch: 70 / 301] loss:   1332.560\n",
            "[Epoch 50 / 50 | Batch: 71 / 301] loss:    815.908\n",
            "[Epoch 50 / 50 | Batch: 72 / 301] loss:   1013.025\n",
            "[Epoch 50 / 50 | Batch: 73 / 301] loss:   1190.146\n",
            "[Epoch 50 / 50 | Batch: 74 / 301] loss:   1199.661\n",
            "[Epoch 50 / 50 | Batch: 75 / 301] loss:   1399.844\n",
            "[Epoch 50 / 50 | Batch: 76 / 301] loss:   1377.100\n",
            "[Epoch 50 / 50 | Batch: 77 / 301] loss:   1025.890\n",
            "[Epoch 50 / 50 | Batch: 78 / 301] loss:   1528.958\n",
            "[Epoch 50 / 50 | Batch: 79 / 301] loss:    927.159\n",
            "[Epoch 50 / 50 | Batch: 80 / 301] loss:    953.633\n",
            "[Epoch 50 / 50 | Batch: 81 / 301] loss:   1305.768\n",
            "[Epoch 50 / 50 | Batch: 82 / 301] loss:   1188.786\n",
            "[Epoch 50 / 50 | Batch: 83 / 301] loss:   1344.963\n",
            "[Epoch 50 / 50 | Batch: 84 / 301] loss:   1136.459\n",
            "[Epoch 50 / 50 | Batch: 85 / 301] loss:   1396.088\n",
            "[Epoch 50 / 50 | Batch: 86 / 301] loss:   1208.395\n",
            "[Epoch 50 / 50 | Batch: 87 / 301] loss:    986.472\n",
            "[Epoch 50 / 50 | Batch: 88 / 301] loss:   1219.917\n",
            "[Epoch 50 / 50 | Batch: 89 / 301] loss:   1175.277\n",
            "[Epoch 50 / 50 | Batch: 90 / 301] loss:   1089.772\n",
            "[Epoch 50 / 50 | Batch: 91 / 301] loss:   1264.802\n",
            "[Epoch 50 / 50 | Batch: 92 / 301] loss:   1161.301\n",
            "[Epoch 50 / 50 | Batch: 93 / 301] loss:    875.860\n",
            "[Epoch 50 / 50 | Batch: 94 / 301] loss:   1194.531\n",
            "[Epoch 50 / 50 | Batch: 95 / 301] loss:   1395.479\n",
            "[Epoch 50 / 50 | Batch: 96 / 301] loss:    965.600\n",
            "[Epoch 50 / 50 | Batch: 97 / 301] loss:   1079.812\n",
            "[Epoch 50 / 50 | Batch: 98 / 301] loss:   1322.281\n",
            "[Epoch 50 / 50 | Batch: 99 / 301] loss:    980.245\n",
            "[Epoch 50 / 50 | Batch: 100 / 301] loss:   1051.847\n",
            "[Epoch 50 / 50 | Batch: 101 / 301] loss:   1049.885\n",
            "[Epoch 50 / 50 | Batch: 102 / 301] loss:   1044.216\n",
            "[Epoch 50 / 50 | Batch: 103 / 301] loss:   1106.139\n",
            "[Epoch 50 / 50 | Batch: 104 / 301] loss:   1201.138\n",
            "[Epoch 50 / 50 | Batch: 105 / 301] loss:   1266.140\n",
            "[Epoch 50 / 50 | Batch: 106 / 301] loss:   1055.327\n",
            "[Epoch 50 / 50 | Batch: 107 / 301] loss:   1159.336\n",
            "[Epoch 50 / 50 | Batch: 108 / 301] loss:   1276.081\n",
            "[Epoch 50 / 50 | Batch: 109 / 301] loss:   1160.841\n",
            "[Epoch 50 / 50 | Batch: 110 / 301] loss:    926.830\n",
            "[Epoch 50 / 50 | Batch: 111 / 301] loss:   1352.241\n",
            "[Epoch 50 / 50 | Batch: 112 / 301] loss:   1163.974\n",
            "[Epoch 50 / 50 | Batch: 113 / 301] loss:   1033.437\n",
            "[Epoch 50 / 50 | Batch: 114 / 301] loss:   1441.989\n",
            "[Epoch 50 / 50 | Batch: 115 / 301] loss:   1142.496\n",
            "[Epoch 50 / 50 | Batch: 116 / 301] loss:   1396.663\n",
            "[Epoch 50 / 50 | Batch: 117 / 301] loss:   1528.651\n",
            "[Epoch 50 / 50 | Batch: 118 / 301] loss:   1248.348\n",
            "[Epoch 50 / 50 | Batch: 119 / 301] loss:   1213.594\n",
            "[Epoch 50 / 50 | Batch: 120 / 301] loss:   1263.315\n",
            "[Epoch 50 / 50 | Batch: 121 / 301] loss:   1248.419\n",
            "[Epoch 50 / 50 | Batch: 122 / 301] loss:    843.346\n",
            "[Epoch 50 / 50 | Batch: 123 / 301] loss:   1653.557\n",
            "[Epoch 50 / 50 | Batch: 124 / 301] loss:   1091.636\n",
            "[Epoch 50 / 50 | Batch: 125 / 301] loss:   1148.675\n",
            "[Epoch 50 / 50 | Batch: 126 / 301] loss:   1249.278\n",
            "[Epoch 50 / 50 | Batch: 127 / 301] loss:   1204.224\n",
            "[Epoch 50 / 50 | Batch: 128 / 301] loss:   1168.275\n",
            "[Epoch 50 / 50 | Batch: 129 / 301] loss:   1201.875\n",
            "[Epoch 50 / 50 | Batch: 130 / 301] loss:    937.452\n",
            "[Epoch 50 / 50 | Batch: 131 / 301] loss:   1060.483\n",
            "[Epoch 50 / 50 | Batch: 132 / 301] loss:   1024.888\n",
            "[Epoch 50 / 50 | Batch: 133 / 301] loss:   1432.639\n",
            "[Epoch 50 / 50 | Batch: 134 / 301] loss:   1237.642\n",
            "[Epoch 50 / 50 | Batch: 135 / 301] loss:   1191.768\n",
            "[Epoch 50 / 50 | Batch: 136 / 301] loss:   1184.369\n",
            "[Epoch 50 / 50 | Batch: 137 / 301] loss:   1526.989\n",
            "[Epoch 50 / 50 | Batch: 138 / 301] loss:   1291.345\n",
            "[Epoch 50 / 50 | Batch: 139 / 301] loss:   1647.758\n",
            "[Epoch 50 / 50 | Batch: 140 / 301] loss:   1099.981\n",
            "[Epoch 50 / 50 | Batch: 141 / 301] loss:    788.403\n",
            "[Epoch 50 / 50 | Batch: 142 / 301] loss:   1387.938\n",
            "[Epoch 50 / 50 | Batch: 143 / 301] loss:   1050.752\n",
            "[Epoch 50 / 50 | Batch: 144 / 301] loss:   1096.656\n",
            "[Epoch 50 / 50 | Batch: 145 / 301] loss:   1095.252\n",
            "[Epoch 50 / 50 | Batch: 146 / 301] loss:   1473.305\n",
            "[Epoch 50 / 50 | Batch: 147 / 301] loss:    870.466\n",
            "[Epoch 50 / 50 | Batch: 148 / 301] loss:   1571.538\n",
            "[Epoch 50 / 50 | Batch: 149 / 301] loss:   1321.447\n",
            "[Epoch 50 / 50 | Batch: 150 / 301] loss:    862.318\n",
            "[Epoch 50 / 50 | Batch: 151 / 301] loss:   1386.460\n",
            "[Epoch 50 / 50 | Batch: 152 / 301] loss:   1052.330\n",
            "[Epoch 50 / 50 | Batch: 153 / 301] loss:   1220.536\n",
            "[Epoch 50 / 50 | Batch: 154 / 301] loss:    965.217\n",
            "[Epoch 50 / 50 | Batch: 155 / 301] loss:   1067.785\n",
            "[Epoch 50 / 50 | Batch: 156 / 301] loss:   1170.905\n",
            "[Epoch 50 / 50 | Batch: 157 / 301] loss:    920.322\n",
            "[Epoch 50 / 50 | Batch: 158 / 301] loss:   1004.984\n",
            "[Epoch 50 / 50 | Batch: 159 / 301] loss:   1122.535\n",
            "[Epoch 50 / 50 | Batch: 160 / 301] loss:   1248.449\n",
            "[Epoch 50 / 50 | Batch: 161 / 301] loss:   1053.123\n",
            "[Epoch 50 / 50 | Batch: 162 / 301] loss:    863.181\n",
            "[Epoch 50 / 50 | Batch: 163 / 301] loss:   1057.706\n",
            "[Epoch 50 / 50 | Batch: 164 / 301] loss:   1269.058\n",
            "[Epoch 50 / 50 | Batch: 165 / 301] loss:   1245.684\n",
            "[Epoch 50 / 50 | Batch: 166 / 301] loss:    985.279\n",
            "[Epoch 50 / 50 | Batch: 167 / 301] loss:    797.212\n",
            "[Epoch 50 / 50 | Batch: 168 / 301] loss:   1329.720\n",
            "[Epoch 50 / 50 | Batch: 169 / 301] loss:   1314.766\n",
            "[Epoch 50 / 50 | Batch: 170 / 301] loss:   1054.582\n",
            "[Epoch 50 / 50 | Batch: 171 / 301] loss:   1680.057\n",
            "[Epoch 50 / 50 | Batch: 172 / 301] loss:   1135.670\n",
            "[Epoch 50 / 50 | Batch: 173 / 301] loss:    857.179\n",
            "[Epoch 50 / 50 | Batch: 174 / 301] loss:   1087.814\n",
            "[Epoch 50 / 50 | Batch: 175 / 301] loss:    990.279\n",
            "[Epoch 50 / 50 | Batch: 176 / 301] loss:    682.374\n",
            "[Epoch 50 / 50 | Batch: 177 / 301] loss:   1088.407\n",
            "[Epoch 50 / 50 | Batch: 178 / 301] loss:   1058.010\n",
            "[Epoch 50 / 50 | Batch: 179 / 301] loss:   1340.089\n",
            "[Epoch 50 / 50 | Batch: 180 / 301] loss:   1175.929\n",
            "[Epoch 50 / 50 | Batch: 181 / 301] loss:   1608.052\n",
            "[Epoch 50 / 50 | Batch: 182 / 301] loss:    758.044\n",
            "[Epoch 50 / 50 | Batch: 183 / 301] loss:   1168.277\n",
            "[Epoch 50 / 50 | Batch: 184 / 301] loss:   1166.894\n",
            "[Epoch 50 / 50 | Batch: 185 / 301] loss:    868.688\n",
            "[Epoch 50 / 50 | Batch: 186 / 301] loss:   1125.458\n",
            "[Epoch 50 / 50 | Batch: 187 / 301] loss:   1346.478\n",
            "[Epoch 50 / 50 | Batch: 188 / 301] loss:   1095.921\n",
            "[Epoch 50 / 50 | Batch: 189 / 301] loss:   1043.820\n",
            "[Epoch 50 / 50 | Batch: 190 / 301] loss:   1020.249\n",
            "[Epoch 50 / 50 | Batch: 191 / 301] loss:   1117.205\n",
            "[Epoch 50 / 50 | Batch: 192 / 301] loss:    967.989\n",
            "[Epoch 50 / 50 | Batch: 193 / 301] loss:    853.286\n",
            "[Epoch 50 / 50 | Batch: 194 / 301] loss:    972.724\n",
            "[Epoch 50 / 50 | Batch: 195 / 301] loss:   1134.334\n",
            "[Epoch 50 / 50 | Batch: 196 / 301] loss:   1226.699\n",
            "[Epoch 50 / 50 | Batch: 197 / 301] loss:   1205.753\n",
            "[Epoch 50 / 50 | Batch: 198 / 301] loss:   1090.220\n",
            "[Epoch 50 / 50 | Batch: 199 / 301] loss:   1103.450\n",
            "[Epoch 50 / 50 | Batch: 200 / 301] loss:   1015.808\n",
            "[Epoch 50 / 50 | Batch: 201 / 301] loss:    918.797\n",
            "[Epoch 50 / 50 | Batch: 202 / 301] loss:    891.744\n",
            "[Epoch 50 / 50 | Batch: 203 / 301] loss:   1220.788\n",
            "[Epoch 50 / 50 | Batch: 204 / 301] loss:    991.614\n",
            "[Epoch 50 / 50 | Batch: 205 / 301] loss:   1174.651\n",
            "[Epoch 50 / 50 | Batch: 206 / 301] loss:    945.337\n",
            "[Epoch 50 / 50 | Batch: 207 / 301] loss:   1561.729\n",
            "[Epoch 50 / 50 | Batch: 208 / 301] loss:   1171.354\n",
            "[Epoch 50 / 50 | Batch: 209 / 301] loss:   1399.835\n",
            "[Epoch 50 / 50 | Batch: 210 / 301] loss:   1039.829\n",
            "[Epoch 50 / 50 | Batch: 211 / 301] loss:    977.564\n",
            "[Epoch 50 / 50 | Batch: 212 / 301] loss:   1440.841\n",
            "[Epoch 50 / 50 | Batch: 213 / 301] loss:   1373.970\n",
            "[Epoch 50 / 50 | Batch: 214 / 301] loss:   1433.346\n",
            "[Epoch 50 / 50 | Batch: 215 / 301] loss:   1487.727\n",
            "[Epoch 50 / 50 | Batch: 216 / 301] loss:   1056.229\n",
            "[Epoch 50 / 50 | Batch: 217 / 301] loss:   1014.999\n",
            "[Epoch 50 / 50 | Batch: 218 / 301] loss:   1509.167\n",
            "[Epoch 50 / 50 | Batch: 219 / 301] loss:   1090.213\n",
            "[Epoch 50 / 50 | Batch: 220 / 301] loss:   1102.594\n",
            "[Epoch 50 / 50 | Batch: 221 / 301] loss:   1181.349\n",
            "[Epoch 50 / 50 | Batch: 222 / 301] loss:   1217.607\n",
            "[Epoch 50 / 50 | Batch: 223 / 301] loss:    977.228\n",
            "[Epoch 50 / 50 | Batch: 224 / 301] loss:    966.688\n",
            "[Epoch 50 / 50 | Batch: 225 / 301] loss:   1453.742\n",
            "[Epoch 50 / 50 | Batch: 226 / 301] loss:   1014.183\n",
            "[Epoch 50 / 50 | Batch: 227 / 301] loss:   1372.050\n",
            "[Epoch 50 / 50 | Batch: 228 / 301] loss:   1028.641\n",
            "[Epoch 50 / 50 | Batch: 229 / 301] loss:   1180.427\n",
            "[Epoch 50 / 50 | Batch: 230 / 301] loss:   1197.011\n",
            "[Epoch 50 / 50 | Batch: 231 / 301] loss:    838.690\n",
            "[Epoch 50 / 50 | Batch: 232 / 301] loss:   1131.452\n",
            "[Epoch 50 / 50 | Batch: 233 / 301] loss:   1547.230\n",
            "[Epoch 50 / 50 | Batch: 234 / 301] loss:   1026.371\n",
            "[Epoch 50 / 50 | Batch: 235 / 301] loss:   1272.279\n",
            "[Epoch 50 / 50 | Batch: 236 / 301] loss:   1040.697\n",
            "[Epoch 50 / 50 | Batch: 237 / 301] loss:   1281.508\n",
            "[Epoch 50 / 50 | Batch: 238 / 301] loss:    832.674\n",
            "[Epoch 50 / 50 | Batch: 239 / 301] loss:   1300.109\n",
            "[Epoch 50 / 50 | Batch: 240 / 301] loss:   1116.401\n",
            "[Epoch 50 / 50 | Batch: 241 / 301] loss:   1780.616\n",
            "[Epoch 50 / 50 | Batch: 242 / 301] loss:   1225.645\n",
            "[Epoch 50 / 50 | Batch: 243 / 301] loss:   1245.659\n",
            "[Epoch 50 / 50 | Batch: 244 / 301] loss:    923.825\n",
            "[Epoch 50 / 50 | Batch: 245 / 301] loss:   1631.060\n",
            "[Epoch 50 / 50 | Batch: 246 / 301] loss:   1257.055\n",
            "[Epoch 50 / 50 | Batch: 247 / 301] loss:   1236.291\n",
            "[Epoch 50 / 50 | Batch: 248 / 301] loss:   1478.812\n",
            "[Epoch 50 / 50 | Batch: 249 / 301] loss:   1138.466\n",
            "[Epoch 50 / 50 | Batch: 250 / 301] loss:   1133.003\n",
            "[Epoch 50 / 50 | Batch: 251 / 301] loss:   1182.438\n",
            "[Epoch 50 / 50 | Batch: 252 / 301] loss:    981.426\n",
            "[Epoch 50 / 50 | Batch: 253 / 301] loss:   1336.883\n",
            "[Epoch 50 / 50 | Batch: 254 / 301] loss:   1193.045\n",
            "[Epoch 50 / 50 | Batch: 255 / 301] loss:    962.419\n",
            "[Epoch 50 / 50 | Batch: 256 / 301] loss:    847.718\n",
            "[Epoch 50 / 50 | Batch: 257 / 301] loss:   1084.895\n",
            "[Epoch 50 / 50 | Batch: 258 / 301] loss:   1146.079\n",
            "[Epoch 50 / 50 | Batch: 259 / 301] loss:   1073.463\n",
            "[Epoch 50 / 50 | Batch: 260 / 301] loss:   1136.351\n",
            "[Epoch 50 / 50 | Batch: 261 / 301] loss:   1361.928\n",
            "[Epoch 50 / 50 | Batch: 262 / 301] loss:   1392.701\n",
            "[Epoch 50 / 50 | Batch: 263 / 301] loss:   1219.798\n",
            "[Epoch 50 / 50 | Batch: 264 / 301] loss:   1149.532\n",
            "[Epoch 50 / 50 | Batch: 265 / 301] loss:    831.666\n",
            "[Epoch 50 / 50 | Batch: 266 / 301] loss:   1364.010\n",
            "[Epoch 50 / 50 | Batch: 267 / 301] loss:   1386.178\n",
            "[Epoch 50 / 50 | Batch: 268 / 301] loss:   1397.518\n",
            "[Epoch 50 / 50 | Batch: 269 / 301] loss:   1115.731\n",
            "[Epoch 50 / 50 | Batch: 270 / 301] loss:   1032.278\n",
            "[Epoch 50 / 50 | Batch: 271 / 301] loss:   1154.360\n",
            "[Epoch 50 / 50 | Batch: 272 / 301] loss:   1198.777\n",
            "[Epoch 50 / 50 | Batch: 273 / 301] loss:   1188.697\n",
            "[Epoch 50 / 50 | Batch: 274 / 301] loss:   1080.215\n",
            "[Epoch 50 / 50 | Batch: 275 / 301] loss:   1185.594\n",
            "[Epoch 50 / 50 | Batch: 276 / 301] loss:   1558.879\n",
            "[Epoch 50 / 50 | Batch: 277 / 301] loss:    905.257\n",
            "[Epoch 50 / 50 | Batch: 278 / 301] loss:   1275.053\n",
            "[Epoch 50 / 50 | Batch: 279 / 301] loss:   1004.841\n",
            "[Epoch 50 / 50 | Batch: 280 / 301] loss:    866.190\n",
            "[Epoch 50 / 50 | Batch: 281 / 301] loss:   1131.642\n",
            "[Epoch 50 / 50 | Batch: 282 / 301] loss:   1220.284\n",
            "[Epoch 50 / 50 | Batch: 283 / 301] loss:   1376.264\n",
            "[Epoch 50 / 50 | Batch: 284 / 301] loss:    885.888\n",
            "[Epoch 50 / 50 | Batch: 285 / 301] loss:   1453.971\n",
            "[Epoch 50 / 50 | Batch: 286 / 301] loss:    953.099\n",
            "[Epoch 50 / 50 | Batch: 287 / 301] loss:   1679.023\n",
            "[Epoch 50 / 50 | Batch: 288 / 301] loss:    894.266\n",
            "[Epoch 50 / 50 | Batch: 289 / 301] loss:   1415.189\n",
            "[Epoch 50 / 50 | Batch: 290 / 301] loss:    992.137\n",
            "[Epoch 50 / 50 | Batch: 291 / 301] loss:   1260.116\n",
            "[Epoch 50 / 50 | Batch: 292 / 301] loss:    789.962\n",
            "[Epoch 50 / 50 | Batch: 293 / 301] loss:    895.485\n",
            "[Epoch 50 / 50 | Batch: 294 / 301] loss:   1368.249\n",
            "[Epoch 50 / 50 | Batch: 295 / 301] loss:    843.569\n",
            "[Epoch 50 / 50 | Batch: 296 / 301] loss:   1113.277\n",
            "[Epoch 50 / 50 | Batch: 297 / 301] loss:   1137.792\n",
            "[Epoch 50 / 50 | Batch: 298 / 301] loss:   1278.892\n",
            "[Epoch 50 / 50 | Batch: 299 / 301] loss:    880.132\n",
            "[Epoch 50 / 50 | Batch: 300 / 301] loss:   1142.220\n",
            "[Epoch 50 / 50 | Batch: 301 / 301] loss:    546.643\n",
            "Epoch loss: 1152.80815\n",
            "\n",
            "Validating...\n",
            "[Validation] [Batch  1 / 15] dev loss:   4363.066\n",
            "[Validation] [Batch  2 / 15] dev loss:   6277.663\n",
            "[Validation] [Batch  3 / 15] dev loss:   6608.044\n",
            "[Validation] [Batch  4 / 15] dev loss:   5621.555\n",
            "[Validation] [Batch  5 / 15] dev loss:   5496.300\n",
            "[Validation] [Batch  6 / 15] dev loss:   9955.124\n",
            "[Validation] [Batch  7 / 15] dev loss:   6365.605\n",
            "[Validation] [Batch  8 / 15] dev loss:   4076.804\n",
            "[Validation] [Batch  9 / 15] dev loss:   3617.853\n",
            "[Validation] [Batch 10 / 15] dev loss:  16131.344\n",
            "[Validation] [Batch 11 / 15] dev loss:  13401.805\n",
            "[Validation] [Batch 12 / 15] dev loss:  10866.615\n",
            "[Validation] [Batch 13 / 15] dev loss:   4563.013\n",
            "[Validation] [Batch 14 / 15] dev loss:  21824.061\n",
            "[Validation] [Batch 15 / 15] dev loss:   1742.631\n",
            "Dev loss 8060.76549\n",
            "\n",
            "saved model to /content/drive/My Drive/colorization/model/colnet.pt\n",
            "\n",
            "\n",
            "Finished Training.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGLNQ92aZvjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "dcc3b45a-8b96-4a09-a3c5-8f12c5af13a1"
      },
      "source": [
        "plt.plot(t.loss_history['train'], label=\"Train\")\n",
        "plt.plot(t.loss_history['val'], label=\"Validate\")\n",
        "plt.title(\"Losses\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLyZBjIZyah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e2b1d8f-6bdc-46d2-d2c0-aba9a92e16c8"
      },
      "source": [
        "t.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make sure you're using up to date model!!!\n",
            "Colorizing /content/drive/My Drive/colorization/test using /content/drive/My Drive/colorization/model/colnet-the-best.pt\n",
            "\n",
            "Resuming training of: /content/drive/My Drive/colorization/model/colnet-the-best.pt\n",
            "Processing batch 1 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 2 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
            "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 3 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 4 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 5 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 6 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 7 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 8 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 9 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing batch 10 / 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved all photos to /content/drive/My Drive/colorization/out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuPguARpavUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}